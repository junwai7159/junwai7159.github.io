{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI/ML Engineer","text":"<p>AI/ML Engineer</p>"},{"location":"about/","title":"About Me","text":""},{"location":"blog/","title":"Blogs","text":""},{"location":"changelog/CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project are documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/CHANGELOG/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/CHANGELOG/#added","title":"Added","text":"<ul> <li>Social links at footer.</li> <li>Copyright notice.</li> </ul>"},{"location":"changelog/CHANGELOG/#020-2024-11-16","title":"[0.2.0] - 2024-11-16","text":""},{"location":"changelog/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Notes for CV, NLP section.</li> <li>Toggle button for light/dark mode.</li> <li>GLightBox plugin for image lightbox effect.</li> <li>Built-in blog plugin.</li> </ul>"},{"location":"changelog/CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Move navigation bar to header.</li> <li>awesome-pages plugin to simplify configuring page titles and their order.</li> </ul>"},{"location":"changelog/CHANGELOG/#010-2024-11-06","title":"[0.1.0] - 2024-11-06","text":""},{"location":"changelog/CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Initial release of the project.</li> <li>Integration with GitHub Pages for deployment.</li> <li>GitHub Actions workflow for continuous deployment.</li> </ul>"},{"location":"changelog/project-roadmap/","title":"Project Roadmap","text":"<ul> <li> Lorem ipsum</li> </ul>"},{"location":"note/","title":"Note","text":""},{"location":"note/Books/","title":"Books","text":"<ul> <li>Just keep buying</li> <li>Staff engineer leadership beyond the management track</li> <li>fundamentals: The neatest little guid to stock market investing by Jason Kelly</li> <li>technical: the art and science of technical analysis by adam grimes</li> <li>strategy/market dynamics: market wizards series by jack schwagger</li> </ul>"},{"location":"note/Computer_Architecture/","title":"Computer Architecture","text":"<ul> <li>CSAPP</li> </ul>"},{"location":"note/Computer_Networks/","title":"Computer Networks","text":"<ul> <li>a top down approach</li> </ul>"},{"location":"note/Computer_Vision/","title":"CV Notes/Codebase","text":"<ul> <li>Computer Vision: Algorithms and Applications, 2nd ed.</li> <li>Modern Computer Vision with PyTorch, 2nd ed.</li> </ul>"},{"location":"note/Computer_Vision/3D_Vision/","title":"3D Vision","text":""},{"location":"note/Computer_Vision/3D_Vision/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/3D_Vision/#camera-models","title":"Camera models","text":""},{"location":"note/Computer_Vision/3D_Vision/#novel-view-synthesis","title":"Novel View Synthesis","text":""},{"location":"note/Computer_Vision/3D_Vision/#stereo-vision","title":"Stereo Vision","text":""},{"location":"note/Computer_Vision/3D_Vision/#models","title":"Models","text":""},{"location":"note/Computer_Vision/3D_Vision/#neural-radiance-fields-nerfs","title":"Neural Radiance Fields (NERFs)","text":""},{"location":"note/Computer_Vision/Artificial_Neural_Networks/","title":"Artificial Neural Networks","text":""},{"location":"note/Computer_Vision/Artificial_Neural_Networks/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/Artificial_Neural_Networks/#universal-approximation-theorem","title":"Universal Approximation Theorem","text":"<p>We can always find a large enough neural network architecture with the right set of weights that can exactly predict any output for any given input.</p>"},{"location":"note/Computer_Vision/Artificial_Neural_Networks/#deep-learning","title":"Deep Learning","text":"<ul> <li>It has been proven that neural networks with one hidden layer are universal approximators</li> <li>In practice, the loss function is non-convex with respect to the weights, it is difficult to optimize</li> <li>Thus modern networks are often arranged in very deep networks</li> </ul>"},{"location":"note/Computer_Vision/Artificial_Neural_Networks/#traditional-ml-vs-deep-learning-workflow","title":"Traditional ML vs Deep Learning Workflow","text":""},{"location":"note/Computer_Vision/Artificial_Neural_Networks/#neuronnodes","title":"Neuron/nodes","text":"\\[a = f(w_{0} + \\sum_{i=1}^{n}{w_{i}x_{i}})\\]"},{"location":"note/Computer_Vision/CV_NLP/","title":"CV + NLP","text":""},{"location":"note/Computer_Vision/CV_NLP/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/CV_NLP/#multimodality","title":"Multimodality","text":""},{"location":"note/Computer_Vision/CV_NLP/#challenges","title":"Challenges","text":"<ol> <li>Representation: explores techniques to effectively summarize multimodal data, capturing the intricate connections among individual modality elements</li> <li>Alignment: focuses on identifying connections and interactions across all elements</li> <li>One modality may dominate others</li> <li>Additional modalities can introduce noise</li> <li>Full coverage over all modalities is not guaranteed</li> <li>Different modalities can have complicated relationships</li> </ol>"},{"location":"note/Computer_Vision/CV_NLP/#paradigm","title":"Paradigm","text":"<ol> <li>Pre-training the model with extensive training data</li> <li>Fine-tuning the pre-trained model with task-specific data</li> <li>Utilizing the trained model for downstream tasks such as classification</li> </ol>"},{"location":"note/Computer_Vision/CV_NLP/#tasks","title":"Tasks","text":"<ol> <li>Visual Question Answering (VQA) &amp; Visual Reasoning</li> </ol> <p> - In general, both VQA and Visual Reasoning are treated as VQA tasks - Popular models: BLIP-VQA, Deplot, VLIT</p> <p>Visual Question Answering (VQA): - Input: An image-question pair - Output:      - Multiple-choice setting: A label corresponding to the correct answer among pre-defined choices     - Open-ended setting: A free-form natural language answer based on the image and question - Task: Answer questions about images (Most VQA models treat as a classification problem with pre-defined answers)</p> <p>Visual Reasoning: - Input:     - VQA: Image-question pairs.     - Matching: Images and text statements.     - Entailment: Image and text pair (potentially with multiple statements).     - Sub-question: Image and a primary question with additional perception-related sub-questions. - Output:     - VQA: Answers to questions about the image.     - Matching: True/False for whether the text is true about the image(s).     - Entailment: Prediction of whether the image semantically entails the text.     - Sub-question: Answers to the sub-questions related to perception. - Task: Performs various reasoning tasks on images</p> <ol> <li>Document Visual Question Answering (DocVQA)</li> </ol> <p> - Popular models: LayoutLM, Donut, Nougat - Input:     - Document image: A scanned or digital image of a document, containing text, layout, and visual elements     - Question about the document: A natural language question posed in text format - Output: Answer to the question: A text response that directly addresses the query and accurately reflects the information found in the document - Task:      - Analyze and understand: The DocVQA model must process both the visual and textual information within the document to fully comprehend its content     - Reason and infer: The model needs to establish relationships between visual elements, text, and the question to draw relevant conclusions     - Generate a natural language answer: The model must produce a clear, concise, and accurate answer to the question in natural language text format</p> <ol> <li>Image Captioning</li> </ol> <p></p> <ul> <li>Popular models: ViT-GPT2, BLIP-Image-Captioning, git-base</li> <li>Input:     - Image     - Pre-trained image feature extractor (optional)</li> <li>Output: Textual captions: single sentence or paragraph that accurately describe the content of the input images, capturing objects, actions, relationships, and overall context</li> <li>Task: To automatically generate natural language descriptions of images     1. Understanding the visual content of the image (objects, actions, relationships)     2. Encoding this information into a meaningful representation     3. Decoding this representation into a coherent, gramatically correct, and informative sentence or phrase</li> </ul> <ol> <li>Image-Text Retrieval</li> </ol> <p></p> <ul> <li>Popular models: CLIP</li> <li>Input:      - Images     - Text: Natural language text, usually in the form of captions, descriptions, or keywords associated with images</li> <li>Output:     - Relevant images: When a text query is given, the system returns a ranked list of images most relevant to the text     - Relevant text: When an image query is given, the system returns a ranked list of text descriptions or captions that best describes the image</li> <li>Task:     - Image-to-text retrieval: Given an image as input, retrieve text descriptions or captions that accurately describe its content     - Text-to-image retrieval: Given a text query, retrieve images that visually match concepts and entities mentioned in the text</li> </ul> <ol> <li>Visual Grounding</li> </ol> <p></p> <ul> <li>Popular models: OWL-ViT, Grounding DINO</li> <li>Input:      - Image: A visual representation of a scene or object     - Natural language query: A text description or question that refers to a specific part of the image</li> <li>Output: Bounding box or segmentation mask</li> <li>Task: Locating the relevant object or region</li> </ul> <ol> <li>Text-to-Image Generation</li> </ol> <p></p> <p>Auto-regressive Models: - Treat the task like translating text descriptions into sequences of image tokens - Image tokens created by image tokenizers like VQ-VAE, represent basic image features - Uses an encoder-decoder architecture:     - Encoder: extracts information from the text prompt     - Decoder: guided by this information, predicts one image token at a time, gradually building the final image pixel by pixel - Allows for high control and detail, but faces challenges in handling long, complex prompts and can be slower than alternative methods like diffusion models</p> <p>Stable Diffusion Models: - Uses \"Latent Diffusion\" technique, where it build images from noise by progressively denoising it, guided by a text prompt and a frozen CLIP text encoder - Its light architecture with a UNet backbone and CLIP encoder allows for GPU-powered image generation, while its latent focus reduces memory consumption</p>"},{"location":"note/Computer_Vision/CV_NLP/#visual-language-models-vlms","title":"Visual Language Models (VLMs)","text":""},{"location":"note/Computer_Vision/CV_NLP/#vision-language-pretrained-models","title":"Vision-Language Pretrained Models","text":""},{"location":"note/Computer_Vision/CV_NLP/#mechanism","title":"Mechanism","text":"<p>Given image-text pairs: - Extract image and text features using text and image encoders - Learn the vision-language correlation with certain pre-training objectives (divided into 3 groups):     1. Contrastive objectives: to learn discriminative representations by pulling paired samples close and pushing others faraway in the embedding space      2. Generative objectives: to learn semantic features by training networks to generate image/text data     3. Alignment objectives: align the image-text pair via global image-text matching or local region-word matching on embedding space - With the learned vision-language correlation, VLMs can be evaluated on unseen data in a zero-shot manner</p>"},{"location":"note/Computer_Vision/CV_NLP/#strategies","title":"Strategies","text":"<ul> <li>Translating images into embedding features that can be jointly trained with token embeddings     - Images are divided into multiple smaller patches and each patch is treated as one token in the input sequence     - e.g. VisualBERT, SimVLM</li> <li>Learning good image embeddings that can work as a prefix for a frozen, pre-trained language model     - Don't change the language model parameters, instead learn an embedding space for images, such that it is compatible with the language model     - e.g. Frozen, ClipCap</li> <li>Using a specially designed cross-attention mechanism to fuse visual information into layers of the language model     - e.g. VisualGPT</li> <li>Combine vision and language models without any training     - e.g. MAGiC</li> </ul>"},{"location":"note/Computer_Vision/CV_NLP/#evaluation","title":"Evaluation","text":"<ol> <li>Zero-shot prediction     - Directly apply pre-trained VLMs to downstream tasks without any task-specific fine-tuning</li> <li>Linear probing     - Freeze the pre-trained VLM and train a linear classifier to classify the VLM-encoded embeddings to measure its representation</li> </ol>"},{"location":"note/Computer_Vision/CV_NLP/#contrastive-loss","title":"Contrastive Loss","text":"<p>\\(\\(L = 1[y_i = y_j]||x_i - x_j||^2 + 1[y_i \\neq y_j] \\max{(0, \\epsilon - ||x_i - x_j||^2)}\\)\\) - If the samples are similar (\\(y_i = y_j\\)), then we minimize the term (\\(||x_i - x_j||^2\\)) that corresponds to their Euclidean distance - If the samples are dissimilar (\\(y_i \\neq y_j\\)), then we minimize the term (\\(\\max{(0, \\epsilon - ||x_i - x_j||^2)}\\)), that is equivalent to maximizing their Euclidean distance until some limit \\(\\epsilon\\)</p>"},{"location":"note/Computer_Vision/CV_NLP/#models","title":"Models","text":""},{"location":"note/Computer_Vision/CV_NLP/#contrastive-language-image-pre-training-clip","title":"Contrastive Language-Image Pre-training (CLIP)","text":"<p>Learning Transferable Visual Models From Natural Language Supervision </p> <ul> <li>Unimodal: pretrains and makes use of image and text features independently</li> <li>Utilizes an image-text contrastive object, to capture rich vision-language correspondence knowledge, enabling zero-shot predictions</li> <li>Outperformed ImageNet models on out-of-distribution tasks</li> <li>Unlikely to outperform a specialized, fine-tuned model</li> <li>Use cases:      - Zero-shot image classification     - Similarity search     - Diffusion models conditioning</li> </ul>"},{"location":"note/Computer_Vision/CV_NLP/#bootstraping-language-image-pre-training-blip","title":"Bootstraping Language-Image Pre-training (BLIP)","text":"<p>BLIP: Bootstraping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</p> <p></p> <p>Architecture: 1. ViT: Plain vision transformer featuring self-attention, feed-forward blocks, and a [CLS] token for embedding representation 2. Unimodal Text Encoder: Resembling BERT's architecture, it uses a [CLS] token for embedding and employs contrastive loss like CLIP, for aligning image and text representation 3. Image-Grounded Text Encoder: Substitutes the [CLS] with an [Encode] token. Cross-attention layers enable the integration of image and text embeddings, creating a multimodal representation. It employes a linear layer to assess the congruence of image-text pairs 4. Image-Grounded Text Decoder: Replacing the bidirectional self-attention with causal self-attention, this decoder is trained via cross-entropy loss in an autoregressive manner for tasks like caption generation or answering visual questions</p> <p>Pre-training Objective: 1. Image-Text Contrastive (ITC) Loss: similar to CLIP, the encoders are trained to generate similar representations for similar image and text pairs and different representations for negative input pairs 2. Image-Text Matching (ITM) Loss: helps the text encoder to learn multimodal representation that captures the fine-grained alignment between vision and language. This is a binary classification task where the loss outputs 1 for a positive image-text pair and 0 for a negative pair 3. Language Modeling (LM) Loss: helps the text decoder generate text descriptions for the corresponding input image</p> <p>CapFilt: Caption &amp; Filtering: 1. Pretrain with noisy web data (alt-texts often don't accurately describe the visual content of the images) +  small but accurate human labeled data 2. Fine-tune ITC, ITM and LM back with human labeled data 3. Generate newer captions using LM on web data, filter using ITM and use them again to train your end-to-end network (containing ITC, ITM and LM)</p>"},{"location":"note/Computer_Vision/CV_NLP/#blip-2","title":"BLIP-2","text":"<p>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models - Can we take off-the-shelf pretrained frozen image encoders and frozen LLMs and use them for vision language pretraining while still preserving their learned representations? - BLIP-2 solves this problem by introducing a Query Transformer that helps generate the most informative visual representation corresponding to a text caption (from the frozen image encoder), which then is fed to a frozen LLM to decode accurate text descriptions</p> <p>Stage 1: Vision-Language Representation Learning 1. An input image is passed through a frozen image encoder (any pretrained vision transformer) 2. Q-Former interacts with the frozen image encoder and generates visual representations that is the most relevant with the input text. Q-former consists of a image transformer and a text transformer are trained using the same set of objectives (ITC, ITM, LM losses) as in BLIP</p> <p></p> <p>Stage 2: Bootstrap Vision-to-Language Generative Learning from a Frozen LLM: 1. Input: image; Output: textual description of the image 2. Input image is passed through the frozen image encoder, followed by Q-Former's image transformer to spit out the output visual represenation \\(z\\) 3. Output image embedding is projected to a dimension same as the text embedding of LLM 4. The projected query embeddings are then prepended to the input text embeddings. They function as soft visual prompts that condition the LLM on visual representation extracted by the Q-Former 5. Two kinds of LLMs are being trained:     - Decoder based: to generate text conditioned on \\(z\\) (uses language modeling loss)     - Encoder-decoder based: visual embedding is prepended to the text embedding and then conditioned to generate text (using a prefix language modeling loss)</p> <p></p>"},{"location":"note/Computer_Vision/CV_NLP/#multimodal-object-detection-owl-vit","title":"Multimodal Object Detection (OWL-ViT)","text":"<p>Simple Open-Vocabulary Object Detection with Vision Transformers - Employes a linear projection of each output token to obtain per-object image embeddings - Box coordinates are derived from token represenations through a small MLP - After fine-tuning, OWL-ViT excels in open-vocabulary object detection </p> <p></p>"},{"location":"note/Computer_Vision/CV_NLP/#tr-ocr","title":"Tr-OCR","text":"<p>TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</p> <p>Workflow: 1. We take an input and resize it to a fixed height and width. 2. Then, we divide the image into a set of patches. 3. We then flatten the patches and fetch embeddings corresponding to each patch. 4. We combine the patch embeddings with position embeddings and pass them through an encoder. 5. The key and value vectors of the encoder are fed into the crossattention of the decoder to fetch the outputs in the final layer.</p>"},{"location":"note/Computer_Vision/CV_NLP/#layoutlm","title":"LayoutLM","text":"<p>LayoutLM: Pre-training of Text and Layout for Document Image Understanding</p> <p></p> <p>Document layout analysis: build a single model that is able to assign a value corresponding to each text within the document image.</p> <p>Workflow: 1. We take an image of the document and extract the various words and their bounding-box coordinates (x0, x1, y0, and y1) \u2013 this is doneusing tools that help in OCR where they provide not only the text but also the bounding box in which the text is present in the document. 2. We take the position embeddings corresponding to these bounding-box coordinates \u2013 position embeddings are calculated based on the bounding-box coordinates. 3. We add the embeddings corresponding to the various texts extracted (text embeddings in the above picture) where we pass the text through a tokenizer, which in turn is passed through a pre-trained Bidirectional Encoder Representation of Transformers (BERT)-like model. 4. During the training phase of the pre-trained LayoutLM, we randomly mask certain words (but not the position embeddings of those words) and predict the masked words given the context (surrounding words and their corresponding position embeddings). 5. Once the pre-trained LayoutLM model is fine-tuned, we extract the embeddings corresponding to each word by summing up the text embeddings of the word with the position embeddings corresponding to the word. 6. Next, we leverage Faster R-CNN to obtain the image embedding corresponding to the location of the word. We leverage image embedding so that we obtain key information regarding the text style (for example, bold, italics, or underlined) that is not available with OCR. 7. Finally, we perform the downstream task of extracting the keys and values corresponding to the image. In the case of document key value extraction, it translates to the task of named entity recognition, where each output word is classified as one of the possible keys or a value associated with a key.</p>"},{"location":"note/Computer_Vision/CV_NLP/#layoutlmv3","title":"LayoutLMv3","text":"<p>LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</p> <p></p> <p>Workflow: 1. Words are obtained from an image using a typical OCR parser. 2. The words are then converted into embeddings using the RoBERTa model. 3. The document is resized into a fixed shape and then converted into multiple patches. 4. Each patch is flattened and passed through a linear layer to obtain embeddings corresponding to the patch. 5. The 1D position embeddings correspond to the index of the word/patch while the 2D position embeddings correspond to thebounding box/segment. 6. Once the embeddings are in place, we perform masked pre-training (MLM Head) in a manner similar to that of LayoutLM, where we mask certain words and predict them using the context. Similarly in masked image modeling (MIM Head), we mask certain blocks and predict the tokens within the block. 7. Word patch alignment (WPA Head) is then performed, which refers to the task of predicting whether a masked image patch has the corresponding tokens masked. If a token is masked and the corresponding image patch is masked, it is aligned; it is unaligned if one of these is masked and the other isn\u2019t</p>"},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/","title":"Layoutlmv3","text":"In\u00a0[30]: Copied! <pre>import torch\nimport numpy as np\nfrom transformers import AutoProcessor\n\nfrom datasets import load_dataset, Features, Sequence, ClassLabel, Value, Array2D, Array3D\nimport evaluate\n</pre> import torch import numpy as np from transformers import AutoProcessor  from datasets import load_dataset, Features, Sequence, ClassLabel, Value, Array2D, Array3D import evaluate In\u00a0[2]: Copied! <pre>dataset = load_dataset(\"sizhkhy/passports\")\nexamples_train = dataset[\"train\"]\nexamples_eval = dataset[\"valid\"]\n</pre> dataset = load_dataset(\"sizhkhy/passports\") examples_train = dataset[\"train\"] examples_eval = dataset[\"valid\"] In\u00a0[3]: Copied! <pre>def flatten(lists):\n    return [y for x in lists for y in x]\n\nid2label = {i:v for i, v in set(list(zip(flatten(examples_train[\"labels\"]), flatten(examples_train[\"label_string\"]))))}\nlabel2id = {v:i for i, v in id2label.items()}\n</pre> def flatten(lists):     return [y for x in lists for y in x]  id2label = {i:v for i, v in set(list(zip(flatten(examples_train[\"labels\"]), flatten(examples_train[\"label_string\"]))))} label2id = {v:i for i, v in id2label.items()} In\u00a0[4]: Copied! <pre>examples_train.column_names\n</pre> examples_train.column_names Out[4]: <pre>['image', 'label_string', 'words', 'labels', 'boxes']</pre> In\u00a0[5]: Copied! <pre>processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n\ndef prepare_examples(examples):\n    images = examples['image']\n    words = examples['words']\n    boxes = examples['boxes']\n    word_labels = examples['labels']\n    encoding = processor(\n        images,\n        words,\n        boxes=boxes,\n        word_labels=word_labels,\n        # return_tensors=\"pt\",\n        padding=\"max_length\",\n        # max_length=512,\n        truncation=True,\n    )\n    return encoding\n</pre> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)  def prepare_examples(examples):     images = examples['image']     words = examples['words']     boxes = examples['boxes']     word_labels = examples['labels']     encoding = processor(         images,         words,         boxes=boxes,         word_labels=word_labels,         # return_tensors=\"pt\",         padding=\"max_length\",         # max_length=512,         truncation=True,     )     return encoding In\u00a0[6]: Copied! <pre>train_dataset = examples_train.map(\n    prepare_examples, \n    batched=True,\n    remove_columns=examples_train.column_names\n)\n\neval_dataset = examples_eval.map(\n    prepare_examples, \n    batched=True,\n    remove_columns=examples_eval.column_names\n)\n</pre> train_dataset = examples_train.map(     prepare_examples,      batched=True,     remove_columns=examples_train.column_names )  eval_dataset = examples_eval.map(     prepare_examples,      batched=True,     remove_columns=examples_eval.column_names ) In\u00a0[7]: Copied! <pre>example = train_dataset[0]\nprocessor.tokenizer.decode(example[\"input_ids\"])\n</pre> example = train_dataset[0] processor.tokenizer.decode(example[\"input_ids\"]) Out[7]: <pre>\"&lt;s&gt; BELGIE BELGIQUE BELGIEN BELGIUM Type / Type Typ / Type Land van algete / Pays \u00e9rinttour Ausstellungsland / issuing country Paspoortiummer / N' du passsport Pass Nur Pasaport no EH100396 P 1.Naam / Noms Name / Surname BEL SPECIMEN 2. Vocmaman / Pr\u00e9noms Vornamen / Given names BARBARA 3. Nationastell / Nationalit\u00e9 Staatsangeh\u00f6rigkeit / Nanonatry BELG 4. Geboortedatum / Date de naissance 6. Geboorteplaats/ Lieu de naissance Geburtsdatum / Date of birth Geburtsort I Place of birth LILLE 06 11 99 B. Geslacht / Seve Gegrzyecht / Sex V-F 7. Datum van afgifte / Dabe de d\u00e9livrance Ausstellungsdatum / Date of issue 04 06 08 # Geldig tout / Date d'expiration Gullig bis / Dato of expiry 03 06 13 Beh\u00f6rde / Authority UTOPIA to: Handtekening van de houder Signature du titulaire Unterschrift des Passinhabers Holder's signattire perimen P&lt;BELSPECIMEN &lt;&lt; BARBARA &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; EH100396&lt;4BEL9911064F1306031 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 00&lt;/s&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\"</pre> In\u00a0[8]: Copied! <pre>train_dataset.set_format(\"torch\")\n</pre> train_dataset.set_format(\"torch\") In\u00a0[9]: Copied! <pre>example = train_dataset[0]\nfor k, v in example.items():\n    print(k, v)\n</pre> example = train_dataset[0] for k, v in example.items():     print(k, v) <pre>labels tensor([-100,    5, -100, -100,    5, -100, -100, -100,    5, -100, -100,    5,\n        -100, -100,    5,    5,    5,    5,    5,    5,    5,    5,    5, -100,\n        -100,    5,    5, -100,    5, -100, -100, -100,    5, -100, -100, -100,\n        -100, -100,    5,    5,    5,    5, -100, -100, -100, -100,    5,    5,\n        -100,    5,    5, -100, -100,    5,    5,    5, -100, -100,    5,   11,\n        -100, -100, -100,    5,    5, -100, -100, -100,    5,    5, -100,    5,\n           5,    5, -100, -100,    5,    2, -100, -100,    5, -100,    5, -100,\n        -100,    5,    5, -100, -100,    5, -100, -100,    5,    5,    5,   13,\n        -100, -100,    5, -100,    5, -100, -100,    5,    5, -100,    5, -100,\n        -100, -100, -100, -100, -100, -100,    5,    5, -100, -100, -100,    5,\n        -100,    5, -100,    5, -100, -100, -100, -100,    5,    5,    5,    5,\n        -100,    5, -100,    5, -100, -100, -100, -100, -100, -100, -100, -100,\n           5, -100,    5,    5, -100,    5, -100, -100, -100, -100, -100,    5,\n           5,    5,    5,    5, -100, -100, -100, -100,    5,    5,    5,    5,\n           5, -100,    0,    3,    3,    5, -100,    5, -100, -100,    5,    5,\n        -100,    5, -100, -100, -100, -100, -100,    5,    5,   12, -100, -100,\n           5, -100,    5, -100,    5,    5, -100, -100,    5,    5, -100,    5,\n           5, -100, -100,    5, -100, -100, -100, -100, -100, -100,    5,    5,\n           5,    5,    6,    7,    7,    5,    5, -100, -100,    5, -100,    5,\n           5,    5, -100, -100, -100,    5, -100, -100,    5,    5,    5, -100,\n           5,    5, -100,   14,    8,    8,    5, -100, -100,    5,    5,    5,\n        -100, -100,    5, -100,    5, -100, -100,    5,    5,    5, -100, -100,\n           5,    5,    5, -100, -100,    5, -100, -100, -100,    5,    5, -100,\n        -100, -100,    5, -100,    5, -100, -100,    5, -100,    5, -100, -100,\n        -100, -100, -100, -100,    5,    5, -100, -100,    5, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100,    5, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    5, -100,\n        -100, -100, -100, -100, -100,    5, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100])\ninput_ids tensor([    0, 36125,   534,  7720, 36125, 21525,  1864,  9162, 36125, 21525,\n         2796, 36125, 21525,  5725,  7773,  1589,  7773, 27533,  1589,  7773,\n         3192,  3538,  1076,  6460,   242,  1589,   221,  4113,  7935, 39907,\n           90,  2126, 16675,   620,  1641,  1545,    29,  1245,  1589, 10392,\n          247,   221,  9331,   139, 48629,  2089,  1589,   234,   108,  4279,\n         1323,    29,  3427,  7124, 17474, 11920,  1115,  2723,   117,   381,\n          725,  1866, 33275,   221,   112,     4, 30612,   424,  1589,   234,\n         6806, 10704,  1589,   208,  8629,  4344, 36125, 44921,  3755,  2796,\n          132,     4, 36413,   119,  7243,  1589,  2869, 10598,  6806,   468,\n         4244, 21228,  1589,  6211,  2523, 26676,   387, 19142,   155,     4,\n         5857,  1988,  1641,  1589,   496, 21193, 20876,  2923, 10987,   298,\n         3671,  7638,  1071,   405,  1589, 13690,   261,   415,  1506, 36125,\n          534,   204,     4,   272,  3209,   139, 25654, 15368,  1589, 10566,\n          263,  2750, 44096,   231,     4,   272,  3209,   139,  2723,   242,\n         2911,   102,  2923,    73, 17974,   257,   263,  2750, 44096,   272,\n         3209,   710,  1872, 36146,   783,  1589, 10566,     9,  3113,   272,\n         3209,   710,  1872,  2723,    38,  6067,     9,  3113,   226, 35025,\n        15007,   365,  6705,   163,     4, 17981,   462, 11629,  1589,  1608,\n          548,   272,  3733,   338,  5144,  7529,    90,  1589, 15516,   468,\n           12,   597,   262,     4, 16673,   783,  3538,  9724, 47324,   859,\n         1589,   211,  5084,   263, 10768, 31332, 16557, 16675,   620,  1641,\n         1545,    29, 36146,   783,  1589, 10566,     9,   696, 14722, 15007,\n        12112,   849,  4177,  4779,  1023,   326,   995,  1589, 10566,   385,\n          108,  3463, 41678,   272,  5023,  1023, 20229,  1589,   211,  3938,\n            9,  4553, 12708, 15171, 15007,   508, 18775, 17920,  2794,  1589,\n         4305, 14019,  5733,  2889,     7,    35,  7406, 28205,  4226,  3538,\n          263,  1368,  1438,  3624, 30072,  4279, 13515,  5571,  1885,  1890,\n         2696,   611, 23203,  2694,  7124,   179, 13899,   268, 20198,    18,\n         1203,  2611,  1885,   228, 34620,   221, 41552,   387,  3721, 45352,\n         3755,  2796, 48188, 26676,   387, 19142, 48188, 48203, 48203, 48203,\n        48203, 48203, 48203, 48203, 48203, 48203, 48203,   381,   725,  1866,\n        33275, 41552,   306,   387,  3721,  2831, 11670,  4027,   597, 11343,\n         2466,  2983, 48188, 48203, 48203, 48203, 48203, 48203, 48203, 16273,\n            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1])\nattention_mask tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\nbbox tensor([[  0,   0,   0,   0],\n        [148,  37, 268,  80],\n        [148,  37, 268,  80],\n        ...,\n        [  0,   0,   0,   0],\n        [  0,   0,   0,   0],\n        [  0,   0,   0,   0]])\npixel_values tensor([[[ 0.7647,  0.6863,  0.3725,  ...,  0.0353, -0.6235, -0.6627],\n         [ 0.7647,  0.6863,  0.3882,  ..., -0.0588, -0.6471, -0.6549],\n         [ 0.7490,  0.6941,  0.4196,  ...,  0.0353, -0.6627, -0.6471],\n         ...,\n         [ 0.7804,  0.7725,  0.7569,  ...,  0.8510,  0.8588,  0.5686],\n         [ 0.7725,  0.7804,  0.7647,  ...,  0.8588,  0.8588,  0.5686],\n         [ 0.7725,  0.7804,  0.7647,  ...,  0.8588,  0.8510,  0.5686]],\n\n        [[ 0.8039,  0.7333,  0.4353,  ...,  0.0902, -0.5765, -0.6392],\n         [ 0.8039,  0.7333,  0.4510,  ..., -0.0039, -0.6000, -0.6314],\n         [ 0.7725,  0.7333,  0.4824,  ...,  0.0902, -0.6157, -0.6235],\n         ...,\n         [ 0.8039,  0.7961,  0.7882,  ...,  0.8510,  0.8588,  0.5686],\n         [ 0.7961,  0.8039,  0.7882,  ...,  0.8588,  0.8588,  0.5686],\n         [ 0.7961,  0.8039,  0.7882,  ...,  0.8588,  0.8510,  0.5686]],\n\n        [[ 0.8275,  0.7098,  0.3412,  ..., -0.0196, -0.6863, -0.7333],\n         [ 0.8275,  0.7098,  0.3569,  ..., -0.1216, -0.7098, -0.7255],\n         [ 0.8118,  0.7098,  0.3882,  ..., -0.0275, -0.7255, -0.7176],\n         ...,\n         [ 0.8588,  0.8353,  0.8039,  ...,  0.8510,  0.8588,  0.5686],\n         [ 0.8667,  0.8510,  0.8275,  ...,  0.8588,  0.8588,  0.5765],\n         [ 0.8667,  0.8667,  0.8353,  ...,  0.8667,  0.8667,  0.5843]]])\n</pre> In\u00a0[10]: Copied! <pre>for id, label in zip(train_dataset[0][\"input_ids\"], train_dataset[0][\"labels\"]):\n    print(processor.tokenizer.decode(id), label.item())\n</pre> for id, label in zip(train_dataset[0][\"input_ids\"], train_dataset[0][\"labels\"]):     print(processor.tokenizer.decode(id), label.item()) <pre>&lt;s&gt; -100\n BEL 5\nG -100\nIE -100\n BEL 5\nGI -100\nQ -100\nUE -100\n BEL 5\nGI -100\nEN -100\n BEL 5\nGI -100\nUM -100\n Type 5\n / 5\n Type 5\n Typ 5\n / 5\n Type 5\n Land 5\n van 5\n al 5\nget -100\ne -100\n / 5\n P 5\nays -100\n \u00e9 5\nrint -100\nt -100\nour -100\n Aus 5\nst -100\nell -100\nung -100\ns -100\nland -100\n / 5\n issuing 5\n country 5\n P 5\nasp -100\no -100\nortium -100\nmer -100\n / 5\n N 5\n' -100\n du 5\n pass 5\ns -100\nport -100\n Pass 5\n Nur 5\n Pas 5\nap -100\nort -100\n no 5\n E 11\nH -100\n100 -100\n396 -100\n P 5\n 1 5\n. -100\nNa -100\nam -100\n / 5\n N 5\noms -100\n Name 5\n / 5\n S 5\nurn -100\name -100\n BEL 5\n SPEC 2\nIM -100\nEN -100\n 2 5\n. -100\n Voc 5\nm -100\naman -100\n / 5\n Pr 5\n\u00e9n -100\noms -100\n V 5\norn -100\namen -100\n / 5\n Given 5\n names 5\n BAR 13\nB -100\nARA -100\n 3 5\n. -100\n Nation 5\nast -100\nell -100\n / 5\n National 5\nit\u00e9 -100\n Sta 5\nats -100\nange -100\nh -100\n\u00f6 -100\nrig -100\nke -100\nit -100\n / 5\n Nan 5\non -100\nat -100\nry -100\n BEL 5\nG -100\n 4 5\n. -100\n G 5\neb -100\no -100\norted -100\natum -100\n / 5\n Date 5\n de 5\n na 5\nissance -100\n 6 5\n. -100\n G 5\neb -100\no -100\nort -100\ne -100\npl -100\na -100\nats -100\n/ -100\n Lie 5\nu -100\n de 5\n na 5\nissance -100\n G 5\neb -100\nur -100\nts -100\ndat -100\num -100\n / 5\n Date 5\n of 5\n birth 5\n G 5\neb -100\nur -100\nts -100\nort -100\n I 5\n Place 5\n of 5\n birth 5\n L 5\nILLE -100\n 06 0\n 11 3\n 99 3\n B 5\n. -100\n Ges 5\nl -100\nacht -100\n / 5\n Se 5\nve -100\n G 5\neg -100\nr -100\nzy -100\nech -100\nt -100\n / 5\n Sex 5\n V 12\n- -100\nF -100\n 7 5\n. -100\n Dat 5\num -100\n van 5\n af 5\ngif -100\nte -100\n / 5\n D 5\nabe -100\n de 5\n d\u00e9 5\nliv -100\nrance -100\n Aus 5\nst -100\nell -100\nung -100\ns -100\ndat -100\num -100\n / 5\n Date 5\n of 5\n issue 5\n 04 6\n 06 7\n 08 7\n # 5\n Ge 5\nld -100\nig -100\n t 5\nout -100\n / 5\n Date 5\n d 5\n' -100\nex -100\npiration -100\n G 5\null -100\nig -100\n bis 5\n / 5\n D 5\nato -100\n of 5\n exp 5\niry -100\n 03 14\n 06 8\n 13 8\n Beh 5\n\u00f6r -100\nde -100\n / 5\n Authority 5\n UT 5\nOP -100\nIA -100\n to 5\n: -100\n Hand 5\ntek -100\nening -100\n van 5\n de 5\n h 5\nou -100\nder -100\n Signature 5\n du 5\n tit 5\nula -100\nire -100\n Un 5\nters -100\nch -100\nrift -100\n des 5\n Pass 5\nin -100\nhab -100\ners -100\n Holder 5\n's -100\n sign 5\natt -100\nire -100\n per 5\nimen -100\n P 5\n&lt; -100\nB -100\nEL -100\nSPEC -100\nIM -100\nEN -100\n &lt;&lt; 5\n BAR 5\nB -100\nARA -100\n &lt;&lt; 5\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n E 5\nH -100\n100 -100\n396 -100\n&lt; -100\n4 -100\nB -100\nEL -100\n99 -100\n110 -100\n64 -100\nF -100\n130 -100\n60 -100\n31 -100\n &lt;&lt; 5\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n&lt;&lt; -100\n 00 5\n&lt;/s&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n&lt;pad&gt; -100\n</pre> In\u00a0[11]: Copied! <pre>metric = evaluate.load(\"seqeval\")\n</pre> metric = evaluate.load(\"seqeval\") In\u00a0[32]: Copied! <pre>return_entity_level_metrics = False\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    if return_entity_level_metrics:\n        # Unpack nested dictionaries\n        final_results = {}\n        for key, value in results.items():\n            if isinstance(value, dict):\n                for n, v in value.items():\n                    final_results[f\"{key}_{n}\"] = v\n            else:\n                final_results[key] = value\n        return final_results\n    else:\n        return {\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"f1\": results[\"overall_f1\"],\n            \"accuracy\": results[\"overall_accuracy\"],\n        }\n</pre> return_entity_level_metrics = False  def compute_metrics(p):     predictions, labels = p     predictions = np.argmax(predictions, axis=2)      # Remove ignored index (special tokens)     true_predictions = [         [id2label[p] for (p, l) in zip(prediction, label) if l != -100]         for prediction, label in zip(predictions, labels)     ]     true_labels = [         [id2label[l] for (p, l) in zip(prediction, label) if l != -100]         for prediction, label in zip(predictions, labels)     ]      results = metric.compute(predictions=true_predictions, references=true_labels)     if return_entity_level_metrics:         # Unpack nested dictionaries         final_results = {}         for key, value in results.items():             if isinstance(value, dict):                 for n, v in value.items():                     final_results[f\"{key}_{n}\"] = v             else:                 final_results[key] = value         return final_results     else:         return {             \"precision\": results[\"overall_precision\"],             \"recall\": results[\"overall_recall\"],             \"f1\": results[\"overall_f1\"],             \"accuracy\": results[\"overall_accuracy\"],         } In\u00a0[33]: Copied! <pre>from transformers import LayoutLMv3ForTokenClassification\n\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\n    \"microsoft/layoutlmv3-base\", \n    id2label=id2label,\n    label2id=label2id\n)\n</pre> from transformers import LayoutLMv3ForTokenClassification  model = LayoutLMv3ForTokenClassification.from_pretrained(     \"microsoft/layoutlmv3-base\",      id2label=id2label,     label2id=label2id ) <pre>Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[34]: Copied! <pre>from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./test\",\n    max_steps=1000,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    learning_rate=1e-5,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n)\n</pre> from transformers import TrainingArguments, Trainer  training_args = TrainingArguments(     output_dir=\"./test\",     max_steps=1000,     per_device_train_batch_size=2,     per_device_eval_batch_size=2,     learning_rate=1e-5,     eval_strategy=\"steps\",     eval_steps=100,     load_best_model_at_end=True,     metric_for_best_model=\"f1\" ) In\u00a0[35]: Copied! <pre>from transformers.data.data_collator import default_data_collator\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=processor,\n    data_collator=default_data_collator,\n    compute_metrics=compute_metrics\n)\n</pre> from transformers.data.data_collator import default_data_collator  trainer = Trainer(     model=model,     args=training_args,     train_dataset=train_dataset,     eval_dataset=eval_dataset,     tokenizer=processor,     data_collator=default_data_collator,     compute_metrics=compute_metrics ) <pre>max_steps is given, it will override any value given in num_train_epochs\n</pre> In\u00a0[36]: Copied! <pre>trainer.train()\n</pre> trainer.train() <pre>  0%|          | 0/1000 [00:00&lt;?, ?it/s]</pre> <pre>c:\\Users\\junwa\\anaconda3\\envs\\wise\\Lib\\site-packages\\transformers\\modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>c:\\Users\\junwa\\anaconda3\\envs\\wise\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> <pre>{'eval_loss': 0.4732014238834381, 'eval_precision': 0.14583333333333334, 'eval_recall': 0.1111111111111111, 'eval_f1': 0.12612612612612614, 'eval_accuracy': 0.8735224586288416, 'eval_runtime': 2.6918, 'eval_samples_per_second': 3.343, 'eval_steps_per_second': 1.857, 'epoch': 2.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.3073630630970001, 'eval_precision': 0.3287671232876712, 'eval_recall': 0.38095238095238093, 'eval_f1': 0.35294117647058826, 'eval_accuracy': 0.9243498817966903, 'eval_runtime': 2.5508, 'eval_samples_per_second': 3.528, 'eval_steps_per_second': 1.96, 'epoch': 4.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.22280269861221313, 'eval_precision': 0.4583333333333333, 'eval_recall': 0.5238095238095238, 'eval_f1': 0.4888888888888889, 'eval_accuracy': 0.950354609929078, 'eval_runtime': 2.683, 'eval_samples_per_second': 3.354, 'eval_steps_per_second': 1.864, 'epoch': 6.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.16266614198684692, 'eval_precision': 0.5616438356164384, 'eval_recall': 0.6507936507936508, 'eval_f1': 0.6029411764705883, 'eval_accuracy': 0.9633569739952719, 'eval_runtime': 2.6984, 'eval_samples_per_second': 3.335, 'eval_steps_per_second': 1.853, 'epoch': 8.0}\n{'loss': 0.302, 'grad_norm': 1.8882520198822021, 'learning_rate': 5e-06, 'epoch': 10.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.12085269391536713, 'eval_precision': 0.7384615384615385, 'eval_recall': 0.7619047619047619, 'eval_f1': 0.75, 'eval_accuracy': 0.9787234042553191, 'eval_runtime': 2.7123, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 1.843, 'epoch': 10.0}\n</pre> <pre>c:\\Users\\junwa\\anaconda3\\envs\\wise\\Lib\\site-packages\\transformers\\modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.12393239140510559, 'eval_precision': 0.6617647058823529, 'eval_recall': 0.7142857142857143, 'eval_f1': 0.6870229007633588, 'eval_accuracy': 0.9692671394799054, 'eval_runtime': 2.6355, 'eval_samples_per_second': 3.415, 'eval_steps_per_second': 1.897, 'epoch': 12.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.12492009252309799, 'eval_precision': 0.819672131147541, 'eval_recall': 0.7936507936507936, 'eval_f1': 0.8064516129032259, 'eval_accuracy': 0.9787234042553191, 'eval_runtime': 2.75, 'eval_samples_per_second': 3.273, 'eval_steps_per_second': 1.818, 'epoch': 14.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.09741049259901047, 'eval_precision': 0.7575757575757576, 'eval_recall': 0.7936507936507936, 'eval_f1': 0.7751937984496123, 'eval_accuracy': 0.9787234042553191, 'eval_runtime': 2.712, 'eval_samples_per_second': 3.319, 'eval_steps_per_second': 1.844, 'epoch': 16.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.10345828533172607, 'eval_precision': 0.7936507936507936, 'eval_recall': 0.7936507936507936, 'eval_f1': 0.7936507936507936, 'eval_accuracy': 0.9810874704491725, 'eval_runtime': 2.5972, 'eval_samples_per_second': 3.465, 'eval_steps_per_second': 1.925, 'epoch': 18.0}\n{'loss': 0.0403, 'grad_norm': 0.21430332958698273, 'learning_rate': 0.0, 'epoch': 20.0}\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>{'eval_loss': 0.10525424778461456, 'eval_precision': 0.8225806451612904, 'eval_recall': 0.8095238095238095, 'eval_f1': 0.8160000000000001, 'eval_accuracy': 0.9822695035460993, 'eval_runtime': 2.6935, 'eval_samples_per_second': 3.341, 'eval_steps_per_second': 1.856, 'epoch': 20.0}\n{'train_runtime': 1386.9536, 'train_samples_per_second': 1.442, 'train_steps_per_second': 0.721, 'train_loss': 0.1711606044769287, 'epoch': 20.0}\n</pre> Out[36]: <pre>TrainOutput(global_step=1000, training_loss=0.1711606044769287, metrics={'train_runtime': 1386.9536, 'train_samples_per_second': 1.442, 'train_steps_per_second': 0.721, 'total_flos': 530877081600000.0, 'train_loss': 0.1711606044769287, 'epoch': 20.0})</pre> In\u00a0[37]: Copied! <pre>trainer.evaluate()\n</pre> trainer.evaluate() <pre>c:\\Users\\junwa\\anaconda3\\envs\\wise\\Lib\\site-packages\\transformers\\modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> Out[37]: <pre>{'eval_loss': 0.10525424778461456,\n 'eval_precision': 0.8225806451612904,\n 'eval_recall': 0.8095238095238095,\n 'eval_f1': 0.8160000000000001,\n 'eval_accuracy': 0.9822695035460993,\n 'eval_runtime': 2.8039,\n 'eval_samples_per_second': 3.21,\n 'eval_steps_per_second': 1.783,\n 'epoch': 20.0}</pre> In\u00a0[92]: Copied! <pre>import time\nstart_time = time.time()\n\nidx = 0\nexample = examples_eval[idx]\nprint(example.keys())\n</pre> import time start_time = time.time()  idx = 0 example = examples_eval[idx] print(example.keys()) <pre>dict_keys(['image', 'label_string', 'words', 'labels', 'boxes'])\n</pre> In\u00a0[93]: Copied! <pre>image = example[\"image\"]\nwords = example[\"words\"]\nboxes = example[\"boxes\"]\nword_labels = example[\"labels\"]\n\nencoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\").to(\"cuda\")\nfor k, v in encoding.items():\n    print(k, v.shape)\n</pre> image = example[\"image\"] words = example[\"words\"] boxes = example[\"boxes\"] word_labels = example[\"labels\"]  encoding = processor(image, words, boxes=boxes, word_labels=word_labels, return_tensors=\"pt\").to(\"cuda\") for k, v in encoding.items():     print(k, v.shape) <pre>input_ids torch.Size([1, 304])\nattention_mask torch.Size([1, 304])\nbbox torch.Size([1, 304, 4])\nlabels torch.Size([1, 304])\npixel_values torch.Size([1, 3, 224, 224])\n</pre> In\u00a0[94]: Copied! <pre>with torch.no_grad():\n  outputs = model(**encoding)\n</pre> with torch.no_grad():   outputs = model(**encoding) In\u00a0[95]: Copied! <pre>logits = outputs.logits\nlogits.shape\n</pre> logits = outputs.logits logits.shape Out[95]: <pre>torch.Size([1, 304, 15])</pre> In\u00a0[96]: Copied! <pre>import torch.nn.functional as F\nconfs = F.softmax(logits, dim=-1).squeeze().tolist()\n</pre> import torch.nn.functional as F confs = F.softmax(logits, dim=-1).squeeze().tolist() In\u00a0[97]: Copied! <pre>predictions = logits.argmax(-1).squeeze().tolist()\nconfs = [li[p] for (li,p) in zip(confs, predictions)]\n</pre> predictions = logits.argmax(-1).squeeze().tolist() confs = [li[p] for (li,p) in zip(confs, predictions)] In\u00a0[98]: Copied! <pre>labels = encoding.labels.squeeze().tolist()\n\nend_time = time.time()\ninference_time = end_time - start_time\nprint(f\"Inference time: {inference_time} seconds\")\n</pre> labels = encoding.labels.squeeze().tolist()  end_time = time.time() inference_time = end_time - start_time print(f\"Inference time: {inference_time} seconds\") <pre>Inference time: 0.5045785903930664 seconds\n</pre> In\u00a0[99]: Copied! <pre>def unnormalize_box(bbox, width, height):\n      return [\n          width * (bbox[0] / 1000),\n          height * (bbox[1] / 1000),\n          width * (bbox[2] / 1000),\n          height * (bbox[3] / 1000),\n      ]\ntoken_boxes = encoding.bbox.squeeze().tolist()\nwidth, height = image.size\n\ntrue_predictions = [model.config.id2label[pred] for pred, label in zip(predictions, labels) if label != - 100]\ntrue_labels = [model.config.id2label[label] for prediction, label in zip(predictions, labels) if label != -100]\ntrue_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100]\n</pre> def unnormalize_box(bbox, width, height):       return [           width * (bbox[0] / 1000),           height * (bbox[1] / 1000),           width * (bbox[2] / 1000),           height * (bbox[3] / 1000),       ] token_boxes = encoding.bbox.squeeze().tolist() width, height = image.size  true_predictions = [model.config.id2label[pred] for pred, label in zip(predictions, labels) if label != - 100] true_labels = [model.config.id2label[label] for prediction, label in zip(predictions, labels) if label != -100] true_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100] In\u00a0[100]: Copied! <pre>from PIL import ImageDraw, ImageFont\n\ndraw = ImageDraw.Draw(image)\n\nfont = ImageFont.load_default()\n\ndef iob_to_label(label):\n    label = label[2:]\n    if not label:\n      return 'other'\n    return label\n\nlabel2color = {'passport_number':'blue', 'date_of_birth':'green', 'name':'orange', 'fathers_name':'violet', 'surname': 'red','other':'black', 'sex':'purple', 'date_of_expiry': 'yellow', 'date_of_issue': 'pink'}\n\nfor prediction, box, conf in zip(true_predictions, true_boxes, confs):\n    predicted_label = iob_to_label(prediction).lower()\n    if predicted_label != 'other':\n      draw.rectangle(box, outline=label2color[predicted_label])\n      draw.text((box[0] + 10, box[1] - 10), text=f'{predicted_label} - {conf}', fill=label2color[predicted_label], font=font)\n\nimage\n</pre> from PIL import ImageDraw, ImageFont  draw = ImageDraw.Draw(image)  font = ImageFont.load_default()  def iob_to_label(label):     label = label[2:]     if not label:       return 'other'     return label  label2color = {'passport_number':'blue', 'date_of_birth':'green', 'name':'orange', 'fathers_name':'violet', 'surname': 'red','other':'black', 'sex':'purple', 'date_of_expiry': 'yellow', 'date_of_issue': 'pink'}  for prediction, box, conf in zip(true_predictions, true_boxes, confs):     predicted_label = iob_to_label(prediction).lower()     if predicted_label != 'other':       draw.rectangle(box, outline=label2color[predicted_label])       draw.text((box[0] + 10, box[1] - 10), text=f'{predicted_label} - {conf}', fill=label2color[predicted_label], font=font)  image Out[100]:"},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#prepare-dataset","title":"Prepare dataset\u00b6","text":""},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#metrics","title":"Metrics\u00b6","text":""},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#model","title":"Model\u00b6","text":""},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#train","title":"Train\u00b6","text":""},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#evaluate","title":"Evaluate\u00b6","text":""},{"location":"note/Computer_Vision/CV_NLP/layoutlmv3/#inference","title":"Inference\u00b6","text":""},{"location":"note/Computer_Vision/Generative/","title":"Generative Models","text":""},{"location":"note/Computer_Vision/Generative/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/Generative/#vae","title":"VAE","text":""},{"location":"note/Computer_Vision/Generative/#gan","title":"GAN","text":""},{"location":"note/Computer_Vision/Generative/#diffusion","title":"Diffusion","text":""},{"location":"note/Computer_Vision/Generative/#synthetic-data-creation","title":"Synthetic Data Creation","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/","title":"CNN Models","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#resnet-resnetv2","title":"ResNet &amp; ResNetV2","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#mobilenet","title":"MobileNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications","title":"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications","text":"<ul> <li>Designed for mobile devices</li> <li>Depthwise Separable Convolutions: achieves high accuracy while minimizng computational overhead</li> <li>Channel-wise Linear Bottleneck Layers: help to further reduce the number of parameters and computational cost while maintaining high accuracy</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#depthwise-separable-convolutions","title":"Depthwise Separable Convolutions","text":"<ul> <li>Depthwise Convolution + Pointwise Convolution</li> </ul> <ol> <li>Standard convolution</li> </ol> <ul> <li>Combines the values of all the input channels</li> <li>e.g. 3 channels --&gt; 1 channel per pixel</li> </ul> <ol> <li>Depthwise convolution</li> </ol> <ul> <li>Does not combine the input channels</li> <li>Convolves on each channel separately</li> <li>e.g. 3 channels --&gt; 3 channels</li> </ul> <ol> <li>Pointwise convolution</li> </ol> <ul> <li>Same as a standard convolution, except using a \\(1 \\times 1\\) kernel</li> <li>Adds up the channels from depthwise convolution as a weighted sum</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#channel-wise-linear-bottleneck-layers","title":"Channel-wise Linear Bottleneck Layers","text":"<p>3 main operations applied sequentially:</p> <ol> <li>Depthwise convolution: This step performs a convolution separately for each channel (a single color or feature) in the input image using a small filter (usually 3x3). The output of this step is the same size as the input but with fewer channels</li> <li>Batch normalization: This operation normalizes the activation values across each channel, helping to stabilize the training process and improve generalization performance</li> <li>Activation function: Typically, a ReLU (Rectified Linear Unit) activation function is used after batch normalization to introduce non-linearity in the network</li> </ol>"},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#mobile-former","title":"Mobile-Former","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#mobile-former-bridging-mobilenet-and-transformer","title":"Mobile-Former: Bridging MobileNet and Transformer","text":"<ol> <li>Use MobileNet as a feature extractor, then fed into a transformer model</li> <li>Training MobileNet and ViT separately and then combining their predictions through ensemble techniques</li> </ol>"},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#densenet","title":"DenseNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#squeezenet","title":"SqueezeNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#efficientnet","title":"EfficientNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#resnext","title":"ResNeXt","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#nasnet","title":"NASNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#inceptionnet","title":"InceptionNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#inceptionresnet","title":"InceptionResNet","text":""},{"location":"note/Computer_Vision/Image_Classification/CNN_Models/#convnext","title":"ConvNeXt","text":"<ul> <li>A significatnt improvement to pure convolution models by incorporating techniques inspired by ViTs and achieving results comparable to ViTs in accuracy and scalability</li> <li>TO-DO</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/","title":"Concepts","text":""},{"location":"note/Computer_Vision/Image_Classification/Concepts/#empirical-risk-minimization","title":"Empirical Risk Minimization","text":"<p>We are interested finding a function \\(f\\) that minimized the expected risk:</p> \\[R_{TRUE}(f) = E[\\mathcal{l}(f(x), y)] = \\int \\mathcal{l}(f(x), y) \\, dp(x,y)\\] <p>with the optimal function:</p> \\[f^{*} = \\argmin_{f}{R_{TRUE}(f)}\\] <ul> <li>\\(R_{TRUE}(f)\\) is the true risk if we have access to an infinite set of all possible data and labels</li> <li>In practical, the joint probability distribution \\(P(x,y) = P(y|x)P(x)\\) is unknown and the only available information is contained in the training set</li> </ul> <p>Thus, the true risk is replaced by the empirical risk, which is the average of sample losses over the training set \\(D\\):</p> \\[R_n(f) = \\frac{1}{n} \\sum_{i=1}^{n}{\\mathcal{l}(f(x_i), y_i)}\\] <p>with attempting to find a function in \\(\\mathcal{F}\\) which minimizes the emprical risk:</p> \\[f_{n} = \\argmin_{f}{R_{n}(f)}\\] <ul> <li>\\(\\mathcal{F}\\) is a family of candidate functions</li> <li>In the case of CNNs, this involves choosing the relevant hyperparameters, model architecture, etc.</li> </ul> <p>Thus finding a function that is as close as possible to \\(f^{*}\\) can be broken down into:</p> <ol> <li>Choosing a class of models that is more likely to contain the optimal function</li> <li>Having a large and broad range of training examples in \\(D\\) to better approximate an infinite set of all possible data and labels</li> </ol>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#deep-transfer-learning","title":"Deep transfer learning","text":"<ul> <li>Given a source domain \\(\\mathcal{D}_S\\) and learning task \\(\\mathcal{T}_S\\), a target domain \\(\\mathcal{D}_T\\) and learning task \\(\\mathcal{D}_T\\)</li> <li>Deep transfer learning aims to improve the performance of the target model \\(M\\) on the target task \\(\\mathcal{D}_T\\) by initializing it with weights \\(W\\)</li> <li>Weights \\(W\\) are trained on source task \\(\\mathcal{T}_S\\) using source dataset \\(\\mathcal{D}_S\\) (pretraining)</li> <li>Where \\(\\mathcal{D}_S \\neq \\mathcal{D}_T\\), or \\(\\mathcal{T}_S \\neq \\mathcal{T}_T\\)</li> <li>With deep neural networks, once the weights have been pretrained to respond to particular features in a large source dataset, the weights will not change far from their pretrained values during fine-tuning</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#datasets-commonly-used-in-transfer-learning-for-image-classification","title":"Datasets commonly used in transfer learning for image classification","text":"<ol> <li>Imagenet 1K, 5K, 9K, 21K: different subset of classes, e.g. 21K has 21000 classes</li> <li>JFT: internal Google Dataset</li> </ol>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#negative-transfer","title":"Negative Transfer","text":"<ul> <li>If the source dataset is not well related to the target dataset, the target model can be negatively impacted by pretraining</li> <li>Negative transfer occurs when NTG is positive</li> <li>Divergence between the source and target domains, the size and quality of the source and target datasets affect negative transfer</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#negative-transfer-gap-ntg","title":"Negative Transfer Gap (NTG)","text":"\\[NTG = \\epsilon_{\\tau}(\\theta(S, \\tau)) - \\epsilon_{\\tau}(\\theta(\\emptyset, \\tau))\\] <ul> <li>\\(\\epsilon_{\\tau}\\) as the test error on the target domain</li> <li>\\(\\theta\\) as the specific transfer learning algorithm</li> <li>\\(\\emptyset\\) as the case where the source domain data/information are not used by the target domain learner </li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#class-activation-map-cam","title":"Class Activation Map (CAM)","text":""},{"location":"note/Computer_Vision/Image_Classification/Concepts/#grad-cam-visual-explanations-from-deep-networks-via-gradient-based-localization","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization","text":"<ul> <li>If a certain pixel is important, then the CNN will have a large activation at those pixels</li> <li>If a certain convolutional channel is important with respect to the required class, the gradients at that channel will be very large</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Concepts/#caveats","title":"Caveats","text":"<ul> <li>Imbalanced data<ul> <li>Confusion matrix</li> <li>Loss function (binary or categorical cross-entropy) ensures that the loss values are high when the amount of misclassification is high</li> <li>Higher class weights to rare class image</li> <li>Over-sample rare class image</li> <li>Data augmentation</li> <li>Transfer learning</li> </ul> </li> <li>The size of the object (small) within an image<ul> <li>Object detection: divide input image into smaller grid cells, then identify whether a grid cell contains the object of interest</li> <li>Model is trained and inferred on images with high resolution</li> </ul> </li> <li>Data drift</li> <li>The number of nodes in the flatten layer<ul> <li>Typically around 500-5000 nodes</li> </ul> </li> <li>Image size<ul> <li>Images of objects might not lose information if resized</li> <li>Images of text might lose considerable information</li> </ul> </li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Survey/","title":"Survey","text":""},{"location":"note/Computer_Vision/Image_Classification/Survey/#deep-transfer-learning-for-image-classification-a-survey","title":"Deep transfer learning for image classification: a survey","text":"<p>Sometimes collecting large amounts of training data is infeasible:</p> <ol> <li>Insufficient data because the data is very rare or there are issues with privacy</li> <li>Expensive to collect and/or label data</li> <li>The long tail distribution where a small number of classes are very frequent and thus easy to model, while many more are rare and thus hard to model</li> </ol> <p>Why learn from a small number of training samples?</p> <ol> <li>From a cognitive science perspective to attempt to mimic the human ability to learn general concepts from a small number of examples</li> <li>Restraints on compute resources</li> </ol>"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/","title":"Transformer Models","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#vision-transformer-vit","title":"Vision Transformer (ViT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","text":"<ul> <li>Inductive biases in CNNs, which are lacking in ViTs:<ol> <li>Translational Equivariance: an object can appear anywhere in the image, and CNNs can detect its features</li> <li>Locality: pixels in an image interact mainly with its surrounding pixels to form features</li> </ol> </li> <li>ViTs are highly scalable and trained on massive amount of images, overcoming the need of these inductive biases</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#how-to-train-your-vit-data-augmentation-and-regularization-in-vision-transformers","title":"How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers","text":"<ul> <li>In comparison to CNNs, ViT's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (\"AugReg\") when training on smaller training datasets</li> <li>Scaling datasets with AugReg and compute</li> <li>Transfer is the better option</li> <li>More data yields more generic models</li> <li>Prefer augmentation to regularization</li> <li>How to select a model for further adaption for an end application? One way is to run downstream adaptation for all available pre-trained models and then select the best performing model, based on the validation score on the downstream task of interest. This could be quite expensive in practice</li> <li>Alternatively, one can select a single pre-trained model based on the upstream validation accuracy and then only use this model for adaptation, which is much cheaper</li> <li>Prefer increasing patch-size to shrinking the model size</li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#scaling-vision-transformers","title":"Scaling Vision Transformers","text":"<ul> <li>Scaling up compute, model and data together improves representation quality</li> <li>Representation quality can be bottlenecked by model size</li> <li>Large models benefit from additional data</li> </ul> Name Width Depth MLP Heads Mio. Param GFLOPs (224\u00b2) GFLOPs (384\u00b2) s/28 256 6 1024 8 5.4 0.7 2.0 s/16 256 6 1024 8 5.0 2.2 7.8 S/32 384 12 1536 6 22 2.3 6.9 Ti/16 192 12 768 3 5.5 2.5 9.5 B/32 768 12 3072 12 87 8.7 26.0 S/16 384 12 1536 6 22 9.2 31.2 B/28 768 12 3072 12 87 11.3 30.5 B/16 768 12 3072 12 86 35.1 111.3 L/16 1024 24 4096 16 303 122.9 382.8 g/14 1408 40 6144 16 1011 533.1 1596.4 G/14 1664 48 8192 16 1843 965.3 2859.9"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#when-vision-transformers-outperform-resnets-without-pre-training-or-strong-data-augmentations","title":"When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations","text":"<ul> <li>ViTs can outperform ResNets of even bigger sizes in both accuracy and various forms of robustness by using a principled optimizer, without the need for large-scale pre-training or strong data augmentations</li> <li>Study from the lens of loss landscapes:<ul> <li>Visualization and Hessian matrices of the loss landscapes reveal that Transformers and MLP-Mixers converge at extremely sharp local minima, whose largest principal curvatures are almost an order of magnitude bigger than ResNets'</li> <li>Such effect accumulates when the gradients backpropogate from the last layer to the first, and the intial embedding layer suffers the largest eigenvalue of the corresponding sub-diagonal Hessian</li> <li>The networks all have very small training errors, and MLP-Mixers are more prone to overfitting than ViTs of more parameters (because of the difference in self-attention)</li> <li>ViTs and MLP-Mixers have worse \"trainabilities\" than ResNets, following the neural tangent kernel analyses</li> </ul> </li> <li>Sharpness-aware minimizer (SAM): explicitly smooths the loss geometry during model training<ul> <li>The first-order optmizers (e.g. SGD, Adam) only seek the model parameters that minimize the training error, which dismiss the higher-order information such as flatness that correlates with generalization</li> <li>SAM strives to find a solution whose entire neighborhood has low losses rather than focus on any singleton point</li> <li>SAM incurs another round of forward and backward propagations to update , which will lead to around 2x computational cost per update </li> </ul> </li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#data-efficient-image-transformet-deit","title":"Data Efficient Image Transformet (DeiT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#mobilevit","title":"MobileViT","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#mlp-mixer","title":"MLP-Mixer","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#mlp-mixer-an-all-mlp-architecture-for-vision","title":"MLP-Mixer: An all-MLP Architecture for Vision","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#swin-transformer","title":"Swin Transformer","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#swin-transformer-hierarchical-vision-transformer-using-shifted-windows","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#shifted-windows","title":"Shifted windows","text":"<ul> <li>In the original ViT, attention is done between patch and all other patches, the processing time complexity increases quadratically with image dimensions</li> <li>Shifted window mechanism helps the model to extract features at variable scales and also restricts the computational complexity with respect to image size to linear</li> <li>Each window block is divided into patches and fed to model in same way the vision transformer processes the entire input image</li> <li>The self-attention block of the transformer computes the key-query weight for these patches within these windows</li> <li>This helps the model emphasize on small scale features, but since the relationship between the patches are computed within the windows self-attention mechanism, it's unable to capture the global context which is a key feature in transformers</li> <li>Window partitioning in successive layers:<ul> <li>In the first layer, the image is divided into windows by red boxes. Each window is further divided into patches denoted by gray boxes</li> <li>In the second layer, this window is shifted, and these windows are overlapping with the windows divided in the previous layer</li> </ul> </li> </ul>"},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#swin-transformer-v2-scaling-up-capacity-and-resolution","title":"Swin Transformer V2: Scaling Up Capacity and Resolution","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#convolutional-vision-transformer-cvt","title":"Convolutional Vision Transformer (CvT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#dilated-neighborhood-attention-transformer-dinat","title":"Dilated Neighborhood Attention Transformer (DiNAT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#conditioinal-position-encoding-vision-transformer-cpvt","title":"Conditioinal Position Encoding Vision Transformer (CPVT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#transformer-in-transformer-tnt","title":"Transformer-iN-Transformer (TNT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#tokens-to-token-vit-t2t-vit","title":"Tokens-to-Token ViT (T2T-ViT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Transformer_Models/#pyramid-vision-transformer-pvt","title":"Pyramid Vision Transformer (PVT)","text":""},{"location":"note/Computer_Vision/Image_Classification/Notebooks/grad_cam/","title":"Grad cam","text":"In\u00a0[1]: Copied! <pre>%%writefile kaggle.json\n{\"username\":\"xx\",\"key\":\"xx\"}\n</pre> %%writefile kaggle.json {\"username\":\"xx\",\"key\":\"xx\"} <pre>Writing kaggle.json\n</pre> In\u00a0[2]: Copied! <pre>!pip install -qq kaggle\n</pre> !pip install -qq kaggle In\u00a0[3]: Copied! <pre>!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!ls ~/.kaggle\n!chmod 600 /root/.kaggle/kaggle.json\n!kaggle datasets download -d iarunava/cell-images-for-detecting-malaria\n!unzip -qq cell-images-for-detecting-malaria.zip\n</pre> !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !ls ~/.kaggle !chmod 600 /root/.kaggle/kaggle.json !kaggle datasets download -d iarunava/cell-images-for-detecting-malaria !unzip -qq cell-images-for-detecting-malaria.zip <pre>kaggle.json\nDataset URL: https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria\nLicense(s): unknown\nDownloading cell-images-for-detecting-malaria.zip to /content\n 99% 668M/675M [00:13&lt;00:00, 49.3MB/s]\n100% 675M/675M [00:13&lt;00:00, 53.1MB/s]\n</pre> In\u00a0[88]: Copied! <pre>import glob\nimport random\nfrom pathlib import Path\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms, models, datasets\nfrom torchsummary import summary\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport matplotlib.pyplot as plt\n</pre> import glob import random from pathlib import Path from PIL import Image import cv2 from tqdm import tqdm import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torchvision from torchvision import transforms, models, datasets from torchsummary import summary from torch.utils.data import TensorDataset, DataLoader, Dataset import matplotlib.pyplot as plt In\u00a0[5]: Copied! <pre>id2int = {'Parasitized': 0, 'Uninfected': 1}\n</pre> id2int = {'Parasitized': 0, 'Uninfected': 1} In\u00a0[6]: Copied! <pre>trn_tfms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    transforms.ColorJitter(brightness=(0.95,1.05),\n                  contrast=(0.95,1.05),\n                  saturation=(0.95,1.05),\n                  hue=0.05),\n    transforms.RandomAffine(5, translate=(0.01,0.1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                std=[0.5, 0.5, 0.5]),\n])\n</pre> trn_tfms = transforms.Compose([     transforms.ToPILImage(),     transforms.Resize(128),     transforms.CenterCrop(128),     transforms.ColorJitter(brightness=(0.95,1.05),                   contrast=(0.95,1.05),                   saturation=(0.95,1.05),                   hue=0.05),     transforms.RandomAffine(5, translate=(0.01,0.1)),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5, 0.5, 0.5],                 std=[0.5, 0.5, 0.5]), ]) In\u00a0[7]: Copied! <pre>val_tfms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(128),\n    transforms.CenterCrop(128),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                std=[0.5, 0.5, 0.5]),\n])\n</pre> val_tfms = transforms.Compose([     transforms.ToPILImage(),     transforms.Resize(128),     transforms.CenterCrop(128),     transforms.ToTensor(),     transforms.Normalize(mean=[0.5, 0.5, 0.5],                 std=[0.5, 0.5, 0.5]), ]) In\u00a0[39]: Copied! <pre>class MalariaImages(Dataset):\n\n    def __init__(self, files, transform=None):\n        super(MalariaImages).__init__()\n        self.files = files\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        fpath = self.files[idx]\n        clss = Path(str(Path(fpath).parent)).name\n        img = Image.open(fpath)\n        return img, clss\n\n    def choose(self):\n        return self[random.randint(len(self))]\n\n    def collate_fn(self, batch):\n        _imgs, classes = list(zip(*batch))\n        if self.transform:\n            imgs = [self.transform(img)[None] for img in _imgs]\n        classes = [torch.tensor([id2int[clss]]) for clss in classes]\n        imgs, classes = [torch.cat(i).to(device) for i in [imgs, classes]]\n        return imgs, classes, _imgs\n</pre> class MalariaImages(Dataset):      def __init__(self, files, transform=None):         super(MalariaImages).__init__()         self.files = files         self.transform = transform      def __len__(self):         return len(self.files)      def __getitem__(self, idx):         fpath = self.files[idx]         clss = Path(str(Path(fpath).parent)).name         img = Image.open(fpath)         return img, clss      def choose(self):         return self[random.randint(len(self))]      def collate_fn(self, batch):         _imgs, classes = list(zip(*batch))         if self.transform:             imgs = [self.transform(img)[None] for img in _imgs]         classes = [torch.tensor([id2int[clss]]) for clss in classes]         imgs, classes = [torch.cat(i).to(device) for i in [imgs, classes]]         return imgs, classes, _imgs In\u00a0[77]: Copied! <pre>device = 'cuda' if torch.cuda.is_available() else 'cpu'\nall_files = glob.glob('cell_images/*/*.png')\nnp.random.seed(10)\nnp.random.shuffle(all_files)\n\nfrom sklearn.model_selection import train_test_split\ntrn_files, val_files = train_test_split(all_files, random_state=1)\n\ntrn_ds = MalariaImages(trn_files, transform=trn_tfms)\nval_ds = MalariaImages(val_files, transform=val_tfms)\ntrn_dl = DataLoader(trn_ds, 32, shuffle=True, collate_fn=trn_ds.collate_fn)\nval_dl = DataLoader(val_ds, 32, shuffle=False, collate_fn=val_ds.collate_fn)\n\n# sample data\nfrom google.colab.patches import cv2_imshow\ncv2_imshow(trn_ds[0][0]), trn_ds[0][1]\n</pre> device = 'cuda' if torch.cuda.is_available() else 'cpu' all_files = glob.glob('cell_images/*/*.png') np.random.seed(10) np.random.shuffle(all_files)  from sklearn.model_selection import train_test_split trn_files, val_files = train_test_split(all_files, random_state=1)  trn_ds = MalariaImages(trn_files, transform=trn_tfms) val_ds = MalariaImages(val_files, transform=val_tfms) trn_dl = DataLoader(trn_ds, 32, shuffle=True, collate_fn=trn_ds.collate_fn) val_dl = DataLoader(val_ds, 32, shuffle=False, collate_fn=val_ds.collate_fn)  # sample data from google.colab.patches import cv2_imshow cv2_imshow(trn_ds[0][0]), trn_ds[0][1] Out[77]: <pre>(None, 'Parasitized')</pre> In\u00a0[78]: Copied! <pre>def convBlock(ni, no):\n    return nn.Sequential(\n        nn.Dropout(0.2),\n        nn.Conv2d(ni, no, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.BatchNorm2d(no),\n        nn.MaxPool2d(2),\n    )\n\nclass MalariaClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            convBlock(3, 64),\n            convBlock(64, 64),\n            convBlock(64, 128),\n            convBlock(128, 256),\n            convBlock(256, 512),\n            convBlock(512, 64),\n            nn.Flatten(),\n            nn.Linear(256, 256),\n            nn.Dropout(0.2),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, len(id2int))\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def compute_metrics(self, preds, targets):\n        loss = self.loss_fn(preds, targets)\n        acc = (torch.max(preds, 1)[1] == targets).float().mean()\n        return loss, acc\n</pre> def convBlock(ni, no):     return nn.Sequential(         nn.Dropout(0.2),         nn.Conv2d(ni, no, kernel_size=3, padding=1),         nn.ReLU(inplace=True),         nn.BatchNorm2d(no),         nn.MaxPool2d(2),     )  class MalariaClassifier(nn.Module):     def __init__(self):         super().__init__()         self.model = nn.Sequential(             convBlock(3, 64),             convBlock(64, 64),             convBlock(64, 128),             convBlock(128, 256),             convBlock(256, 512),             convBlock(512, 64),             nn.Flatten(),             nn.Linear(256, 256),             nn.Dropout(0.2),             nn.ReLU(inplace=True),             nn.Linear(256, len(id2int))         )         self.loss_fn = nn.CrossEntropyLoss()      def forward(self, x):         return self.model(x)      def compute_metrics(self, preds, targets):         loss = self.loss_fn(preds, targets)         acc = (torch.max(preds, 1)[1] == targets).float().mean()         return loss, acc In\u00a0[79]: Copied! <pre>def train_batch(model, data, optimizer, criterion):\n    model.train()\n    ims, labels, _ = data\n    _preds = model(ims)\n    optimizer.zero_grad()\n    loss, acc = criterion(_preds, labels)\n    loss.backward()\n    optimizer.step()\n    return loss.item(), acc.item()\n\n@torch.no_grad()\ndef validate_batch(model, data, criterion):\n    model.eval()\n    ims, labels, _ = data\n    _preds = model(ims)\n    loss, acc = criterion(_preds, labels)\n    return loss.item(), acc.item()\n</pre> def train_batch(model, data, optimizer, criterion):     model.train()     ims, labels, _ = data     _preds = model(ims)     optimizer.zero_grad()     loss, acc = criterion(_preds, labels)     loss.backward()     optimizer.step()     return loss.item(), acc.item()  @torch.no_grad() def validate_batch(model, data, criterion):     model.eval()     ims, labels, _ = data     _preds = model(ims)     loss, acc = criterion(_preds, labels)     return loss.item(), acc.item() In\u00a0[80]: Copied! <pre>model = MalariaClassifier().to(device)\ncriterion = model.compute_metrics\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nn_epochs = 2\n\ntrain_loss, test_loss = [], []\ntrain_acc, test_acc = [], []\nfor epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n    epoch_train_loss, epoch_test_loss = 0, 0\n    epoch_train_acc, epoch_test_acc = 0, 0\n\n    N = len(trn_dl)\n    for idx, data in enumerate(trn_dl):\n        loss, acc = train_batch(model, data, optimizer, criterion)\n        epoch_train_loss += loss; epoch_train_acc += acc\n    epoch_train_loss /= N; epoch_train_acc /= N\n\n    N = len(val_dl)\n    for bx, data in enumerate(val_dl):\n        loss, acc = validate_batch(model, data, criterion)\n        epoch_test_loss += loss; epoch_test_acc += acc\n    epoch_test_loss /= N; epoch_test_acc /= N\n\n    train_loss.append(epoch_train_loss); train_acc.append(epoch_train_acc)\n    test_loss.append(epoch_test_loss); test_acc.append(epoch_test_acc)\n\n    # Print metrics for the current epoch\n    tqdm.write(f\"Epoch [{epoch+1}/{n_epochs}], \"\n               f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, \"\n               f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\")\n</pre> model = MalariaClassifier().to(device) criterion = model.compute_metrics optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) n_epochs = 2  train_loss, test_loss = [], [] train_acc, test_acc = [], [] for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):     epoch_train_loss, epoch_test_loss = 0, 0     epoch_train_acc, epoch_test_acc = 0, 0      N = len(trn_dl)     for idx, data in enumerate(trn_dl):         loss, acc = train_batch(model, data, optimizer, criterion)         epoch_train_loss += loss; epoch_train_acc += acc     epoch_train_loss /= N; epoch_train_acc /= N      N = len(val_dl)     for bx, data in enumerate(val_dl):         loss, acc = validate_batch(model, data, criterion)         epoch_test_loss += loss; epoch_test_acc += acc     epoch_test_loss /= N; epoch_test_acc /= N      train_loss.append(epoch_train_loss); train_acc.append(epoch_train_acc)     test_loss.append(epoch_test_loss); test_acc.append(epoch_test_acc)      # Print metrics for the current epoch     tqdm.write(f\"Epoch [{epoch+1}/{n_epochs}], \"                f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, \"                f\"Test Loss: {epoch_test_loss:.4f}, Test Acc: {epoch_test_acc:.4f}\") <pre>Epochs:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [02:06&lt;02:06, 126.73s/it]</pre> <pre>Epoch [1/2], Train Loss: 0.2402, Train Acc: 0.9113, Test Loss: 0.1576, Test Acc: 0.9522\n</pre> <pre>Epochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [04:13&lt;00:00, 126.77s/it]</pre> <pre>Epoch [2/2], Train Loss: 0.1692, Train Acc: 0.9452, Test Loss: 0.1798, Test Acc: 0.9402\n</pre> <pre>\n</pre> In\u00a0[81]: Copied! <pre>im2fmap = nn.Sequential(*(list(model.model[:5].children()) + list(model.model[5][:2].children())))\n</pre> im2fmap = nn.Sequential(*(list(model.model[:5].children()) + list(model.model[5][:2].children()))) In\u00a0[91]: Copied! <pre>def im2gradCAM(x):\n    model.eval()\n    logits = model(x)\n    heatmaps = []\n    activations = im2fmap(x)\n    # print(activations.shape)\n    pred = logits.max(-1)[-1]\n\n    # get the model's prediction\n    model.zero_grad()\n\n    # compute gradients with respect to model's most confident logit\n    logits[0,pred].backward(retain_graph=True)\n\n    # get the gradients at the required featuremap location\n    # and take the avg gradient for every featuremap\n    pooled_grads = model.model[-6][1].weight.grad.data.mean((1,2,3))\n\n    # multiply each activation map with corresponding gradient average\n    for i in range(activations.shape[1]):\n        activations[:,i,:,:] *= pooled_grads[i]\n\n    # take the mean of all weighted activation maps\n    # (that has been weighted by avg. grad at each fmap)\n    heatmap = torch.mean(activations, dim=1)[0].cpu().detach()\n    return heatmap, 'Uninfected' if pred.item() else 'Parasitized'\n</pre> def im2gradCAM(x):     model.eval()     logits = model(x)     heatmaps = []     activations = im2fmap(x)     # print(activations.shape)     pred = logits.max(-1)[-1]      # get the model's prediction     model.zero_grad()      # compute gradients with respect to model's most confident logit     logits[0,pred].backward(retain_graph=True)      # get the gradients at the required featuremap location     # and take the avg gradient for every featuremap     pooled_grads = model.model[-6][1].weight.grad.data.mean((1,2,3))      # multiply each activation map with corresponding gradient average     for i in range(activations.shape[1]):         activations[:,i,:,:] *= pooled_grads[i]      # take the mean of all weighted activation maps     # (that has been weighted by avg. grad at each fmap)     heatmap = torch.mean(activations, dim=1)[0].cpu().detach()     return heatmap, 'Uninfected' if pred.item() else 'Parasitized' In\u00a0[89]: Copied! <pre>SZ = 128\ndef upsampleHeatmap(map, img):\n    m,M = map.min(), map.max()\n    map = 255 * ((map-m) / (M-m))\n    map = np.uint8(map)\n    map = cv2.resize(map, (SZ,SZ))\n    map = cv2.applyColorMap(255-map, cv2.COLORMAP_JET)\n    map = np.uint8(map)\n    map = np.uint8(map*0.7 + img*0.3)\n    return map\n\ndef subplots(images, nc=2, figsize=(10, 5), suptitle=None):\n    \"\"\"Display images in subplots.\"\"\"\n    fig, axs = plt.subplots(1, nc, figsize=figsize)\n    for ax, img in zip(axs, images):\n        ax.imshow(img, aspect='auto')\n        ax.axis('off')  # Hide axes\n    if suptitle:\n        plt.suptitle(suptitle)\n    plt.show()\n</pre> SZ = 128 def upsampleHeatmap(map, img):     m,M = map.min(), map.max()     map = 255 * ((map-m) / (M-m))     map = np.uint8(map)     map = cv2.resize(map, (SZ,SZ))     map = cv2.applyColorMap(255-map, cv2.COLORMAP_JET)     map = np.uint8(map)     map = np.uint8(map*0.7 + img*0.3)     return map  def subplots(images, nc=2, figsize=(10, 5), suptitle=None):     \"\"\"Display images in subplots.\"\"\"     fig, axs = plt.subplots(1, nc, figsize=figsize)     for ax, img in zip(axs, images):         ax.imshow(img, aspect='auto')         ax.axis('off')  # Hide axes     if suptitle:         plt.suptitle(suptitle)     plt.show() In\u00a0[92]: Copied! <pre>N = 20\n_val_dl = DataLoader(val_ds, batch_size=N, shuffle=True, collate_fn=val_ds.collate_fn)\nx,y,z = next(iter(_val_dl))\n\nfor i in range(N):\n    image = cv2.resize(z[i], (SZ, SZ))\n    heatmap, pred = im2gradCAM(x[i:i+1])\n    if(pred=='Uninfected'):\n        continue\n    heatmap = upsampleHeatmap(heatmap, image)\n    subplots([image, heatmap], nc=2, figsize=(5,3), suptitle=pred)\n</pre> N = 20 _val_dl = DataLoader(val_ds, batch_size=N, shuffle=True, collate_fn=val_ds.collate_fn) x,y,z = next(iter(_val_dl))  for i in range(N):     image = cv2.resize(z[i], (SZ, SZ))     heatmap, pred = im2gradCAM(x[i:i+1])     if(pred=='Uninfected'):         continue     heatmap = upsampleHeatmap(heatmap, image)     subplots([image, heatmap], nc=2, figsize=(5,3), suptitle=pred)"},{"location":"note/Computer_Vision/Image_Classification/Notebooks/vit_keras/","title":"Vit keras","text":"In\u00a0[8]: Copied! <pre>import glob, random, os, warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nfrom vit_keras import vit, utils, visualize\n</pre> import glob, random, os, warnings import numpy as np import pandas as pd import tensorflow as tf import tensorflow.keras.layers as L import tensorflow_addons as tfa import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix, classification_report import seaborn as sns from vit_keras import vit, utils, visualize In\u00a0[2]: Copied! <pre>image_size = 224\nbatch_size = 32\nn_classes = 3\nepochs = 30\n</pre> image_size = 224 batch_size = 32 n_classes = 3 epochs = 30 In\u00a0[5]: Copied! <pre>vit_model = vit.vit_b16(\n        image_size = image_size,\n        activation = 'softmax',\n        pretrained = True,\n        include_top = False,\n        pretrained_top = False,\n        classes = 5)\n</pre> vit_model = vit.vit_b16(         image_size = image_size,         activation = 'softmax',         pretrained = True,         include_top = False,         pretrained_top = False,         classes = 5) <pre>2024-10-30 09:44:51.406222: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n/home/junwai/.local/lib/python3.8/site-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n  warnings.warn(\n</pre> In\u00a0[6]: Copied! <pre>class Patches(L.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images = images,\n            sizes = [1, self.patch_size, self.patch_size, 1],\n            strides = [1, self.patch_size, self.patch_size, 1],\n            rates = [1, 1, 1, 1],\n            padding = 'VALID',\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches\n</pre> class Patches(L.Layer):     def __init__(self, patch_size):         super(Patches, self).__init__()         self.patch_size = patch_size      def call(self, images):         batch_size = tf.shape(images)[0]         patches = tf.image.extract_patches(             images = images,             sizes = [1, self.patch_size, self.patch_size, 1],             strides = [1, self.patch_size, self.patch_size, 1],             rates = [1, 1, 1, 1],             padding = 'VALID',         )         patch_dims = patches.shape[-1]         patches = tf.reshape(patches, [batch_size, -1, patch_dims])         return patches In\u00a0[17]: Copied! <pre>model = tf.keras.Sequential([\n        vit_model,\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(128, activation = tfa.activations.gelu),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(64, activation = tfa.activations.gelu),\n        tf.keras.layers.Dense(32, activation = tfa.activations.gelu),\n        tf.keras.layers.Dense(3, 'softmax')\n    ],\n    name = 'vision_transformer')\n\nmodel.summary()\n</pre> model = tf.keras.Sequential([         vit_model,         tf.keras.layers.Flatten(),         tf.keras.layers.BatchNormalization(),         tf.keras.layers.Dense(128, activation = tfa.activations.gelu),         tf.keras.layers.BatchNormalization(),         tf.keras.layers.Dense(64, activation = tfa.activations.gelu),         tf.keras.layers.Dense(32, activation = tfa.activations.gelu),         tf.keras.layers.Dense(3, 'softmax')     ],     name = 'vision_transformer')  model.summary() <pre>Model: \"vision_transformer\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n vit-b16 (Functional)        (None, 768)               85798656  \n                                                                 \n flatten (Flatten)           (None, 768)               0         \n                                                                 \n batch_normalization (Batch  (None, 768)               3072      \n Normalization)                                                  \n                                                                 \n dense (Dense)               (None, 128)               98432     \n                                                                 \n batch_normalization_1 (Bat  (None, 128)               512       \n chNormalization)                                                \n                                                                 \n dense_1 (Dense)             (None, 64)                8256      \n                                                                 \n dense_2 (Dense)             (None, 32)                2080      \n                                                                 \n dense_3 (Dense)             (None, 3)                 99        \n                                                                 \n=================================================================\nTotal params: 85911107 (327.72 MB)\nTrainable params: 85909315 (327.72 MB)\nNon-trainable params: 1792 (7.00 KB)\n_________________________________________________________________\n</pre> In\u00a0[14]: Copied! <pre>plt.figure(figsize=(4, 4))\npatch_size = 16  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\n\nurl = 'https://upload.wikimedia.org/wikipedia/commons/b/bc/Free%21_%283987584939%29.jpg'\nimage = utils.read(url, image_size)\n</pre> plt.figure(figsize=(4, 4)) patch_size = 16  # Size of the patches to be extract from the input images num_patches = (image_size // patch_size) ** 2  url = 'https://upload.wikimedia.org/wikipedia/commons/b/bc/Free%21_%283987584939%29.jpg' image = utils.read(url, image_size) <pre>&lt;Figure size 400x400 with 0 Axes&gt;</pre> In\u00a0[16]: Copied! <pre># Attention map\n\nattention_map = visualize.attention_map(model=vit_model, image=image)\n# print('Prediction:', classes[\n#     model.predict(vit.preprocess_inputs(image)[np.newaxis])[0].argmax()]\n# )  # Prediction: Eskimo dog, husky\n\n# Plot results\nfig, (ax1, ax2) = plt.subplots(ncols=2)\nax1.axis('off')\nax2.axis('off')\nax1.set_title('Original')\nax2.set_title('Attention Map')\n_ = ax1.imshow(image)\n_ = ax2.imshow(attention_map)\n</pre> # Attention map  attention_map = visualize.attention_map(model=vit_model, image=image) # print('Prediction:', classes[ #     model.predict(vit.preprocess_inputs(image)[np.newaxis])[0].argmax()] # )  # Prediction: Eskimo dog, husky  # Plot results fig, (ax1, ax2) = plt.subplots(ncols=2) ax1.axis('off') ax2.axis('off') ax1.set_title('Original') ax2.set_title('Attention Map') _ = ax1.imshow(image) _ = ax2.imshow(attention_map) <pre>1/1 [==============================] - 3s 3s/step\n</pre> In\u00a0[15]: Copied! <pre># Patches\n\nplt.imshow(image.astype('uint8'))\nplt.axis('off')\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size = (image_size, image_size)\n)\n\npatches = Patches(patch_size)(resized_image)\nprint(f'Image size: {image_size} X {image_size}')\nprint(f'Patch size: {patch_size} X {patch_size}')\nprint(f'Patches per image: {patches.shape[1]}')\nprint(f'Elements per patch: {patches.shape[-1]}')\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\n\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(patch_img.numpy().astype('uint8'))\n    plt.axis('off')\n</pre> # Patches  plt.imshow(image.astype('uint8')) plt.axis('off')  resized_image = tf.image.resize(     tf.convert_to_tensor([image]), size = (image_size, image_size) )  patches = Patches(patch_size)(resized_image) print(f'Image size: {image_size} X {image_size}') print(f'Patch size: {patch_size} X {patch_size}') print(f'Patches per image: {patches.shape[1]}') print(f'Elements per patch: {patches.shape[-1]}')  n = int(np.sqrt(patches.shape[1])) plt.figure(figsize=(4, 4))  for i, patch in enumerate(patches[0]):     ax = plt.subplot(n, n, i + 1)     patch_img = tf.reshape(patch, (patch_size, patch_size, 3))     plt.imshow(patch_img.numpy().astype('uint8'))     plt.axis('off') <pre>Image size: 224 X 224\nPatch size: 16 X 16\nPatches per image: 196\nElements per patch: 768\n</pre>"},{"location":"note/Computer_Vision/Image_Classification/Notebooks/vit_keras/#build-model","title":"Build Model\u00b6","text":""},{"location":"note/Computer_Vision/Image_Classification/Notebooks/vit_keras/#visualization","title":"Visualization\u00b6","text":""},{"location":"note/Computer_Vision/Image_Classification/Notebooks/vit_scratch/","title":"Vit scratch","text":"In\u00a0[10]: Copied! <pre>import os\n\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import layers\nfrom keras import ops\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import os  os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  import keras from keras import layers from keras import ops  import numpy as np import matplotlib.pyplot as plt In\u00a0[11]: Copied! <pre>num_classes = 100\ninput_shape = (32, 32, 3)\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n\nprint(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\nprint(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n</pre> num_classes = 100 input_shape = (32, 32, 3)  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()  print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\") print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\") <pre>Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n169001437/169001437 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25s 0us/step\nx_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\nx_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n</pre> In\u00a0[12]: Copied! <pre>learning_rate = 0.001\nweight_decay = 0.0001\nbatch_size = 256\nnum_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\nimage_size = 72  # We'll resize input images to this size\npatch_size = 6  # Size of the patches to be extract from the input images\nnum_patches = (image_size // patch_size) ** 2\nprojection_dim = 64\nnum_heads = 4\ntransformer_units = [\n    projection_dim * 2,\n    projection_dim,\n]  # Size of the transformer layers\ntransformer_layers = 8\nmlp_head_units = [\n    2048,\n    1024,\n]  # Size of the dense layers of the final classifier\n\n# Layers: transformer_layers\n# Hidden size: projection_dim\n# MLP size:\n# Heads: num_heads\n</pre> learning_rate = 0.001 weight_decay = 0.0001 batch_size = 256 num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value image_size = 72  # We'll resize input images to this size patch_size = 6  # Size of the patches to be extract from the input images num_patches = (image_size // patch_size) ** 2 projection_dim = 64 num_heads = 4 transformer_units = [     projection_dim * 2,     projection_dim, ]  # Size of the transformer layers transformer_layers = 8 mlp_head_units = [     2048,     1024, ]  # Size of the dense layers of the final classifier  # Layers: transformer_layers # Hidden size: projection_dim # MLP size: # Heads: num_heads In\u00a0[13]: Copied! <pre>data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(image_size, image_size),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.02),\n        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(x_train)\n</pre> data_augmentation = keras.Sequential(     [         layers.Normalization(),         layers.Resizing(image_size, image_size),         layers.RandomFlip(\"horizontal\"),         layers.RandomRotation(factor=0.02),         layers.RandomZoom(height_factor=0.2, width_factor=0.2),     ],     name=\"data_augmentation\", ) # Compute the mean and the variance of the training data for normalization. data_augmentation.layers[0].adapt(x_train) In\u00a0[14]: Copied! <pre>def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x\n</pre> def mlp(x, hidden_units, dropout_rate):     for units in hidden_units:         x = layers.Dense(units, activation=keras.activations.gelu)(x)         x = layers.Dropout(dropout_rate)(x)     return x In\u00a0[15]: Copied! <pre>class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, images):\n        input_shape = ops.shape(images)\n        batch_size = input_shape[0]\n        height = input_shape[1]\n        width = input_shape[2]\n        channels = input_shape[3]\n        num_patches_h = height // self.patch_size\n        num_patches_w = width // self.patch_size\n        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n        patches = ops.reshape(\n            patches,\n            (\n                batch_size,\n                num_patches_h * num_patches_w,\n                self.patch_size * self.patch_size * channels,\n            ),\n        )\n        return patches\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"patch_size\": self.patch_size})\n        return config\n</pre> class Patches(layers.Layer):     def __init__(self, patch_size):         super().__init__()         self.patch_size = patch_size      def call(self, images):         input_shape = ops.shape(images)         batch_size = input_shape[0]         height = input_shape[1]         width = input_shape[2]         channels = input_shape[3]         num_patches_h = height // self.patch_size         num_patches_w = width // self.patch_size         patches = keras.ops.image.extract_patches(images, size=self.patch_size)         patches = ops.reshape(             patches,             (                 batch_size,                 num_patches_h * num_patches_w,                 self.patch_size * self.patch_size * channels,             ),         )         return patches      def get_config(self):         config = super().get_config()         config.update({\"patch_size\": self.patch_size})         return config In\u00a0[16]: Copied! <pre>plt.figure(figsize=(4, 4))\nimage = x_train[np.random.choice(range(x_train.shape[0]))]\nplt.imshow(image.astype(\"uint8\"))\nplt.axis(\"off\")\n\nresized_image = ops.image.resize(\n    ops.convert_to_tensor([image]), size=(image_size, image_size)\n)\npatches = Patches(patch_size)(resized_image)\nprint(f\"Image size: {image_size} X {image_size}\")\nprint(f\"Patch size: {patch_size} X {patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n    plt.axis(\"off\")\n</pre> plt.figure(figsize=(4, 4)) image = x_train[np.random.choice(range(x_train.shape[0]))] plt.imshow(image.astype(\"uint8\")) plt.axis(\"off\")  resized_image = ops.image.resize(     ops.convert_to_tensor([image]), size=(image_size, image_size) ) patches = Patches(patch_size)(resized_image) print(f\"Image size: {image_size} X {image_size}\") print(f\"Patch size: {patch_size} X {patch_size}\") print(f\"Patches per image: {patches.shape[1]}\") print(f\"Elements per patch: {patches.shape[-1]}\")  n = int(np.sqrt(patches.shape[1])) plt.figure(figsize=(4, 4)) for i, patch in enumerate(patches[0]):     ax = plt.subplot(n, n, i + 1)     patch_img = ops.reshape(patch, (patch_size, patch_size, 3))     plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))     plt.axis(\"off\") <pre>Image size: 72 X 72\nPatch size: 6 X 6\nPatches per image: 144\nElements per patch: 108\n</pre> In\u00a0[17]: Copied! <pre>class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(units=projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n\n    def call(self, patch):\n        positions = ops.expand_dims(\n            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n        )\n        projected_patches = self.projection(patch)\n        encoded = projected_patches + self.position_embedding(positions)\n        return encoded\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"num_patches\": self.num_patches})\n        return config\n</pre> class PatchEncoder(layers.Layer):     def __init__(self, num_patches, projection_dim):         super().__init__()         self.num_patches = num_patches         self.projection = layers.Dense(units=projection_dim)         self.position_embedding = layers.Embedding(             input_dim=num_patches, output_dim=projection_dim         )      def call(self, patch):         positions = ops.expand_dims(             ops.arange(start=0, stop=self.num_patches, step=1), axis=0         )         projected_patches = self.projection(patch)         encoded = projected_patches + self.position_embedding(positions)         return encoded      def get_config(self):         config = super().get_config()         config.update({\"num_patches\": self.num_patches})         return config In\u00a0[25]: Copied! <pre>def create_vit_classifier():\n    inputs = keras.Input(shape=input_shape)\n    print(\"inputs:\", inputs.shape)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    print(\"augmented:\", augmented.shape)\n    # Create patches.\n    patches = Patches(patch_size)(augmented)\n    print(\"patches:\", patches.shape)\n    # Encode patches.\n    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n    print(\"encoded_patches:\", encoded_patches.shape)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(transformer_layers):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n        )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(num_classes)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model\n</pre> def create_vit_classifier():     inputs = keras.Input(shape=input_shape)     print(\"inputs:\", inputs.shape)     # Augment data.     augmented = data_augmentation(inputs)     print(\"augmented:\", augmented.shape)     # Create patches.     patches = Patches(patch_size)(augmented)     print(\"patches:\", patches.shape)     # Encode patches.     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)     print(\"encoded_patches:\", encoded_patches.shape)      # Create multiple layers of the Transformer block.     for _ in range(transformer_layers):         # Layer normalization 1.         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)         # Create a multi-head attention layer.         attention_output = layers.MultiHeadAttention(             num_heads=num_heads, key_dim=projection_dim, dropout=0.1         )(x1, x1)         # Skip connection 1.         x2 = layers.Add()([attention_output, encoded_patches])         # Layer normalization 2.         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)         # MLP.         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)         # Skip connection 2.         encoded_patches = layers.Add()([x3, x2])      # Create a [batch_size, projection_dim] tensor.     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)     representation = layers.Flatten()(representation)     representation = layers.Dropout(0.5)(representation)     # Add MLP.     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)     # Classify outputs.     logits = layers.Dense(num_classes)(features)     # Create the Keras model.     model = keras.Model(inputs=inputs, outputs=logits)     return model In\u00a0[26]: Copied! <pre>def run_experiment(model):\n    optimizer = keras.optimizers.AdamW(\n        learning_rate=learning_rate, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n        checkpoint_filepath,\n        monitor=\"val_accuracy\",\n        save_best_only=True,\n        save_weights_only=True,\n    )\n\n    history = model.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=batch_size,\n        epochs=num_epochs,\n        validation_split=0.1,\n        callbacks=[checkpoint_callback],\n    )\n\n    model.load_weights(checkpoint_filepath)\n    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return history\n\n\nvit_classifier = create_vit_classifier()\nhistory = run_experiment(vit_classifier)\n\n\ndef plot_history(item):\n    plt.plot(history.history[item], label=item)\n    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(item)\n    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\nplot_history(\"loss\")\nplot_history(\"top-5-accuracy\")\n</pre> def run_experiment(model):     optimizer = keras.optimizers.AdamW(         learning_rate=learning_rate, weight_decay=weight_decay     )      model.compile(         optimizer=optimizer,         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),         metrics=[             keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),             keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),         ],     )      checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"     checkpoint_callback = keras.callbacks.ModelCheckpoint(         checkpoint_filepath,         monitor=\"val_accuracy\",         save_best_only=True,         save_weights_only=True,     )      history = model.fit(         x=x_train,         y=y_train,         batch_size=batch_size,         epochs=num_epochs,         validation_split=0.1,         callbacks=[checkpoint_callback],     )      model.load_weights(checkpoint_filepath)     _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)     print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")     print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")      return history   vit_classifier = create_vit_classifier() history = run_experiment(vit_classifier)   def plot_history(item):     plt.plot(history.history[item], label=item)     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)     plt.xlabel(\"Epochs\")     plt.ylabel(item)     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)     plt.legend()     plt.grid()     plt.show()   plot_history(\"loss\") plot_history(\"top-5-accuracy\") <pre>inputs: (None, 32, 32, 3)\naugmented: (None, 72, 72, 3)\npatches: (None, 144, 108)\nencoded_patches: (None, 144, 64)\nEpoch 1/10\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[26], line 41\n     37     return history\n     40 vit_classifier = create_vit_classifier()\n---&gt; 41 history = run_experiment(vit_classifier)\n     44 def plot_history(item):\n     45     plt.plot(history.history[item], label=item)\n\nCell In[26], line 23, in run_experiment(model)\n     15 checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n     16 checkpoint_callback = keras.callbacks.ModelCheckpoint(\n     17     checkpoint_filepath,\n     18     monitor=\"val_accuracy\",\n     19     save_best_only=True,\n     20     save_weights_only=True,\n     21 )\n---&gt; 23 history = model.fit(\n     24     x=x_train,\n     25     y=y_train,\n     26     batch_size=batch_size,\n     27     epochs=num_epochs,\n     28     validation_split=0.1,\n     29     callbacks=[checkpoint_callback],\n     30 )\n     32 model.load_weights(checkpoint_filepath)\n     33 _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    115 filtered_tb = None\n    116 try:\n--&gt; 117     return fn(*args, **kwargs)\n    118 except Exception as e:\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320, in TensorFlowTrainer.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\n    318 for step, iterator in epoch_iterator.enumerate_epoch():\n    319     callbacks.on_train_batch_begin(step)\n--&gt; 320     logs = self.train_function(iterator)\n    321     callbacks.on_train_batch_end(step, logs)\n    322     if self.stop_training:\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    148 filtered_tb = None\n    149 try:\n--&gt; 150   return fn(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833, in Function.__call__(self, *args, **kwds)\n    830 compiler = \"xla\" if self._jit_compile else \"nonXla\"\n    832 with OptionalXlaContext(self._jit_compile):\n--&gt; 833   result = self._call(*args, **kwds)\n    835 new_tracing_count = self.experimental_get_tracing_count()\n    836 without_tracing = (tracing_count == new_tracing_count)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889, in Function._call(self, *args, **kwds)\n    886 try:\n    887   # This is the first call of __call__, so we have to initialize.\n    888   initializers = []\n--&gt; 889   self._initialize(args, kwds, add_initializers_to=initializers)\n    890 finally:\n    891   # At this point we know that the initialization is complete (or less\n    892   # interestingly an exception was raised) so we no longer need a lock.\n    893   self._lock.release()\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696, in Function._initialize(self, args, kwds, add_initializers_to)\n    691 self._variable_creation_config = self._generate_scoped_tracing_options(\n    692     variable_capturing_scope,\n    693     tracing_compilation.ScopeType.VARIABLE_CREATION,\n    694 )\n    695 # Force the definition of the function for these arguments\n--&gt; 696 self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n    697     args, kwds, self._variable_creation_config\n    698 )\n    700 def invalid_creator_scope(*unused_args, **unused_kwds):\n    701   \"\"\"Disables variable creation.\"\"\"\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178, in trace_function(args, kwargs, tracing_options)\n    175     args = tracing_options.input_signature\n    176     kwargs = {}\n--&gt; 178   concrete_function = _maybe_define_function(\n    179       args, kwargs, tracing_options\n    180   )\n    182 if not tracing_options.bind_graph_to_function:\n    183   concrete_function._garbage_collector.release()  # pylint: disable=protected-access\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283, in _maybe_define_function(args, kwargs, tracing_options)\n    281 else:\n    282   target_func_type = lookup_func_type\n--&gt; 283 concrete_function = _create_concrete_function(\n    284     target_func_type, lookup_func_context, func_graph, tracing_options\n    285 )\n    287 if tracing_options.function_cache is not None:\n    288   tracing_options.function_cache.add(\n    289       concrete_function, current_func_context\n    290   )\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310, in _create_concrete_function(function_type, type_context, func_graph, tracing_options)\n    303   placeholder_bound_args = function_type.placeholder_arguments(\n    304       placeholder_context\n    305   )\n    307 disable_acd = tracing_options.attributes and tracing_options.attributes.get(\n    308     attributes_lib.DISABLE_ACD, False\n    309 )\n--&gt; 310 traced_func_graph = func_graph_module.func_graph_from_py_func(\n    311     tracing_options.name,\n    312     tracing_options.python_function,\n    313     placeholder_bound_args.args,\n    314     placeholder_bound_args.kwargs,\n    315     None,\n    316     func_graph=func_graph,\n    317     add_control_dependencies=not disable_acd,\n    318     arg_names=function_type_utils.to_arg_names(function_type),\n    319     create_placeholders=False,\n    320 )\n    322 transform.apply_func_graph_transforms(traced_func_graph)\n    324 graph_capture_container = traced_func_graph.function_captures\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\n   1056   return x\n   1058 _, original_func = tf_decorator.unwrap(python_func)\n-&gt; 1059 func_outputs = python_func(*func_args, **func_kwargs)\n   1061 # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n   1062 # TensorArrays and `None`s.\n   1063 func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599, in Function._generate_scoped_tracing_options.&lt;locals&gt;.wrapped_fn(*args, **kwds)\n    595 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access\n    596   # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    597   # the function a weak reference to itself to avoid a reference cycle.\n    598   with OptionalXlaContext(compile_with_xla):\n--&gt; 599     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    600   return out\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41, in py_func_from_autograph.&lt;locals&gt;.autograph_handler(*args, **kwargs)\n     39 \"\"\"Calls a converted version of original_func.\"\"\"\n     40 try:\n---&gt; 41   return api.converted_call(\n     42       original_func,\n     43       args,\n     44       kwargs,\n     45       options=converter.ConversionOptions(\n     46           recursive=True,\n     47           optional_features=autograph_options,\n     48           user_requested=True,\n     49       ))\n     50 except Exception as e:  # pylint:disable=broad-except\n     51   if hasattr(e, \"ag_error_metadata\"):\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339, in converted_call(f, args, kwargs, caller_fn_scope, options)\n    337 if is_autograph_artifact(f):\n    338   logging.log(2, 'Permanently allowed: %s: AutoGraph artifact', f)\n--&gt; 339   return _call_unconverted(f, args, kwargs, options)\n    341 # If this is a partial, unwrap it and redo all the checks.\n    342 if isinstance(f, functools.partial):\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459, in _call_unconverted(f, args, kwargs, options, update_cache)\n    456   return f.__self__.call(args, kwargs)\n    458 if kwargs is not None:\n--&gt; 459   return f(*args, **kwargs)\n    460 return f(*args)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643, in do_not_convert.&lt;locals&gt;.wrapper(*args, **kwargs)\n    641 def wrapper(*args, **kwargs):\n    642   with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n--&gt; 643     return func(*args, **kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121, in TensorFlowTrainer.make_train_function.&lt;locals&gt;.one_step_on_iterator(iterator)\n    119 \"\"\"Runs a single training step given a Dataset iterator.\"\"\"\n    120 data = next(iterator)\n--&gt; 121 outputs = self.distribute_strategy.run(\n    122     one_step_on_data, args=(data,)\n    123 )\n    124 outputs = reduce_per_replica(\n    125     outputs,\n    126     self.distribute_strategy,\n    127     reduction=\"auto\",\n    128 )\n    129 return outputs\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673, in StrategyBase.run(***failed resolving arguments***)\n   1668 with self.scope():\n   1669   # tf.distribute supports Eager functions, so AutoGraph should not be\n   1670   # applied when the caller is also in Eager mode.\n   1671   fn = autograph.tf_convert(\n   1672       fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n-&gt; 1673   return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263, in StrategyExtendedV1.call_for_each_replica(self, fn, args, kwargs)\n   3261   kwargs = {}\n   3262 with self._container_strategy().scope():\n-&gt; 3263   return self._call_for_each_replica(fn, args, kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061, in _DefaultDistributionExtended._call_for_each_replica(self, fn, args, kwargs)\n   4059 def _call_for_each_replica(self, fn, args, kwargs):\n   4060   with ReplicaContext(self._container_strategy(), replica_id_in_sync_group=0):\n-&gt; 4061     return fn(*args, **kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    148 filtered_tb = None\n    149 try:\n--&gt; 150   return fn(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833, in Function.__call__(self, *args, **kwds)\n    830 compiler = \"xla\" if self._jit_compile else \"nonXla\"\n    832 with OptionalXlaContext(self._jit_compile):\n--&gt; 833   result = self._call(*args, **kwds)\n    835 new_tracing_count = self.experimental_get_tracing_count()\n    836 without_tracing = (tracing_count == new_tracing_count)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889, in Function._call(self, *args, **kwds)\n    886 try:\n    887   # This is the first call of __call__, so we have to initialize.\n    888   initializers = []\n--&gt; 889   self._initialize(args, kwds, add_initializers_to=initializers)\n    890 finally:\n    891   # At this point we know that the initialization is complete (or less\n    892   # interestingly an exception was raised) so we no longer need a lock.\n    893   self._lock.release()\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696, in Function._initialize(self, args, kwds, add_initializers_to)\n    691 self._variable_creation_config = self._generate_scoped_tracing_options(\n    692     variable_capturing_scope,\n    693     tracing_compilation.ScopeType.VARIABLE_CREATION,\n    694 )\n    695 # Force the definition of the function for these arguments\n--&gt; 696 self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n    697     args, kwds, self._variable_creation_config\n    698 )\n    700 def invalid_creator_scope(*unused_args, **unused_kwds):\n    701   \"\"\"Disables variable creation.\"\"\"\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178, in trace_function(args, kwargs, tracing_options)\n    175     args = tracing_options.input_signature\n    176     kwargs = {}\n--&gt; 178   concrete_function = _maybe_define_function(\n    179       args, kwargs, tracing_options\n    180   )\n    182 if not tracing_options.bind_graph_to_function:\n    183   concrete_function._garbage_collector.release()  # pylint: disable=protected-access\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283, in _maybe_define_function(args, kwargs, tracing_options)\n    281 else:\n    282   target_func_type = lookup_func_type\n--&gt; 283 concrete_function = _create_concrete_function(\n    284     target_func_type, lookup_func_context, func_graph, tracing_options\n    285 )\n    287 if tracing_options.function_cache is not None:\n    288   tracing_options.function_cache.add(\n    289       concrete_function, current_func_context\n    290   )\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310, in _create_concrete_function(function_type, type_context, func_graph, tracing_options)\n    303   placeholder_bound_args = function_type.placeholder_arguments(\n    304       placeholder_context\n    305   )\n    307 disable_acd = tracing_options.attributes and tracing_options.attributes.get(\n    308     attributes_lib.DISABLE_ACD, False\n    309 )\n--&gt; 310 traced_func_graph = func_graph_module.func_graph_from_py_func(\n    311     tracing_options.name,\n    312     tracing_options.python_function,\n    313     placeholder_bound_args.args,\n    314     placeholder_bound_args.kwargs,\n    315     None,\n    316     func_graph=func_graph,\n    317     add_control_dependencies=not disable_acd,\n    318     arg_names=function_type_utils.to_arg_names(function_type),\n    319     create_placeholders=False,\n    320 )\n    322 transform.apply_func_graph_transforms(traced_func_graph)\n    324 graph_capture_container = traced_func_graph.function_captures\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\n   1056   return x\n   1058 _, original_func = tf_decorator.unwrap(python_func)\n-&gt; 1059 func_outputs = python_func(*func_args, **func_kwargs)\n   1061 # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n   1062 # TensorArrays and `None`s.\n   1063 func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599, in Function._generate_scoped_tracing_options.&lt;locals&gt;.wrapped_fn(*args, **kwds)\n    595 with default_graph._variable_creator_scope(scope, priority=50):  # pylint: disable=protected-access\n    596   # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    597   # the function a weak reference to itself to avoid a reference cycle.\n    598   with OptionalXlaContext(compile_with_xla):\n--&gt; 599     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n    600   return out\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41, in py_func_from_autograph.&lt;locals&gt;.autograph_handler(*args, **kwargs)\n     39 \"\"\"Calls a converted version of original_func.\"\"\"\n     40 try:\n---&gt; 41   return api.converted_call(\n     42       original_func,\n     43       args,\n     44       kwargs,\n     45       options=converter.ConversionOptions(\n     46           recursive=True,\n     47           optional_features=autograph_options,\n     48           user_requested=True,\n     49       ))\n     50 except Exception as e:  # pylint:disable=broad-except\n     51   if hasattr(e, \"ag_error_metadata\"):\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339, in converted_call(f, args, kwargs, caller_fn_scope, options)\n    337 if is_autograph_artifact(f):\n    338   logging.log(2, 'Permanently allowed: %s: AutoGraph artifact', f)\n--&gt; 339   return _call_unconverted(f, args, kwargs, options)\n    341 # If this is a partial, unwrap it and redo all the checks.\n    342 if isinstance(f, functools.partial):\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459, in _call_unconverted(f, args, kwargs, options, update_cache)\n    456   return f.__self__.call(args, kwargs)\n    458 if kwargs is not None:\n--&gt; 459   return f(*args, **kwargs)\n    460 return f(*args)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643, in do_not_convert.&lt;locals&gt;.wrapper(*args, **kwargs)\n    641 def wrapper(*args, **kwargs):\n    642   with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n--&gt; 643     return func(*args, **kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108, in TensorFlowTrainer.make_train_function.&lt;locals&gt;.one_step_on_data(data)\n    105 @tf.autograph.experimental.do_not_convert\n    106 def one_step_on_data(data):\n    107     \"\"\"Runs a single training step on a batch of data.\"\"\"\n--&gt; 108     return self.train_step(data)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:70, in TensorFlowTrainer.train_step(self, data)\n     68 if self.trainable_weights:\n     69     trainable_weights = self.trainable_weights\n---&gt; 70     gradients = tape.gradient(loss, trainable_weights)\n     72     # Update weights\n     73     self.optimizer.apply_gradients(zip(gradients, trainable_weights))\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)\n   1060   output_gradients = (\n   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(\n   1062           output_gradients))\n   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)\n   1064                       for x in output_gradients]\n-&gt; 1066 flat_grad = imperative_grad.imperative_grad(\n   1067     self._tape,\n   1068     flat_targets,\n   1069     flat_sources,\n   1070     output_gradients=output_gradients,\n   1071     sources_raw=flat_sources_raw,\n   1072     unconnected_gradients=unconnected_gradients)\n   1074 if not self._persistent:\n   1075   # Keep track of watched variables before setting tape to None\n   1076   self._watched_variables = self._tape.watched_variables()\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n     63 except ValueError:\n     64   raise ValueError(\n     65       \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n---&gt; 67 return pywrap_tfe.TFE_Py_TapeGradient(\n     68     tape._tape,  # pylint: disable=protected-access\n     69     target,\n     70     sources,\n     71     output_gradients,\n     72     sources_raw,\n     73     compat.as_str(unconnected_gradients.value))\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:148, in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\n    146     gradient_name_scope += forward_pass_name_scope + \"/\"\n    147   with ops.name_scope(gradient_name_scope):\n--&gt; 148     return grad_fn(mock_op, *out_grads)\n    149 else:\n    150   return grad_fn(mock_op, *out_grads)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1436, in _MulGrad(op, grad)\n   1433 else:\n   1434   gy = gen_math_ops.mul(math_ops.conj(x), grad)\n-&gt; 1436 return _ReduceGradientArgs(x, y, gx, gy)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:144, in _ReduceGradientArgs(x, y, gx, gy)\n    142   bx, by = SmartBroadcastGradientArgs(x, y)\n    143   gx = _ReduceGradientArg(gx, bx)\n--&gt; 144   gy = _ReduceGradientArg(gy, by)\n    145 return gx, gy\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:134, in _ReduceGradientArg(grad, shape_axes_must_reduce)\n    129 shape, axes, must_reduce = shape_axes_must_reduce\n    130 if grad is not None and must_reduce:\n    131   # Applying keepdims=True in presence of unknown axes opens up some\n    132   # opportunities for optimizations. For example, _SumGrad below won't have to\n    133   # emit extra ops to recover reduced indices for broadcasting.\n--&gt; 134   grad = math_ops.reduce_sum(grad, axes, keepdims=True)\n    135   grad = array_ops.reshape(grad, shape)\n    136 return grad\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88, in weak_tensor_unary_op_wrapper.&lt;locals&gt;.wrapper(*args, **kwargs)\n     86 def wrapper(*args, **kwargs):\n     87   if not ops.is_auto_dtype_conversion_enabled():\n---&gt; 88     return op(*args, **kwargs)\n     89   bound_arguments = signature.bind(*args, **kwargs)\n     90   bound_arguments.apply_defaults()\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    148 filtered_tb = None\n    149 try:\n--&gt; 150   return fn(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260, in add_dispatch_support.&lt;locals&gt;.decorator.&lt;locals&gt;.op_dispatch_handler(*args, **kwargs)\n   1258 # Fallback dispatch system (dispatch v1):\n   1259 try:\n-&gt; 1260   return dispatch_target(*args, **kwargs)\n   1261 except (TypeError, ValueError):\n   1262   # Note: convert_to_eager_tensor currently raises a ValueError, not a\n   1263   # TypeError, when given unexpected types.  So we need to catch both.\n   1264   result = dispatch(op_dispatch_handler, args, kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2215, in reduce_sum(input_tensor, axis, keepdims, name)\n   2152 @tf_export(\"math.reduce_sum\", \"reduce_sum\", v1=[])\n   2153 @dispatch.add_dispatch_support\n   2154 def reduce_sum(input_tensor, axis=None, keepdims=False, name=None):\n   2155   \"\"\"Computes the sum of elements across dimensions of a tensor.\n   2156 \n   2157   This is the reduction operation for the elementwise `tf.math.add` op.\n   (...)\n   2212   @end_compatibility\n   2213   \"\"\"\n-&gt; 2215   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n   2216                               _ReductionDims(input_tensor, axis))\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2227, in reduce_sum_with_dims(input_tensor, axis, keepdims, name, dims)\n   2219 def reduce_sum_with_dims(input_tensor,\n   2220                          axis=None,\n   2221                          keepdims=False,\n   2222                          name=None,\n   2223                          dims=None):\n   2224   keepdims = False if keepdims is None else bool(keepdims)\n   2225   return _may_reduce_to_scalar(\n   2226       keepdims, axis,\n-&gt; 2227       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13763, in _sum(input, axis, keep_dims, name)\n  13761   keep_dims = False\n  13762 keep_dims = _execute.make_bool(keep_dims, \"keep_dims\")\n&gt; 13763 _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  13764       \"Sum\", input=input, reduction_indices=axis, keep_dims=keep_dims,\n  13765              name=name)\n  13766 _result = _outputs[:]\n  13767 if _execute.must_record_gradient():\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:778, in _apply_op_helper(op_type_name, name, **keywords)\n    776 with g.as_default(), ops.name_scope(name) as scope:\n    777   if fallback:\n--&gt; 778     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n    779                            keywords, default_type_attr_map, attrs, inputs,\n    780                            input_types)\n    781     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n    782                            default_type_attr_map, attrs)\n    783     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:531, in _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\n    529 inferred = None\n    530 try:\n--&gt; 531   inferred = ops.convert_to_tensor(\n    532       values, name=input_arg.name, as_ref=input_arg.is_ref)\n    533 except TypeError as err:\n    534   # When converting a python object such as a list of Dimensions, we\n    535   # need a dtype to be specified, thus tensor conversion may throw\n    536   # an exception which we will ignore and try again below.\n    537   pass\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183, in trace_wrapper.&lt;locals&gt;.inner_wrapper.&lt;locals&gt;.wrapped(*args, **kwargs)\n    181   with Trace(trace_name, **trace_kwargs):\n    182     return func(*args, **kwargs)\n--&gt; 183 return func(*args, **kwargs)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:713, in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\n    711 # TODO(b/142518781): Fix all call-sites and remove redundant arg\n    712 preferred_dtype = preferred_dtype or dtype_hint\n--&gt; 713 return tensor_conversion_registry.convert(\n    714     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n    715 )\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234, in convert(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\n    225       raise RuntimeError(\n    226           _add_error_prefix(\n    227               f\"Conversion function {conversion_func!r} for type \"\n   (...)\n    230               f\"actual = {ret.dtype.base_dtype.name}\",\n    231               name=name))\n    233 if ret is None:\n--&gt; 234   ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    236 if ret is NotImplemented:\n    237   continue\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29, in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n     26 from tensorflow.python.framework import constant_op  # pylint: disable=g-import-not-at-top\n     28 _ = as_ref\n---&gt; 29 return constant_op.constant(v, dtype=dtype, name=name)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142, in weak_tensor_binary_op_wrapper.&lt;locals&gt;.wrapper(*args, **kwargs)\n    140 def wrapper(*args, **kwargs):\n    141   if not ops.is_auto_dtype_conversion_enabled():\n--&gt; 142     return op(*args, **kwargs)\n    143   bound_arguments = signature.bind(*args, **kwargs)\n    144   bound_arguments.apply_defaults()\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276, in constant(value, dtype, shape, name)\n    177 @tf_export(\"constant\", v1=[])\n    178 def constant(\n    179     value, dtype=None, shape=None, name=\"Const\"\n    180 ) -&gt; Union[ops.Operation, ops._EagerTensorBase]:\n    181   \"\"\"Creates a constant tensor from a tensor-like object.\n    182 \n    183   Note: All eager `tf.Tensor` values are immutable (in contrast to\n   (...)\n    274     ValueError: if called on a symbolic tensor.\n    275   \"\"\"\n--&gt; 276   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n    277                         allow_broadcast=True)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:291, in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\n    288       return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n    289   return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n--&gt; 291 const_tensor = ops._create_graph_constant(  # pylint: disable=protected-access\n    292     value, dtype, shape, name, verify_shape, allow_broadcast\n    293 )\n    294 return const_tensor\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:281, in _create_graph_constant(value, dtype, shape, name, verify_shape, allow_broadcast)\n    279 dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n    280 attrs = {\"value\": tensor_value, \"dtype\": dtype_value}\n--&gt; 281 const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n    282     \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\n    284 if op_callbacks.should_invoke_op_callbacks():\n    285   # TODO(b/147670703): Once the special-op creation code paths\n    286   # are unified. Remove this `if` block.\n    287   callback_outputs = op_callbacks.invoke_op_callbacks(\n    288       \"Const\", tuple(), attrs, (const_tensor,), op_name=name, graph=g)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670, in FuncGraph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\n    668   inp = self.capture(inp)\n    669   captured_inputs.append(inp)\n--&gt; 670 return super()._create_op_internal(  # pylint: disable=protected-access\n    671     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n    672     compute_device)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2682, in Graph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\n   2679 # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\n   2680 # Session.run call cannot occur between creating and mutating the op.\n   2681 with self._mutation_lock():\n-&gt; 2682   ret = Operation.from_node_def(\n   2683       node_def,\n   2684       self,\n   2685       inputs=inputs,\n   2686       output_types=dtypes,\n   2687       control_inputs=control_inputs,\n   2688       input_types=input_types,\n   2689       original_op=self._default_original_op,\n   2690       op_def=op_def,\n   2691   )\n   2692   self._create_op_helper(ret, compute_device=compute_device)\n   2693 return ret\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1177, in Operation.from_node_def(***failed resolving arguments***)\n   1174     control_input_ops.append(control_op)\n   1176 # Initialize c_op from node_def and other inputs\n-&gt; 1177 c_op = _create_c_op(g, node_def, inputs, control_input_ops, op_def=op_def)\n   1178 self = Operation(c_op, SymbolicTensor)\n   1179 self._init(g)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    148 filtered_tb = None\n    149 try:\n--&gt; 150   return fn(*args, **kwargs)\n    151 except Exception as e:\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\n\nFile c:\\Users\\junwa\\Anaconda3\\envs\\wise\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1007, in _create_c_op(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\n   1005 # pylint: disable=protected-access\n   1006 with graph._c_graph.get() as c_graph:\n-&gt; 1007   op_desc = pywrap_tf_session.TF_NewOperation(c_graph,\n   1008                                               compat.as_str(node_def.op),\n   1009                                               compat.as_str(node_def.name))\n   1010 if node_def.device:\n   1011   pywrap_tf_session.TF_SetDevice(op_desc, compat.as_str(node_def.device))\n\nKeyboardInterrupt: </pre>"},{"location":"note/Computer_Vision/Image_Segmentation/","title":"Image Segmentation","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#models","title":"Models","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#u-net","title":"U-Net","text":"<p>Convolutional Networks for Biomedical Image Segmentation </p> <ul> <li>Left half: image size keeps reducing, channel size keeps increasing</li> <li>Right half: <ul> <li>upscale the downscaled image, back to the original height and width but with as many channels as there are classes</li> <li>leverage information from the corresponding layers in the left half using skip connections </li> </ul> </li> </ul>"},{"location":"note/Computer_Vision/Image_Segmentation/#mask-r-cnn","title":"Mask R-CNN","text":"<p>Mask R-CNN </p> <ul> <li>mask: 0 or 1, indicates whether the pixel contains an object or not</li> </ul>"},{"location":"note/Computer_Vision/Image_Segmentation/#sam","title":"SAM","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#maskformer","title":"MaskFormer","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#oneformer","title":"OneFormer","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/Image_Segmentation/#transposed-convolution","title":"Transposed Convolution","text":"<p>https://makeyourownneuralnetwork.blogspot.com/2020/02/calculating-output-size-of-convolutions.html - Internally, padding is calculated as dilation * (kernel_size \u2013 1) - padding. Hence, it is 1*(2-1)-0 = 1, where we add zero padding of 1 to both dimensions of the input array</p> <p>Convoultion output size \\(\\text{output size} = \\lfloor \\frac{(\\text{input size}) + 2*\\text{padding} - (\\text{kernel size - 1}) - 1}{\\text{stride}} \\rfloor\\)</p> <p>Transposed convolution output size \\(\\text{output size} = (\\text{input size} - 1)*\\text{stride} - 2*\\text{padding} + (\\text{kernel size} - 1) + 1\\)</p>"},{"location":"note/Computer_Vision/Image_Segmentation/#roi-align","title":"RoI Align","text":"<p>https://erdem.pl/2020/02/understanding-region-of-interest-part-2-ro-i-align - RoI Pooling has information loss, since certain parts of the region have more weight than others - get a more accurate representation of the region proposal</p>"},{"location":"note/Computer_Vision/Image_Segmentation/#fully-convolutional-network-fcn","title":"Fully Convolutional Network (FCN)","text":"<ul> <li>e.g U-Net</li> <li>can accept inputs of any size<ul> <li>The reason is that when using a convolutional layer, you select the size of the filter kernels, which are independent of the image/layer input size (provided that images smaller than the kernels are padded appropriately).</li> <li>once the kernel and step sizes are described, the convolution at each layer can generate appropriate dimension outputs according to the corresponding inputs https://d2l.ai/chapter_computer-vision/fcn.html</li> </ul> </li> </ul>"},{"location":"note/Computer_Vision/Image_Segmentation/mask-r-cnn/","title":"Mask r cnn","text":"In\u00a0[\u00a0]: Copied! <pre>!wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/images.tar\n!wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar\n!tar -xf images.tar\n!tar -xf annotations_instance.tar\n!rm images.tar annotations_instance.tar\n!pip install -qU torch_snippets\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/engine.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/utils.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/transforms.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_eval.py\n!wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_utils.py\n!pip install -q -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n</pre> !wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/images.tar !wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar !tar -xf images.tar !tar -xf annotations_instance.tar !rm images.tar annotations_instance.tar !pip install -qU torch_snippets !wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/engine.py !wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/utils.py !wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/transforms.py !wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_eval.py !wget --quiet https://raw.githubusercontent.com/pytorch/vision/release/0.12/references/detection/coco_utils.py !pip install -q -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' <pre>     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48 kB 2.6 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60 kB 5.9 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.9 MB 30.2 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 78 kB 6.3 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 948 kB 29.1 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58 kB 5.3 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232 kB 48.2 MB/s \n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51 kB 6.4 MB/s \n  Building wheel for typing (setup.py) ... done\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nalbumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.4.0 which is incompatible.\n  Building wheel for pycocotools (setup.py) ... done\n</pre> In\u00a0[\u00a0]: Copied! <pre>from torch_snippets import *\nfrom torch_snippets.inspector import inspect\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n</pre> from torch_snippets import * from torch_snippets.inspector import inspect import torchvision from torchvision.models.detection.faster_rcnn import FastRCNNPredictor from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor  from engine import train_one_epoch, evaluate import utils import transforms as T device = 'cuda' if torch.cuda.is_available() else 'cpu' In\u00a0[\u00a0]: Copied! <pre>all_images = Glob('images/training')\nall_annots = Glob('annotations_instance/training')\n</pre> all_images = Glob('images/training') all_annots = Glob('annotations_instance/training') In\u00a0[\u00a0]: Copied! <pre>f = 'ADE_train_00014301'\n\nim = read(find(f, all_images), 1)\nan = read(find(f, all_annots), 1).transpose(2,0,1)\nr,g,b = an\nnzs = np.nonzero(r==4) # 4 stands for person\ninstances = np.unique(g[nzs])\nmasks = np.zeros((len(instances), *r.shape))\nfor ix,_id in enumerate(instances):\n    masks[ix] = g==_id\n\nsubplots([im, *masks], sz=20)\n</pre> f = 'ADE_train_00014301'  im = read(find(f, all_images), 1) an = read(find(f, all_annots), 1).transpose(2,0,1) r,g,b = an nzs = np.nonzero(r==4) # 4 stands for person instances = np.unique(g[nzs]) masks = np.zeros((len(instances), *r.shape)) for ix,_id in enumerate(instances):     masks[ix] = g==_id  subplots([im, *masks], sz=20) In\u00a0[\u00a0]: Copied! <pre>annots = []\nfor ann in Tqdm(all_annots[:5000]):\n    _ann = read(ann, 1).transpose(2,0,1)\n    r,g,b = _ann\n    if 4 not in np.unique(r): continue\n    annots.append(ann)\n</pre> annots = [] for ann in Tqdm(all_annots[:5000]):     _ann = read(ann, 1).transpose(2,0,1)     r,g,b = _ann     if 4 not in np.unique(r): continue     annots.append(ann) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:27&lt;00:00, 179.49it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n_annots = stems(annots)\ntrn_items, val_items = train_test_split(_annots, random_state=2)\n</pre> from sklearn.model_selection import train_test_split _annots = stems(annots) trn_items, val_items = train_test_split(_annots, random_state=2) In\u00a0[\u00a0]: Copied! <pre>def get_transform(train):\n    image_transforms = []\n    image_transforms.append(T.PILToTensor())\n    if train:\n        image_transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(image_transforms)\n</pre> def get_transform(train):     image_transforms = []     image_transforms.append(T.PILToTensor())     if train:         image_transforms.append(T.RandomHorizontalFlip(0.5))     return T.Compose(image_transforms) In\u00a0[\u00a0]: Copied! <pre>class MasksDataset(Dataset):\n    def __init__(self, items, transforms, N):\n        self.items = items\n        self.transforms = transforms\n        self.N = N\n    def get_mask(self, path):\n        an = read(path, 1).transpose(2,0,1)\n        r,g,b = an\n        nzs = np.nonzero(r==4)\n        instances = np.unique(g[nzs])\n        masks = np.zeros((len(instances), *r.shape))\n        for ix,_id in enumerate(instances):\n            masks[ix] = g==_id\n        return masks\n    def __getitem__(self, ix):\n        _id = self.items[ix]\n        img_path = f'images/training/{_id}.jpg'\n        mask_path = f'annotations_instance/training/{_id}.png'\n        masks = self.get_mask(mask_path)\n        obj_ids = np.arange(1, len(masks)+1)\n        img = Image.open(img_path).convert(\"RGB\")\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            obj_pixels = np.where(masks[i])\n            xmin = np.min(obj_pixels[1])\n            xmax = np.max(obj_pixels[1])\n            ymin = np.min(obj_pixels[0])\n            ymax = np.max(obj_pixels[0])\n            if (((xmax-xmin)&lt;=10) | (ymax-ymin)&lt;=10):\n                xmax = xmin+10\n                ymax = ymin+10\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n        image_id = torch.tensor([ix])\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :\n          img = img/255.\n        return img, target\n    def __len__(self):\n        return self.N\n    def choose(self):\n        return self[randint(len(self))]\n</pre> class MasksDataset(Dataset):     def __init__(self, items, transforms, N):         self.items = items         self.transforms = transforms         self.N = N     def get_mask(self, path):         an = read(path, 1).transpose(2,0,1)         r,g,b = an         nzs = np.nonzero(r==4)         instances = np.unique(g[nzs])         masks = np.zeros((len(instances), *r.shape))         for ix,_id in enumerate(instances):             masks[ix] = g==_id         return masks     def __getitem__(self, ix):         _id = self.items[ix]         img_path = f'images/training/{_id}.jpg'         mask_path = f'annotations_instance/training/{_id}.png'         masks = self.get_mask(mask_path)         obj_ids = np.arange(1, len(masks)+1)         img = Image.open(img_path).convert(\"RGB\")         num_objs = len(obj_ids)         boxes = []         for i in range(num_objs):             obj_pixels = np.where(masks[i])             xmin = np.min(obj_pixels[1])             xmax = np.max(obj_pixels[1])             ymin = np.min(obj_pixels[0])             ymax = np.max(obj_pixels[0])             if (((xmax-xmin)&lt;=10) | (ymax-ymin)&lt;=10):                 xmax = xmin+10                 ymax = ymin+10             boxes.append([xmin, ymin, xmax, ymax])         boxes = torch.as_tensor(boxes, dtype=torch.float32)         labels = torch.ones((num_objs,), dtype=torch.int64)         masks = torch.as_tensor(masks, dtype=torch.uint8)         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])         iscrowd = torch.zeros((num_objs,), dtype=torch.int64)         image_id = torch.tensor([ix])         target = {}         target[\"boxes\"] = boxes         target[\"labels\"] = labels         target[\"masks\"] = masks         target[\"image_id\"] = image_id         target[\"area\"] = area         target[\"iscrowd\"] = iscrowd         if self.transforms is not None:             img, target = self.transforms(img, target)         if (img.dtype == torch.float32) or (img.dtype == torch.uint8) :           img = img/255.         return img, target     def __len__(self):         return self.N     def choose(self):         return self[randint(len(self))] In\u00a0[\u00a0]: Copied! <pre>x = MasksDataset(trn_items, get_transform(train=True), N=100)\nim,targ = x[0]\ninspect(im,targ)\nsubplots([im, *targ['masks']], sz=10)\n</pre> x = MasksDataset(trn_items, get_transform(train=True), N=100) im,targ = x[0] inspect(im,targ) subplots([im, *targ['masks']], sz=10) <pre> Tensor\tShape: torch.Size([3, 512, 684])\tMin: 0.000\tMax: 1.000\tMean: 0.486\tdtype: torch.float32\n Dict Of 6 items\n\tBOXES:\n\t Tensor\tShape: torch.Size([3, 4])\tMin: 42.000\tMax: 477.000\tMean: 259.417\tdtype: torch.float32\n\tLABELS:\n\t Tensor\tShape: torch.Size([3])\tMin: 1.000\tMax: 1.000\tMean: 1.000\tdtype: torch.int64\n\tMASKS:\n\t Tensor\tShape: torch.Size([3, 512, 684])\tMin: 0.000\tMax: 1.000\tMean: 0.008\tdtype: torch.uint8\n\tIMAGE_ID:\n\t Tensor\tShape: torch.Size([1])\tMin: 0.000\tMax: 0.000\tMean: 0.000\tdtype: torch.int64\n\tAREA:\n\t Tensor\tShape: torch.Size([3])\tMin: 1932.000\tMax: 10688.000\tMean: 5270.667\tdtype: torch.float32\n... ... 1 more item(s)\n</pre> In\u00a0[\u00a0]: Copied! <pre>def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,num_classes)\n    return model\n</pre> def get_model_instance_segmentation(num_classes):     # load an instance segmentation model pre-trained pre-trained on COCO     model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)      # get number of input features for the classifier     in_features = model.roi_heads.box_predictor.cls_score.in_features     # replace the pre-trained head with a new one     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)      # now get the number of input features for the mask classifier     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels     hidden_layer = 256     # and replace the mask predictor with a new one     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,                                                        hidden_layer,num_classes)     return model In\u00a0[\u00a0]: Copied! <pre>model = get_model_instance_segmentation(2).to(device)\nmodel\n</pre> model = get_model_instance_segmentation(2).to(device) model Out[\u00a0]: <pre>MaskRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n      )\n      (layer_blocks): ModuleList(\n        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n    )\n    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n    (mask_head): MaskRCNNHeads(\n      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu1): ReLU(inplace=True)\n      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu2): ReLU(inplace=True)\n      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu3): ReLU(inplace=True)\n      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (relu4): ReLU(inplace=True)\n    )\n    (mask_predictor): MaskRCNNPredictor(\n      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (relu): ReLU(inplace=True)\n      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)</pre> In\u00a0[\u00a0]: Copied! <pre>dataset = MasksDataset(trn_items, get_transform(train=True), N=len(trn_items))\ndataset_test = MasksDataset(val_items, get_transform(train=False), N=len(val_items))\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=0,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n    collate_fn=utils.collate_fn)\n</pre> dataset = MasksDataset(trn_items, get_transform(train=True), N=len(trn_items)) dataset_test = MasksDataset(val_items, get_transform(train=False), N=len(val_items))  # define training and validation data loaders data_loader = torch.utils.data.DataLoader(     dataset, batch_size=2, shuffle=True, num_workers=0,     collate_fn=utils.collate_fn)  data_loader_test = torch.utils.data.DataLoader(     dataset_test, batch_size=1, shuffle=False, num_workers=0,     collate_fn=utils.collate_fn) In\u00a0[\u00a0]: Copied! <pre>num_classes = 2\nmodel = get_model_instance_segmentation(num_classes).to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n# and a learning rate scheduler\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                step_size=3,\n                                                gamma=0.1)\n</pre> num_classes = 2 model = get_model_instance_segmentation(num_classes).to(device) params = [p for p in model.parameters() if p.requires_grad] optimizer = torch.optim.SGD(params, lr=0.005,                             momentum=0.9, weight_decay=0.0005) # and a learning rate scheduler lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,                                                 step_size=3,                                                 gamma=0.1) In\u00a0[\u00a0]: Copied! <pre>num_epochs = 1\n\ntrn_history = []\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    res = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    trn_history.append(res)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset \n    res = evaluate(model, data_loader_test, device=device)\n</pre> num_epochs = 1  trn_history = [] for epoch in range(num_epochs):     # train for one epoch, printing every 10 iterations     res = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)     trn_history.append(res)     # update the learning rate     lr_scheduler.step()     # evaluate on the test dataset      res = evaluate(model, data_loader_test, device=device) <pre>Epoch: [0]  [  0/482]  eta: 0:19:00  lr: 0.000015  loss: 5.5752 (5.5752)  loss_classifier: 0.8792 (0.8792)  loss_box_reg: 0.8384 (0.8384)  loss_mask: 3.7591 (3.7591)  loss_objectness: 0.0579 (0.0579)  loss_rpn_box_reg: 0.0406 (0.0406)  time: 2.3664  data: 0.2396  max mem: 5656\nEpoch: [0]  [ 10/482]  eta: 0:17:21  lr: 0.000119  loss: 4.4057 (4.4962)  loss_classifier: 0.8939 (0.8836)  loss_box_reg: 0.3374 (0.3255)  loss_mask: 3.1220 (3.2297)  loss_objectness: 0.0395 (0.0374)  loss_rpn_box_reg: 0.0189 (0.0200)  time: 2.2061  data: 0.0599  max mem: 7010\nEpoch: [0]  [ 20/482]  eta: 0:16:13  lr: 0.000223  loss: 2.3401 (3.1059)  loss_classifier: 0.5854 (0.6268)  loss_box_reg: 0.2879 (0.3154)  loss_mask: 1.5016 (2.1052)  loss_objectness: 0.0275 (0.0391)  loss_rpn_box_reg: 0.0097 (0.0193)  time: 2.0950  data: 0.0462  max mem: 7010\nEpoch: [0]  [ 30/482]  eta: 0:15:52  lr: 0.000327  loss: 1.3235 (2.5105)  loss_classifier: 0.2697 (0.5083)  loss_box_reg: 0.3295 (0.3287)  loss_mask: 0.6481 (1.6156)  loss_objectness: 0.0256 (0.0382)  loss_rpn_box_reg: 0.0169 (0.0197)  time: 2.0544  data: 0.0498  max mem: 7010\nEpoch: [0]  [ 40/482]  eta: 0:15:16  lr: 0.000431  loss: 1.1525 (2.1411)  loss_classifier: 0.2526 (0.4368)  loss_box_reg: 0.3490 (0.3244)  loss_mask: 0.4338 (1.3189)  loss_objectness: 0.0398 (0.0401)  loss_rpn_box_reg: 0.0169 (0.0209)  time: 2.0381  data: 0.0498  max mem: 7010\nEpoch: [0]  [ 50/482]  eta: 0:14:54  lr: 0.000535  loss: 0.9247 (1.9162)  loss_classifier: 0.1698 (0.3886)  loss_box_reg: 0.2237 (0.3282)  loss_mask: 0.3867 (1.1401)  loss_objectness: 0.0281 (0.0392)  loss_rpn_box_reg: 0.0114 (0.0202)  time: 2.0131  data: 0.0610  max mem: 7010\nEpoch: [0]  [ 60/482]  eta: 0:14:28  lr: 0.000638  loss: 0.8918 (1.7327)  loss_classifier: 0.1380 (0.3478)  loss_box_reg: 0.2182 (0.3140)  loss_mask: 0.3384 (1.0056)  loss_objectness: 0.0281 (0.0426)  loss_rpn_box_reg: 0.0114 (0.0226)  time: 2.0290  data: 0.0806  max mem: 7010\nEpoch: [0]  [ 70/482]  eta: 0:14:18  lr: 0.000742  loss: 0.6852 (1.5877)  loss_classifier: 0.1196 (0.3171)  loss_box_reg: 0.2182 (0.3067)  loss_mask: 0.2703 (0.9032)  loss_objectness: 0.0281 (0.0394)  loss_rpn_box_reg: 0.0066 (0.0212)  time: 2.1122  data: 0.0643  max mem: 7010\nEpoch: [0]  [ 80/482]  eta: 0:13:59  lr: 0.000846  loss: 0.7877 (1.5065)  loss_classifier: 0.1280 (0.2988)  loss_box_reg: 0.3146 (0.3121)  loss_mask: 0.2894 (0.8359)  loss_objectness: 0.0127 (0.0377)  loss_rpn_box_reg: 0.0114 (0.0220)  time: 2.1728  data: 0.0598  max mem: 7397\nEpoch: [0]  [ 90/482]  eta: 0:13:40  lr: 0.000950  loss: 0.9356 (1.4376)  loss_classifier: 0.1458 (0.2816)  loss_box_reg: 0.3707 (0.3133)  loss_mask: 0.3574 (0.7842)  loss_objectness: 0.0190 (0.0365)  loss_rpn_box_reg: 0.0180 (0.0220)  time: 2.1258  data: 0.0642  max mem: 7397\nEpoch: [0]  [100/482]  eta: 0:13:17  lr: 0.001054  loss: 0.9208 (1.3765)  loss_classifier: 0.1397 (0.2671)  loss_box_reg: 0.3707 (0.3114)  loss_mask: 0.3647 (0.7410)  loss_objectness: 0.0144 (0.0351)  loss_rpn_box_reg: 0.0127 (0.0219)  time: 2.0945  data: 0.0482  max mem: 7397\nEpoch: [0]  [110/482]  eta: 0:12:57  lr: 0.001158  loss: 0.7449 (1.3182)  loss_classifier: 0.1145 (0.2534)  loss_box_reg: 0.3175 (0.3092)  loss_mask: 0.3104 (0.7013)  loss_objectness: 0.0113 (0.0331)  loss_rpn_box_reg: 0.0096 (0.0213)  time: 2.0841  data: 0.0597  max mem: 7397\nEpoch: [0]  [120/482]  eta: 0:12:37  lr: 0.001262  loss: 0.8370 (1.2859)  loss_classifier: 0.1253 (0.2448)  loss_box_reg: 0.3549 (0.3110)  loss_mask: 0.3249 (0.6721)  loss_objectness: 0.0138 (0.0340)  loss_rpn_box_reg: 0.0193 (0.0240)  time: 2.1204  data: 0.0764  max mem: 7397\nEpoch: [0]  [130/482]  eta: 0:12:17  lr: 0.001365  loss: 0.7976 (1.2434)  loss_classifier: 0.1197 (0.2353)  loss_box_reg: 0.2635 (0.3046)  loss_mask: 0.3405 (0.6463)  loss_objectness: 0.0226 (0.0334)  loss_rpn_box_reg: 0.0215 (0.0238)  time: 2.1225  data: 0.0722  max mem: 7397\nEpoch: [0]  [140/482]  eta: 0:11:59  lr: 0.001469  loss: 0.7081 (1.2095)  loss_classifier: 0.1068 (0.2274)  loss_box_reg: 0.2080 (0.3000)  loss_mask: 0.3405 (0.6248)  loss_objectness: 0.0217 (0.0327)  loss_rpn_box_reg: 0.0171 (0.0246)  time: 2.1536  data: 0.0611  max mem: 7397\nEpoch: [0]  [150/482]  eta: 0:11:37  lr: 0.001573  loss: 0.7522 (1.1775)  loss_classifier: 0.0847 (0.2193)  loss_box_reg: 0.1924 (0.2910)  loss_mask: 0.3606 (0.6111)  loss_objectness: 0.0174 (0.0320)  loss_rpn_box_reg: 0.0199 (0.0241)  time: 2.1382  data: 0.0556  max mem: 7397\nEpoch: [0]  [160/482]  eta: 0:11:14  lr: 0.001677  loss: 0.7844 (1.1583)  loss_classifier: 0.1275 (0.2149)  loss_box_reg: 0.1283 (0.2864)  loss_mask: 0.3421 (0.5944)  loss_objectness: 0.0413 (0.0379)  loss_rpn_box_reg: 0.0124 (0.0246)  time: 2.0422  data: 0.0565  max mem: 7397\nEpoch: [0]  [170/482]  eta: 0:10:53  lr: 0.001781  loss: 0.7441 (1.1383)  loss_classifier: 0.1275 (0.2104)  loss_box_reg: 0.1731 (0.2818)  loss_mask: 0.3421 (0.5821)  loss_objectness: 0.0512 (0.0393)  loss_rpn_box_reg: 0.0140 (0.0247)  time: 2.0471  data: 0.0580  max mem: 7397\nEpoch: [0]  [180/482]  eta: 0:10:30  lr: 0.001885  loss: 0.7389 (1.1167)  loss_classifier: 0.1088 (0.2058)  loss_box_reg: 0.1782 (0.2773)  loss_mask: 0.3572 (0.5695)  loss_objectness: 0.0371 (0.0394)  loss_rpn_box_reg: 0.0169 (0.0247)  time: 2.0214  data: 0.0548  max mem: 7397\nEpoch: [0]  [190/482]  eta: 0:10:09  lr: 0.001988  loss: 0.7369 (1.0932)  loss_classifier: 0.1137 (0.2013)  loss_box_reg: 0.1782 (0.2730)  loss_mask: 0.3162 (0.5559)  loss_objectness: 0.0230 (0.0385)  loss_rpn_box_reg: 0.0142 (0.0244)  time: 2.0172  data: 0.0557  max mem: 7472\nEpoch: [0]  [200/482]  eta: 0:09:49  lr: 0.002092  loss: 0.6406 (1.0779)  loss_classifier: 0.1390 (0.1998)  loss_box_reg: 0.1775 (0.2714)  loss_mask: 0.3061 (0.5442)  loss_objectness: 0.0176 (0.0383)  loss_rpn_box_reg: 0.0076 (0.0242)  time: 2.1139  data: 0.0526  max mem: 7472\nEpoch: [0]  [210/482]  eta: 0:09:27  lr: 0.002196  loss: 0.6591 (1.0593)  loss_classifier: 0.1058 (0.1949)  loss_box_reg: 0.1948 (0.2685)  loss_mask: 0.3317 (0.5347)  loss_objectness: 0.0178 (0.0372)  loss_rpn_box_reg: 0.0128 (0.0239)  time: 2.0869  data: 0.0533  max mem: 7472\nEpoch: [0]  [220/482]  eta: 0:09:05  lr: 0.002300  loss: 0.5995 (1.0413)  loss_classifier: 0.0771 (0.1903)  loss_box_reg: 0.1384 (0.2628)  loss_mask: 0.3158 (0.5275)  loss_objectness: 0.0091 (0.0368)  loss_rpn_box_reg: 0.0132 (0.0239)  time: 2.0243  data: 0.0517  max mem: 7472\nEpoch: [0]  [230/482]  eta: 0:08:45  lr: 0.002404  loss: 0.5488 (1.0206)  loss_classifier: 0.0771 (0.1861)  loss_box_reg: 0.1384 (0.2585)  loss_mask: 0.3013 (0.5169)  loss_objectness: 0.0074 (0.0357)  loss_rpn_box_reg: 0.0060 (0.0234)  time: 2.0572  data: 0.0482  max mem: 7472\nEpoch: [0]  [240/482]  eta: 0:08:24  lr: 0.002508  loss: 0.5726 (1.0085)  loss_classifier: 0.0902 (0.1839)  loss_box_reg: 0.1435 (0.2558)  loss_mask: 0.3014 (0.5098)  loss_objectness: 0.0091 (0.0353)  loss_rpn_box_reg: 0.0133 (0.0237)  time: 2.1009  data: 0.0742  max mem: 7472\nEpoch: [0]  [250/482]  eta: 0:08:03  lr: 0.002612  loss: 0.5975 (0.9965)  loss_classifier: 0.0902 (0.1816)  loss_box_reg: 0.1355 (0.2537)  loss_mask: 0.3355 (0.5032)  loss_objectness: 0.0154 (0.0347)  loss_rpn_box_reg: 0.0170 (0.0233)  time: 2.0566  data: 0.0683  max mem: 7472\nEpoch: [0]  [260/482]  eta: 0:07:43  lr: 0.002715  loss: 0.6450 (0.9821)  loss_classifier: 0.0816 (0.1786)  loss_box_reg: 0.1494 (0.2506)  loss_mask: 0.3354 (0.4959)  loss_objectness: 0.0153 (0.0341)  loss_rpn_box_reg: 0.0119 (0.0229)  time: 2.1059  data: 0.0468  max mem: 7472\nEpoch: [0]  [270/482]  eta: 0:07:23  lr: 0.002819  loss: 0.6738 (0.9781)  loss_classifier: 0.1087 (0.1784)  loss_box_reg: 0.1711 (0.2504)  loss_mask: 0.3463 (0.4899)  loss_objectness: 0.0163 (0.0361)  loss_rpn_box_reg: 0.0119 (0.0233)  time: 2.1858  data: 0.0666  max mem: 7472\nEpoch: [0]  [280/482]  eta: 0:07:01  lr: 0.002923  loss: 0.6262 (0.9664)  loss_classifier: 0.0892 (0.1756)  loss_box_reg: 0.1449 (0.2462)  loss_mask: 0.3501 (0.4849)  loss_objectness: 0.0280 (0.0364)  loss_rpn_box_reg: 0.0087 (0.0232)  time: 2.1137  data: 0.0605  max mem: 7472\nEpoch: [0]  [290/482]  eta: 0:06:40  lr: 0.003027  loss: 0.6054 (0.9550)  loss_classifier: 0.0817 (0.1727)  loss_box_reg: 0.0929 (0.2428)  loss_mask: 0.3019 (0.4783)  loss_objectness: 0.0440 (0.0382)  loss_rpn_box_reg: 0.0113 (0.0231)  time: 2.0563  data: 0.0434  max mem: 7472\nEpoch: [0]  [300/482]  eta: 0:06:20  lr: 0.003131  loss: 0.6487 (0.9488)  loss_classifier: 0.0922 (0.1709)  loss_box_reg: 0.1191 (0.2404)  loss_mask: 0.3320 (0.4754)  loss_objectness: 0.0427 (0.0389)  loss_rpn_box_reg: 0.0121 (0.0232)  time: 2.0836  data: 0.0515  max mem: 7472\nEpoch: [0]  [310/482]  eta: 0:05:59  lr: 0.003235  loss: 0.7082 (0.9449)  loss_classifier: 0.0969 (0.1694)  loss_box_reg: 0.1515 (0.2396)  loss_mask: 0.3683 (0.4714)  loss_objectness: 0.0424 (0.0398)  loss_rpn_box_reg: 0.0133 (0.0247)  time: 2.0858  data: 0.0690  max mem: 7472\nEpoch: [0]  [320/482]  eta: 0:05:38  lr: 0.003338  loss: 0.7512 (0.9427)  loss_classifier: 0.0994 (0.1687)  loss_box_reg: 0.1919 (0.2402)  loss_mask: 0.3849 (0.4693)  loss_objectness: 0.0353 (0.0396)  loss_rpn_box_reg: 0.0357 (0.0250)  time: 2.1452  data: 0.0762  max mem: 7472\nEpoch: [0]  [330/482]  eta: 0:05:18  lr: 0.003442  loss: 0.8277 (0.9388)  loss_classifier: 0.1592 (0.1682)  loss_box_reg: 0.2393 (0.2398)  loss_mask: 0.3809 (0.4659)  loss_objectness: 0.0306 (0.0397)  loss_rpn_box_reg: 0.0325 (0.0252)  time: 2.2168  data: 0.0701  max mem: 7472\nEpoch: [0]  [340/482]  eta: 0:04:57  lr: 0.003546  loss: 0.6757 (0.9307)  loss_classifier: 0.1065 (0.1662)  loss_box_reg: 0.1474 (0.2375)  loss_mask: 0.3438 (0.4626)  loss_objectness: 0.0306 (0.0396)  loss_rpn_box_reg: 0.0133 (0.0248)  time: 2.1240  data: 0.0569  max mem: 7472\nEpoch: [0]  [350/482]  eta: 0:04:35  lr: 0.003650  loss: 0.5709 (0.9201)  loss_classifier: 0.0669 (0.1634)  loss_box_reg: 0.1112 (0.2336)  loss_mask: 0.3369 (0.4597)  loss_objectness: 0.0229 (0.0388)  loss_rpn_box_reg: 0.0117 (0.0246)  time: 1.9926  data: 0.0469  max mem: 7472\nEpoch: [0]  [360/482]  eta: 0:04:14  lr: 0.003754  loss: 0.6046 (0.9188)  loss_classifier: 0.1003 (0.1641)  loss_box_reg: 0.1324 (0.2334)  loss_mask: 0.3303 (0.4571)  loss_objectness: 0.0204 (0.0392)  loss_rpn_box_reg: 0.0121 (0.0250)  time: 2.0180  data: 0.0626  max mem: 7472\nEpoch: [0]  [370/482]  eta: 0:03:53  lr: 0.003858  loss: 0.6006 (0.9078)  loss_classifier: 0.0758 (0.1615)  loss_box_reg: 0.0887 (0.2301)  loss_mask: 0.3290 (0.4529)  loss_objectness: 0.0181 (0.0386)  loss_rpn_box_reg: 0.0084 (0.0247)  time: 2.0244  data: 0.0655  max mem: 7472\nEpoch: [0]  [380/482]  eta: 0:03:32  lr: 0.003962  loss: 0.5364 (0.9007)  loss_classifier: 0.0682 (0.1604)  loss_box_reg: 0.0931 (0.2289)  loss_mask: 0.2617 (0.4485)  loss_objectness: 0.0139 (0.0383)  loss_rpn_box_reg: 0.0050 (0.0247)  time: 2.0151  data: 0.0667  max mem: 7472\nEpoch: [0]  [390/482]  eta: 0:03:11  lr: 0.004065  loss: 0.7302 (0.9014)  loss_classifier: 0.1498 (0.1610)  loss_box_reg: 0.2004 (0.2305)  loss_mask: 0.3288 (0.4462)  loss_objectness: 0.0307 (0.0386)  loss_rpn_box_reg: 0.0100 (0.0250)  time: 2.1033  data: 0.0753  max mem: 7472\nEpoch: [0]  [400/482]  eta: 0:02:51  lr: 0.004169  loss: 0.7474 (0.8952)  loss_classifier: 0.1134 (0.1600)  loss_box_reg: 0.2025 (0.2292)  loss_mask: 0.3490 (0.4431)  loss_objectness: 0.0315 (0.0381)  loss_rpn_box_reg: 0.0130 (0.0248)  time: 2.1243  data: 0.0560  max mem: 7472\nEpoch: [0]  [410/482]  eta: 0:02:30  lr: 0.004273  loss: 0.5860 (0.8895)  loss_classifier: 0.0841 (0.1584)  loss_box_reg: 0.1454 (0.2277)  loss_mask: 0.3100 (0.4401)  loss_objectness: 0.0135 (0.0377)  loss_rpn_box_reg: 0.0110 (0.0257)  time: 2.1124  data: 0.0537  max mem: 7472\nEpoch: [0]  [420/482]  eta: 0:02:09  lr: 0.004377  loss: 0.6747 (0.8857)  loss_classifier: 0.0979 (0.1575)  loss_box_reg: 0.1829 (0.2272)  loss_mask: 0.3432 (0.4379)  loss_objectness: 0.0167 (0.0374)  loss_rpn_box_reg: 0.0182 (0.0256)  time: 2.0534  data: 0.0636  max mem: 7472\nEpoch: [0]  [430/482]  eta: 0:01:48  lr: 0.004481  loss: 0.6763 (0.8817)  loss_classifier: 0.1123 (0.1566)  loss_box_reg: 0.2011 (0.2271)  loss_mask: 0.3441 (0.4354)  loss_objectness: 0.0201 (0.0370)  loss_rpn_box_reg: 0.0182 (0.0256)  time: 1.9787  data: 0.0590  max mem: 7472\nEpoch: [0]  [440/482]  eta: 0:01:27  lr: 0.004585  loss: 0.5922 (0.8753)  loss_classifier: 0.0770 (0.1549)  loss_box_reg: 0.1138 (0.2247)  loss_mask: 0.3409 (0.4336)  loss_objectness: 0.0169 (0.0367)  loss_rpn_box_reg: 0.0143 (0.0255)  time: 2.0144  data: 0.0529  max mem: 7472\nEpoch: [0]  [450/482]  eta: 0:01:06  lr: 0.004688  loss: 0.5922 (0.8718)  loss_classifier: 0.0838 (0.1539)  loss_box_reg: 0.1332 (0.2244)  loss_mask: 0.3409 (0.4317)  loss_objectness: 0.0113 (0.0363)  loss_rpn_box_reg: 0.0107 (0.0254)  time: 2.0415  data: 0.0510  max mem: 7472\nEpoch: [0]  [460/482]  eta: 0:00:45  lr: 0.004792  loss: 0.7216 (0.8688)  loss_classifier: 0.1217 (0.1534)  loss_box_reg: 0.2018 (0.2241)  loss_mask: 0.3192 (0.4296)  loss_objectness: 0.0107 (0.0362)  loss_rpn_box_reg: 0.0123 (0.0254)  time: 2.0111  data: 0.0544  max mem: 7472\nEpoch: [0]  [470/482]  eta: 0:00:24  lr: 0.004896  loss: 0.6759 (0.8650)  loss_classifier: 0.1234 (0.1532)  loss_box_reg: 0.1583 (0.2228)  loss_mask: 0.3192 (0.4277)  loss_objectness: 0.0207 (0.0359)  loss_rpn_box_reg: 0.0134 (0.0254)  time: 1.9929  data: 0.0480  max mem: 7472\nEpoch: [0]  [480/482]  eta: 0:00:04  lr: 0.005000  loss: 0.6775 (0.8623)  loss_classifier: 0.1407 (0.1530)  loss_box_reg: 0.1559 (0.2217)  loss_mask: 0.3240 (0.4261)  loss_objectness: 0.0219 (0.0360)  loss_rpn_box_reg: 0.0148 (0.0254)  time: 1.9480  data: 0.0479  max mem: 7472\nEpoch: [0]  [481/482]  eta: 0:00:02  lr: 0.005000  loss: 0.6555 (0.8611)  loss_classifier: 0.1407 (0.1528)  loss_box_reg: 0.1366 (0.2213)  loss_mask: 0.3131 (0.4257)  loss_objectness: 0.0219 (0.0359)  loss_rpn_box_reg: 0.0134 (0.0254)  time: 1.9022  data: 0.0456  max mem: 7472\nEpoch: [0] Total time: 0:16:38 (2.0720 s / it)\ncreating index...\nindex created!\nTest:  [  0/321]  eta: 0:03:07  model_time: 0.5358 (0.5358)  evaluator_time: 0.0336 (0.0336)  time: 0.5840  data: 0.0141  max mem: 7472\nTest:  [100/321]  eta: 0:01:56  model_time: 0.4708 (0.4680)  evaluator_time: 0.0227 (0.0377)  time: 0.5279  data: 0.0188  max mem: 7472\nTest:  [200/321]  eta: 0:01:04  model_time: 0.4536 (0.4670)  evaluator_time: 0.0357 (0.0438)  time: 0.5308  data: 0.0233  max mem: 7472\nTest:  [300/321]  eta: 0:00:11  model_time: 0.4652 (0.4660)  evaluator_time: 0.0366 (0.0433)  time: 0.5570  data: 0.0269  max mem: 7472\nTest:  [320/321]  eta: 0:00:00  model_time: 0.4608 (0.4663)  evaluator_time: 0.0370 (0.0441)  time: 0.5569  data: 0.0280  max mem: 7472\nTest: Total time: 0:02:51 (0.5352 s / it)\nAveraged stats: model_time: 0.4608 (0.4663)  evaluator_time: 0.0370 (0.0441)\nAccumulating evaluation results...\nDONE (t=0.13s).\nAccumulating evaluation results...\nDONE (t=0.14s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.338\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.665\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.296\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.208\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.402\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.488\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.140\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.405\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.374\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.614\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.295\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.630\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.231\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.158\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.466\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.126\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.356\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.304\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.487\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553\n</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nplt.title('Training Loss') \nlosses = [np.mean(list(trn_history[i].meters['loss'].deque)) for i in range(len(trn_history))]\nplt.plot(losses)\n</pre> import matplotlib.pyplot as plt plt.title('Training Loss')  losses = [np.mean(list(trn_history[i].meters['loss'].deque)) for i in range(len(trn_history))] plt.plot(losses) In\u00a0[\u00a0]: Copied! <pre>model.eval()\nim = dataset_test[10][0]\nshow(im)\nwith torch.no_grad():\n    prediction = model([im.to(device)])\n    for i in range(len(prediction[0]['masks'])):\n        plt.imshow(Image.fromarray(prediction[0]['masks'][i, 0].mul(255).byte().cpu().numpy()))\n        plt.title('Class: '+str(prediction[0]['labels'][i].cpu().numpy())+' Score:'+str(prediction[0]['scores'][i].cpu().numpy()))\n        plt.show()\n</pre> model.eval() im = dataset_test[10][0] show(im) with torch.no_grad():     prediction = model([im.to(device)])     for i in range(len(prediction[0]['masks'])):         plt.imshow(Image.fromarray(prediction[0]['masks'][i, 0].mul(255).byte().cpu().numpy()))         plt.title('Class: '+str(prediction[0]['labels'][i].cpu().numpy())+' Score:'+str(prediction[0]['scores'][i].cpu().numpy()))         plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"note/Computer_Vision/Misc/","title":"Miscellaneous","text":""},{"location":"note/Computer_Vision/Misc/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/Misc/#multi-regression","title":"Multi-regression","text":"<ul> <li>Prediction of multiple values given an image as input</li> <li>e.g. facial keypoint detection</li> </ul>"},{"location":"note/Computer_Vision/Misc/#multi-task-learning","title":"Multi-task learning","text":"<ul> <li>Prediction of multiple items in a single shot</li> <li>e.g. age estimation and gender classification</li> </ul>"},{"location":"note/Computer_Vision/Misc/#self-training","title":"Self-training","text":"<ul> <li>A semi-supervised technique where a model is intially trained on a labeled dataset</li> <li>Then it uses its own predictions on unlabeled data to iteratively improve itself</li> <li>After predicting labels for the unlabeled data, it takes the most confident predictions and retrains itself with this pseudo-labeled data</li> </ul>"},{"location":"note/Computer_Vision/Misc/#self-supervised-learning","title":"Self-supervised learning","text":"<ul> <li>A type of unsupervised learning</li> <li>The model generates labels from the data itself to create supervision signals</li> </ul>"},{"location":"note/Computer_Vision/Misc/#siamese-network","title":"Siamese network","text":"<ul> <li>Takes in two separate inputs, processes each of the inputs with the same set of weights, and then compares the resulting outputs to determine how similar or dissimilar the two inputs are</li> <li>Loss functions: contrastive loss, triplet loss</li> </ul>"},{"location":"note/Computer_Vision/Misc/#teacher-forcing","title":"Teacher Forcing","text":"<ul> <li>Primarily used in sequence prediction models</li> <li>When generating sequences, the model predics each element based on the previous elements</li> <li>In teacher forcing however, rather than using the model's own predictions as inputs for the next step, the true values from the training data are fed as inputs for each subsequent step</li> </ul>"},{"location":"note/Computer_Vision/Misc/#meta-learning","title":"Meta-Learning","text":""},{"location":"note/Computer_Vision/Misc/#zero-shot-computer-vision","title":"Zero-shot Computer Vision","text":""},{"location":"note/Computer_Vision/Misc/#ethics-biases","title":"Ethics &amp; Biases","text":""},{"location":"note/Computer_Vision/Misc/#neural-tangent-kernel-ntk","title":"Neural Tangent Kernel (NTK)","text":""},{"location":"note/Computer_Vision/Misc/#hessian-matrix","title":"Hessian Matrix","text":""},{"location":"note/Computer_Vision/Misc/#models","title":"Models","text":""},{"location":"note/Computer_Vision/Misc/#mixup","title":"Mixup","text":""},{"location":"note/Computer_Vision/Misc/#randaugment","title":"RandAugment","text":""},{"location":"note/Computer_Vision/Misc/#i-jepa","title":"I-JEPA","text":"<p>Self-Supervised Learning from Images with a  Joint-Embedding Predictive Architecture</p>"},{"location":"note/Computer_Vision/Misc/facial_keypoint_detection/","title":"Facial keypoint detection","text":"In\u00a0[2]: Copied! <pre>import os\nimport glob\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms, models, datasets\nfrom torchsummary import summary\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom sklearn import cluster\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n</pre> import os import glob from copy import deepcopy import numpy as np import pandas as pd import cv2 import torch import torch.nn as nn import torch.nn.functional as F import torchvision from torchvision import transforms, models, datasets from torchsummary import summary from torch.utils.data import TensorDataset, DataLoader, Dataset from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from sklearn import cluster from sklearn.model_selection import train_test_split from tqdm import tqdm  device = 'cuda' if torch.cuda.is_available() else 'cpu' In\u00a0[3]: Copied! <pre>!git clone https://github.com/udacity/P1_Facial_Keypoints.git\n!cd P1_Facial_Keypoints\nroot_dir = 'P1_Facial_Keypoints/data/training/'\nall_img_paths = glob.glob(os.path.join(root_dir, '*.jpg'))\ndata = pd.read_csv('P1_Facial_Keypoints/data/training_frames_keypoints.csv')\n</pre> !git clone https://github.com/udacity/P1_Facial_Keypoints.git !cd P1_Facial_Keypoints root_dir = 'P1_Facial_Keypoints/data/training/' all_img_paths = glob.glob(os.path.join(root_dir, '*.jpg')) data = pd.read_csv('P1_Facial_Keypoints/data/training_frames_keypoints.csv') <pre>Cloning into 'P1_Facial_Keypoints'...\nremote: Enumerating objects: 6049, done.\nremote: Counting objects: 100% (11/11), done.\nremote: Compressing objects: 100% (9/9), done.\nremote: Total 6049 (delta 3), reused 7 (delta 2), pack-reused 6038 (from 1)\nReceiving objects: 100% (6049/6049), 329.53 MiB | 15.07 MiB/s, done.\nResolving deltas: 100% (156/156), done.\nUpdating files: 100% (5805/5805), done.\n</pre> In\u00a0[4]: Copied! <pre># 68 keypoints of the face\n# even columns: x-axis, odd columns: y-axis\ndata\n</pre> # 68 keypoints of the face # even columns: x-axis, odd columns: y-axis data Out[4]: Unnamed: 0 0 1 2 3 4 5 6 7 8 ... 126 127 128 129 130 131 132 133 134 135 0 Luis_Fonsi_21.jpg 45.0 98.0 47.0 106.0 49.0 110.0 53.0 119.0 56.0 ... 83.0 119.0 90.0 117.0 83.0 119.0 81.0 122.0 77.0 122.0 1 Lincoln_Chafee_52.jpg 41.0 83.0 43.0 91.0 45.0 100.0 47.0 108.0 51.0 ... 85.0 122.0 94.0 120.0 85.0 122.0 83.0 122.0 79.0 122.0 2 Valerie_Harper_30.jpg 56.0 69.0 56.0 77.0 56.0 86.0 56.0 94.0 58.0 ... 79.0 105.0 86.0 108.0 77.0 105.0 75.0 105.0 73.0 105.0 3 Angelo_Reyes_22.jpg 61.0 80.0 58.0 95.0 58.0 108.0 58.0 120.0 58.0 ... 98.0 136.0 107.0 139.0 95.0 139.0 91.0 139.0 85.0 136.0 4 Kristen_Breitweiser_11.jpg 58.0 94.0 58.0 104.0 60.0 113.0 62.0 121.0 67.0 ... 92.0 117.0 103.0 118.0 92.0 120.0 88.0 122.0 84.0 122.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3457 Thomas_Ferguson_02.jpg 35.0 94.0 35.0 105.0 38.0 112.0 40.0 123.0 46.0 ... 71.0 123.0 79.0 121.0 71.0 123.0 66.0 126.0 64.0 126.0 3458 Valerie_Harper_00.jpg 72.0 133.0 76.0 147.0 80.0 159.0 87.0 170.0 91.0 ... 120.0 156.0 135.0 159.0 124.0 163.0 117.0 167.0 113.0 166.0 3459 Maggie_Smith_00.jpg 52.0 149.0 56.0 160.0 63.0 168.0 71.0 179.0 78.0 ... 122.0 169.0 133.0 169.0 122.0 172.0 115.0 172.0 111.0 172.0 3460 Erin_Hershey_Presley_41.jpg 82.0 91.0 82.0 105.0 82.0 116.0 86.0 128.0 90.0 ... 119.0 132.0 134.0 136.0 119.0 139.0 112.0 139.0 108.0 139.0 3461 Rocco_Buttiglione_42.jpg 93.0 134.0 93.0 147.0 95.0 157.0 100.0 167.0 105.0 ... 167.0 174.0 167.0 168.0 167.0 174.0 164.0 173.0 159.0 173.0 <p>3462 rows \u00d7 137 columns</p> In\u00a0[5]: Copied! <pre>class FacesData(Dataset):\n    def __init__(self, df):\n      super(FacesData).__init__()\n      self.df = df\n      self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                            std=[0.229, 0.224, 0.225])\n\n    def __len__(self):\n      return len(self.df)\n\n    def __getitem__(self, idx):\n      img_path = \"P1_Facial_Keypoints/data/training/\" + self.df.iloc[idx, 0]\n      img = cv2.imread(img_path) / 255.\n      kp = deepcopy(self.df.iloc[idx, 1:].tolist())\n      kp_x = (np.array(kp[0::2]) / img.shape[1]).tolist()\n      kp_y = (np.array(kp[1::2]) / img.shape[0]).tolist()\n      kp2 = kp_x + kp_y\n      kp2 = torch.tensor(kp2)\n      img = self.preprocess_input(img)\n      return img, kp2\n\n    def preprocess_input(self, img):\n      img = cv2.resize(img, (224, 224))\n      img = torch.tensor(img).permute(2, 0, 1)\n      img = self.normalize(img).float()\n      return img.to(device)\n\n    def load_img(self, idx):\n      img_path = \"P1_Facial_Keypoints/data/training/\" + self.df.iloc[idx, 0]\n      img = cv2.imread(img_path)\n      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.\n      img = cv2.resize(img, (224, 224))\n      return img\n</pre> class FacesData(Dataset):     def __init__(self, df):       super(FacesData).__init__()       self.df = df       self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                             std=[0.229, 0.224, 0.225])      def __len__(self):       return len(self.df)      def __getitem__(self, idx):       img_path = \"P1_Facial_Keypoints/data/training/\" + self.df.iloc[idx, 0]       img = cv2.imread(img_path) / 255.       kp = deepcopy(self.df.iloc[idx, 1:].tolist())       kp_x = (np.array(kp[0::2]) / img.shape[1]).tolist()       kp_y = (np.array(kp[1::2]) / img.shape[0]).tolist()       kp2 = kp_x + kp_y       kp2 = torch.tensor(kp2)       img = self.preprocess_input(img)       return img, kp2      def preprocess_input(self, img):       img = cv2.resize(img, (224, 224))       img = torch.tensor(img).permute(2, 0, 1)       img = self.normalize(img).float()       return img.to(device)      def load_img(self, idx):       img_path = \"P1_Facial_Keypoints/data/training/\" + self.df.iloc[idx, 0]       img = cv2.imread(img_path)       img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.       img = cv2.resize(img, (224, 224))       return img In\u00a0[6]: Copied! <pre>train, test = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_dataset = FacesData(train.reset_index(drop=True))\ntest_dataset = FacesData(test.reset_index(drop=True))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n</pre> train, test = train_test_split(data, test_size=0.2, random_state=42)  train_dataset = FacesData(train.reset_index(drop=True)) test_dataset = FacesData(test.reset_index(drop=True))  train_loader = DataLoader(train_dataset, batch_size=32) test_loader = DataLoader(test_dataset, batch_size=32) In\u00a0[17]: Copied! <pre>def get_model():\n  model = models.vgg16(pretrained=True)\n\n  for param in model.parameters():\n    param.requires_grad = False\n\n  model.avgpool = nn.Sequential(\n      nn.Conv2d(512, 512, 3),\n      nn.MaxPool2d(2),\n      nn.Flatten()\n  )\n\n  model.classifier = nn.Sequential(\n      nn.Linear(2048, 512),\n      nn.ReLU(),\n      nn.Dropout(0.5),\n      nn.Linear(512, 136),\n      nn.Sigmoid()\n  )\n\n  criterion = nn.L1Loss()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n  return model.to(device), criterion, optimizer\n</pre> def get_model():   model = models.vgg16(pretrained=True)    for param in model.parameters():     param.requires_grad = False    model.avgpool = nn.Sequential(       nn.Conv2d(512, 512, 3),       nn.MaxPool2d(2),       nn.Flatten()   )    model.classifier = nn.Sequential(       nn.Linear(2048, 512),       nn.ReLU(),       nn.Dropout(0.5),       nn.Linear(512, 136),       nn.Sigmoid()   )    criterion = nn.L1Loss()    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)    return model.to(device), criterion, optimizer In\u00a0[18]: Copied! <pre>model, criterion, optimizer = get_model()\n</pre> model, criterion, optimizer = get_model() <pre>/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n</pre> In\u00a0[9]: Copied! <pre>model\n</pre> model Out[9]: <pre>VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): Flatten(start_dim=1, end_dim=-1)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=2048, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=512, out_features=136, bias=True)\n    (4): Sigmoid()\n  )\n)</pre> In\u00a0[10]: Copied! <pre>summary(model, input_size=(3, 224, 224))\n</pre> summary(model, input_size=(3, 224, 224)) <pre>----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n           Conv2d-15          [-1, 256, 56, 56]         590,080\n             ReLU-16          [-1, 256, 56, 56]               0\n        MaxPool2d-17          [-1, 256, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n             ReLU-19          [-1, 512, 28, 28]               0\n           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n             ReLU-21          [-1, 512, 28, 28]               0\n           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n             ReLU-23          [-1, 512, 28, 28]               0\n        MaxPool2d-24          [-1, 512, 14, 14]               0\n           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n             ReLU-26          [-1, 512, 14, 14]               0\n           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n             ReLU-28          [-1, 512, 14, 14]               0\n           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n             ReLU-30          [-1, 512, 14, 14]               0\n        MaxPool2d-31            [-1, 512, 7, 7]               0\n           Conv2d-32            [-1, 512, 5, 5]       2,359,808\n        MaxPool2d-33            [-1, 512, 2, 2]               0\n          Flatten-34                 [-1, 2048]               0\n           Linear-35                  [-1, 512]       1,049,088\n             ReLU-36                  [-1, 512]               0\n          Dropout-37                  [-1, 512]               0\n           Linear-38                  [-1, 136]          69,768\n          Sigmoid-39                  [-1, 136]               0\n================================================================\nTotal params: 18,193,352\nTrainable params: 3,478,664\nNon-trainable params: 14,714,688\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 218.54\nParams size (MB): 69.40\nEstimated Total Size (MB): 288.51\n----------------------------------------------------------------\n</pre> In\u00a0[11]: Copied! <pre>def train_batch(img, kps, model, optimizer, criterion):\n  model.train()\n  optimizer.zero_grad()\n  _kps = model(img.to(device))\n  loss = criterion(_kps, kps.to(device))\n  loss.backward()\n  optimizer.step()\n  return loss\n</pre> def train_batch(img, kps, model, optimizer, criterion):   model.train()   optimizer.zero_grad()   _kps = model(img.to(device))   loss = criterion(_kps, kps.to(device))   loss.backward()   optimizer.step()   return loss In\u00a0[12]: Copied! <pre>@torch.no_grad()\ndef validate_batch(img, kps, model, criterion):\n  model.eval()\n  _kps = model(img.to(device))\n  loss = criterion(_kps, kps.to(device))\n  return _kps, loss\n</pre> @torch.no_grad() def validate_batch(img, kps, model, criterion):   model.eval()   _kps = model(img.to(device))   loss = criterion(_kps, kps.to(device))   return _kps, loss In\u00a0[19]: Copied! <pre>train_loss, test_loss = [], []\nn_epochs = 50\n\nfor epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n  epoch_train_loss, epoch_test_loss = 0, 0\n  for idx, (img, kps) in enumerate(train_loader):\n    loss = train_batch(img, kps, model, optimizer, criterion)\n    epoch_train_loss += loss.item()\n  epoch_train_loss /= (idx + 1)\n\n  for idx, (img, kps) in enumerate(test_loader):\n    ps, loss = validate_batch(img, kps, model, criterion)\n    epoch_test_loss += loss.item()\n  epoch_test_loss /= (idx + 1)\n\n  train_loss.append(epoch_train_loss)\n  test_loss.append(epoch_test_loss)\n</pre> train_loss, test_loss = [], [] n_epochs = 50  for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):   epoch_train_loss, epoch_test_loss = 0, 0   for idx, (img, kps) in enumerate(train_loader):     loss = train_batch(img, kps, model, optimizer, criterion)     epoch_train_loss += loss.item()   epoch_train_loss /= (idx + 1)    for idx, (img, kps) in enumerate(test_loader):     ps, loss = validate_batch(img, kps, model, criterion)     epoch_test_loss += loss.item()   epoch_test_loss /= (idx + 1)    train_loss.append(epoch_train_loss)   test_loss.append(epoch_test_loss) <pre>Epochs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [27:43&lt;00:00, 33.27s/it]\n</pre> In\u00a0[20]: Copied! <pre>epochs = np.arange(50)+1\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n%matplotlib inline\nplt.plot(epochs, train_loss, 'bo', label='Training loss')\nplt.plot(epochs, test_loss, 'r', label='Test loss')\nplt.title('Training and Test loss over increasing epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid('off')\nplt.show()\n</pre> epochs = np.arange(50)+1 import matplotlib.ticker as mtick import matplotlib.pyplot as plt import matplotlib.ticker as mticker %matplotlib inline plt.plot(epochs, train_loss, 'bo', label='Training loss') plt.plot(epochs, test_loss, 'r', label='Test loss') plt.title('Training and Test loss over increasing epochs') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.grid('off') plt.show() In\u00a0[21]: Copied! <pre>idx = 0\nplt.figure(figsize=(10,10))\nplt.subplot(221)\nplt.title('Original image')\nim = test_dataset.load_img(idx)\nplt.imshow(im)\nplt.grid(False)\nplt.subplot(222)\nplt.title('Image with facial keypoints')\nx, _ = test_dataset[idx]\nplt.imshow(im)\nkp = model(x[None]).flatten().detach().cpu()\nplt.scatter(kp[:68]*224, kp[68:]*224, c='r')\nplt.grid(False)\nplt.show()\n</pre> idx = 0 plt.figure(figsize=(10,10)) plt.subplot(221) plt.title('Original image') im = test_dataset.load_img(idx) plt.imshow(im) plt.grid(False) plt.subplot(222) plt.title('Image with facial keypoints') x, _ = test_dataset[idx] plt.imshow(im) kp = model(x[None]).flatten().detach().cpu() plt.scatter(kp[:68]*224, kp[68:]*224, c='r') plt.grid(False) plt.show()"},{"location":"note/Computer_Vision/Object_Detection/","title":"Object Detection","text":""},{"location":"note/Computer_Vision/Object_Detection/#workflow-few-shot","title":"Workflow (Few-shot)","text":"<ol> <li>Creating ground-truth data that contains labels of the bounding box and class corresponding to various objects present in the image</li> <li>Coming up with mechanisms that scan through the image to identify regions (region proposals) that are likely to contain objects</li> <li>Creating the target class variable by using the IoU metric</li> <li>Creating the target bounding-box offset variable to make corrections to the location of the region proposal in step 2</li> <li>Building a model that can predict the class of object along with the target bounding-box offset corresponding to the region proposal</li> <li>Measuring the accuracy of object detection using mean average precision (mAP)</li> </ol>"},{"location":"note/Computer_Vision/Object_Detection/#models","title":"Models","text":""},{"location":"note/Computer_Vision/Object_Detection/#r-cnn-regions-with-cnn-features","title":"R-CNN: Regions with CNN features","text":"<p>Rich feature hierarchies for accurate object detection and semantic segmentation </p> <p>Workflow: 1. Extract region proposals with selective search from an image. We need to ensure that we extract a high number of proposals to not miss out on any potential object within the image. 2. Resize (warp) all the extracted regions to get regions of the same size. 3. Pass the resized region proposals through a network. Typically, we pass the resized region proposals through a pretrained model, such as VGG16 or ResNet50, and extract the features in a fully connected layer. 4. Create data for model training, where the input is features extracted by passing the region proposals through a pretrained model. The outputs are the class corresponding to each region proposal and the offset of the region proposal from the ground truth corresponding to the image.   - If a region proposal has an IoU greater than a specific threshold with the object, we create training data.   - We calculate the offset between the region proposal bounding box and the ground-truth bounding box as the differencebetween the center coordinates of the two bounding boxes <code>(dx, dy)</code>, and the difference between the height and width of the bounding boxes <code>(dw, dh)</code> 5. Connect two output heads, one corresponding to the class of image and the other corresponding to the offset of region proposal with the ground-truth bounding box, to extract the fine bounding box on the object 6. Train the model after writing a custom loss function that minimizes both the object classification error and the bounding-box offset error</p> <p>Caveats: - The feature vector generated by the CNN, is consumed by a binary SVM trained for each class independently</p>"},{"location":"note/Computer_Vision/Object_Detection/#fast-r-cnn","title":"Fast R-CNN","text":"<p>Fast R-CNN </p> <p>Advantages of Fast-RCNN over RCNN - Fast-RCNN passes the entire image through the pretrained model to extract the features, and then fetches the region of features that corresponds to the region proposals (obtained from selective search) of the original image   - R-CNN generates region proposals for each image, resizing the crops of regions, and extracting features corresponding to each crop (region proposal) create a bottleneck</p> <p>Workflow: 1. Pass the image through a pretrained model to extract features prior to the flatenning layer 2. Extract region proposals corresponding to the image 3. Extract the feature map area corresponding to the region proposals 4. Pass the feature maps corresponding to region proposals through the region of interest (RoI) pooling layer one at a time   - thus all feature maps of region proposals have a similar shape 5. Pass the RoI pooling output value through a fully connected layer 6. Train the model to predict class and offsets corresponding to each region proposal</p>"},{"location":"note/Computer_Vision/Object_Detection/#faster-r-cnn","title":"Faster R-CNN","text":"<p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks </p> <p>Advantages of Faster R-CNN over Fast R-CNN 1. Replace selective search with RPN: reduce region proposals without loss of accuracy   - Fast-RCNN: selective search usually generates a lot of region proposals 2. End-to-end training: RPN is jointly trained with the rest of the model   - RPN learns to generate high-quality region proposals   - the objective function of the faster R-CNN includes not only the class and bounding box prediction in object detection, but also the binary class and bounding box prediction of anchor boxes in the region proposal network</p> <p>RPN Workflow: 1. Use a \\(3 \\times 3\\) convolutional layer with padding of 1 to transform the CNN output to a new output with \\(c\\) channels    - Each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length \\(c\\) 2. Centered on each pixel of the feature maps, generate multiple anchor boxes of different scaled and aspect ratios and label them 3. Using the length-\\(c\\) feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box 4. Consider those predicted bounding boxes whose predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer</p>"},{"location":"note/Computer_Vision/Object_Detection/#yolo","title":"YOLO","text":"<p>You Only Look Once: Unified, Real-Time Object Detection</p> <p>Advantages of YOLO over Faster-RCNN 1. One-stage vs Two-stage Detectors   - Faster R-CNN: RPN (proposed regions are sparse as the potential bounding box candidates can be infinite) + Classifier (predicts class and bounding box)   - YOLO: runs detection directly over a dense sampling of possible locations 2. YOLO looks at the whole image while predicting the bounding box corresponding to an image   - Faster R-CNN: in the fully connected layer, only the detected region's RoI pooling output is passed as input   - in case of regions that do not fully encompass the object, the network has to guess the real boundaries of the object, as it has not seen the full image (but has seen only the region proposals)_</p>"},{"location":"note/Computer_Vision/Object_Detection/#ssd","title":"SSD","text":"<p>https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/ - TO-DO</p>"},{"location":"note/Computer_Vision/Object_Detection/#detr","title":"DETR","text":""},{"location":"note/Computer_Vision/Object_Detection/#concepts","title":"Concepts","text":""},{"location":"note/Computer_Vision/Object_Detection/#felzenszwalbs-algorithm","title":"Felzenszwalb's algorithm","text":"<p>Efficient Graph-Based Image Segmentation - segmentation based on the color, texture, size, and shape compatibility of content within an image</p>"},{"location":"note/Computer_Vision/Object_Detection/#selective-search","title":"Selective Search","text":"<p>Selective Search for Object Recognition - region proposal algorithm - generate proposals of regions that are likely to be grouped together based on their pixel intensities - group pixels based on the hierarchical grouping of similar pixels (leverages the color, texture, size, and shape compatibility of content within an image) - Workflow:   1. over-segments an image by grouping pixels based on the preceding attributes   2. iterates through these over-segmented groups and groups them based on similarity     - at each iteration, it combines smaller regions to form a larger region</p>"},{"location":"note/Computer_Vision/Object_Detection/#non-max-suppression","title":"Non-max suppression","text":"<ul> <li>non-max: boxes that don't have the highest probability of containing an object</li> <li>suppresion: discarding those boxes</li> <li>Workflow:<ol> <li>identify the bounding box that has the highest probability of containing the object</li> <li>discard all the other bounding boxes that have an IoU below a certain threshold with the box (highest probability containing an object)</li> </ol> </li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/#mean-average-precision","title":"Mean average precision","text":"<ul> <li>Precision: \\(Precision = \\frac{TP}{TP + FP}\\)<ul> <li>True positive: bounding boxes that predicted the correct class of object and have an IoU with a ground truth that is greater than a certain threshold</li> <li>False positive: bounding boxes that predicted the class incorrectly or have an overlap that is less than the defined threshold with the ground truth</li> <li>If there are multiple bounding boxes that are identified for the same ground-truth bounding box, only one box can be a true positive, and everything else is a false positive</li> </ul> </li> <li>Average precision: average of precision values calculated at various IoU thresholds</li> <li>mAP: average of precision values calculated at various IoU threshold values across all the classes of objects</li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/#roi-pooling","title":"RoI Pooling","text":"<ul> <li>ensures fixed-size output for varying RoI dimensions</li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/#anchor-boxes","title":"Anchor boxes","text":"<ul> <li>handy replacement for selective search<ul> <li>majority of objects have a similar shape, e.g. the bounding box for a person will have a greater height than width</li> <li>objects of interest might be scaled, but maintains the aspect ratio (height/width)</li> </ul> </li> <li>define the anchor boxes with heights and width representing the majority of object's bounding boxes within the dataset<ul> <li>obtained by using K-means clustering on top of the ground-truth bounding boxes</li> </ul> </li> <li>also create anchor boxes with varying scales (same center but different aspect ratios)</li> <li>Workflow:<ol> <li>slide each anchor box over an image from top left to bottom right</li> <li>the anchor box with a high IoU with the object will have a label that mentions it contains an objects, otherwise will be labelled as <code>0</code></li> </ol> </li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/#region-proposal-network-rpn","title":"Region proposal network (RPN)","text":"<ul> <li>RPN vs selective search:<ul> <li>selective search gives us a region candidate based on a set of computations on top of pixel values (done outside the neural network)</li> <li>RPN generates region candidates based on anchor boxes and the strides with which anchor boxes are slid over the image</li> </ul> </li> <li>Workflow:<ol> <li>Slide anchor boxes of different aspect ratios and sizes across the image to fetch crops of an image</li> <li>Calculate the IoU between the ground-truth bounding boxes of objects in the image and the crops obtained in the previous step</li> <li>Preparing the training dataset in such a way that crops with an IoU greater than a threshold contain an object, and crops with an IoU less than a threshold do not contain an object</li> <li>Train the model to identify regions that contain an object</li> <li>Perform NMS to identify the region candidate that has the highest probability of contaning an object and eliminate other region candidates that have a high overlap with it</li> </ol> </li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/#references","title":"References","text":"<ul> <li>https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/</li> <li>https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/</li> <li>https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/</li> <li>https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/</li> </ul>"},{"location":"note/Computer_Vision/Object_Detection/selective_search/","title":"Selective search","text":"In\u00a0[5]: Copied! <pre>!pip -q install selectivesearch torch_snippets\n</pre> !pip -q install selectivesearch torch_snippets <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/78.6 kB ? eta -:--:--\r     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.6/78.6 kB 3.3 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 103.0/103.0 kB 7.3 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 82.7/82.7 kB 5.7 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 119.4/119.4 kB 7.9 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.5/62.5 kB 4.0 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 218.7/218.7 kB 12.8 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 162.6/162.6 kB 9.7 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 99.0/99.0 kB 5.5 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.0/6.0 MB 31.0 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 469.0/469.0 kB 26.6 MB/s eta 0:00:00\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1/3.1 MB 56.4 MB/s eta 0:00:00\n  Building wheel for typing (setup.py) ... done\n</pre> In\u00a0[6]: Copied! <pre>import selectivesearch\nfrom skimage.segmentation import felzenszwalb\nfrom torch_snippets import *\n</pre> import selectivesearch from skimage.segmentation import felzenszwalb from torch_snippets import * In\u00a0[7]: Copied! <pre>!wget -q https://www.dropbox.com/s/lpw10qawsc5ipbn/MyImage.JPG -O MyImage.jpg\nimg = read('MyImage.jpg', 1)\n</pre> !wget -q https://www.dropbox.com/s/lpw10qawsc5ipbn/MyImage.JPG -O MyImage.jpg img = read('MyImage.jpg', 1) In\u00a0[11]: Copied! <pre>felzenszwalb(img, scale=200).shape\n</pre> felzenszwalb(img, scale=200).shape Out[11]: <pre>((302, 298), (302, 298, 3))</pre> In\u00a0[15]: Copied! <pre>segments_fz = felzenszwalb(img, scale=200) # scale = no. of clusters\nsubplots([img, segments_fz], titles=['Original Image','Image post \\nfelzenszwalb segmentation'], figsize=(10,10), nc=2)\n</pre> segments_fz = felzenszwalb(img, scale=200) # scale = no. of clusters subplots([img, segments_fz], titles=['Original Image','Image post \\nfelzenszwalb segmentation'], figsize=(10,10), nc=2) In\u00a0[35]: Copied! <pre># fetch the candidate regions\ndef extract_candidates(img):\n  img_lbl, regions = selectivesearch.selective_search(img, scale=200, min_size=100)\n  img_area = np.prod(img.shape[:2])\n  candidates = []\n  for r in regions:\n    if r['rect'] in candidates: continue\n    # fetch only those that 5% &lt; img_area &lt;= 100% of the img area\n    if r['size'] &lt; (0.05 * img_area): continue\n    if r['size'] &gt; (1 * img_area): continue\n    x, y, w, h = r['rect']\n    candidates.append(list(r['rect']))\n  return candidates\n</pre> # fetch the candidate regions def extract_candidates(img):   img_lbl, regions = selectivesearch.selective_search(img, scale=200, min_size=100)   img_area = np.prod(img.shape[:2])   candidates = []   for r in regions:     if r['rect'] in candidates: continue     # fetch only those that 5% &lt; img_area &lt;= 100% of the img area     if r['size'] &lt; (0.05 * img_area): continue     if r['size'] &gt; (1 * img_area): continue     x, y, w, h = r['rect']     candidates.append(list(r['rect']))   return candidates In\u00a0[36]: Copied! <pre>candidates = extract_candidates(img)\nshow(img, bbs = candidates)\n</pre> candidates = extract_candidates(img) show(img, bbs = candidates) <pre>/usr/local/lib/python3.10/dist-packages/skimage/feature/texture.py:360: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n  warnings.warn(\n</pre>"},{"location":"note/Computer_Vision/Video/","title":"Video","text":""},{"location":"note/Data_Science/","title":"Data Science","text":"<ul> <li>Probability &amp; Statistics</li> <li>R, Scala</li> <li>Tableu, Power BI</li> <li>Econometrics, Time Series Analysis</li> </ul>"},{"location":"note/Data_Structure_%26_Algorithms/","title":"Data Structures and Algorithms","text":"<ul> <li>Introduction to the design and analysis of algorithms</li> </ul>"},{"location":"note/Graph_Neural_Networks/","title":"Graph Neural Networks","text":""},{"location":"note/Machine%20Learning/","title":"Machine Learning","text":"<ul> <li>approaching almost any ml problem</li> <li>ml with python, theory and implementation</li> </ul> <p>Concepts:</p> <ul> <li>Learning rate scheduler (cosine)</li> <li>warmup</li> <li>different optimizers</li> <li>Restricted Boltzmann machine</li> <li>Hidden Markov Models (HMM), Conditional Random Fields (CRF),</li> </ul>"},{"location":"note/Missing_Semester/","title":"Missing Semester","text":""},{"location":"note/Natural_Language_Processing/","title":"Natural Language Processing","text":"<ul> <li>Speech and Lanaguage Processing, 3rd ed. draft</li> <li>Hands-On Large Language Models</li> <li>Build a Large Language Model (From Scratch)</li> <li>Large Language Models: A Deep Dive</li> </ul>"},{"location":"note/Natural_Language_Processing/Agent/","title":"AI Agents","text":""},{"location":"note/Natural_Language_Processing/Architecture/","title":"Architecture","text":""},{"location":"note/Natural_Language_Processing/Architecture/#attention","title":"Attention","text":""},{"location":"note/Natural_Language_Processing/Architecture/#standard-attention","title":"Standard Attention","text":""},{"location":"note/Natural_Language_Processing/Architecture/#self-attention","title":"Self-Attention","text":"\\[\\textbf{A} = \\text{softmax}\\left(\\frac{\\textbf{Q}\\textbf{K}^\\text{T}}{\\sqrt{d_k}} \\right) \\textbf{V}\\]"},{"location":"note/Natural_Language_Processing/Architecture/#multi-head-attention-mha","title":"Multi-Head Attention (MHA)","text":"<ul> <li>Allows the model to focus on different parts of the input data simultaneously by employing multiple attention heads</li> </ul>"},{"location":"note/Natural_Language_Processing/Architecture/#multi-query-attention-mqa","title":"Multi-Query Attention (MQA)","text":"<ul> <li>Simplifies MHA by using a single shared query across all heads</li> <li>But allowing different key and value projections</li> <li>Reduces complexity in both space and time</li> </ul>"},{"location":"note/Natural_Language_Processing/Architecture/#grouped-query-attention-gqa","title":"Grouped-Query Attention (GQA)","text":"<ul> <li>A variant of MHA that reduces computational complexity by sharing query representations across multiple heads</li> <li>While allowing separate key and value representations</li> <li>The idea is to use fewer query groups but still preserve a level of diversity in the attention mechanism</li> </ul>"},{"location":"note/Natural_Language_Processing/Architecture/#multi-head-latent-attention-mla","title":"Multi-Head Latent Attention (MLA)","text":"<ul> <li>Achieves better results than MHA through low-rank key-value joint compression</li> <li>Requires much less Key-Value (KV) Cache</li> </ul>"},{"location":"note/Natural_Language_Processing/Architecture/#localsparse-attention","title":"Local/Sparse Attention","text":""},{"location":"note/Natural_Language_Processing/Architecture/#flash-attention","title":"Flash-Attention","text":""},{"location":"note/Natural_Language_Processing/Architecture/#feed-forward-neural-network-fnn","title":"Feed-Forward Neural Network (FNN)","text":""},{"location":"note/Natural_Language_Processing/Architecture/#standard-fnn","title":"Standard FNN","text":""},{"location":"note/Natural_Language_Processing/Architecture/#gated-fnn","title":"Gated FNN","text":""},{"location":"note/Natural_Language_Processing/Architecture/#activation-function","title":"Activation Function","text":""},{"location":"note/Natural_Language_Processing/Architecture/#rectified-linear-unit-relu","title":"Rectified Linear Unit (ReLU)","text":""},{"location":"note/Natural_Language_Processing/Architecture/#gaussian-error-linear-unit-gelu","title":"Gaussian Error Linear Unit (GELU)","text":""},{"location":"note/Natural_Language_Processing/Architecture/#gelu-tanh","title":"GELU-tanh","text":""},{"location":"note/Natural_Language_Processing/Architecture/#silu-sigmoid-linear-unit","title":"SiLU (Sigmoid Linear Unit)","text":""},{"location":"note/Natural_Language_Processing/Architecture/#normalization","title":"Normalization","text":""},{"location":"note/Natural_Language_Processing/Architecture/#batchnorm","title":"BatchNorm","text":""},{"location":"note/Natural_Language_Processing/Architecture/#layernorm","title":"LayerNorm","text":""},{"location":"note/Natural_Language_Processing/Architecture/#rmsnorm","title":"RMSNorm","text":""},{"location":"note/Natural_Language_Processing/Architecture/#positional-embeddings","title":"Positional Embeddings","text":""},{"location":"note/Natural_Language_Processing/Architecture/#sinusoidal-position","title":"Sinusoidal Position","text":""},{"location":"note/Natural_Language_Processing/Architecture/#rope","title":"RoPE","text":""},{"location":"note/Natural_Language_Processing/Architecture/#misc","title":"Misc.","text":""},{"location":"note/Natural_Language_Processing/Architecture/#parameter-sharing","title":"Parameter Sharing","text":""},{"location":"note/Natural_Language_Processing/Architecture/#layer-wise-parameter-scaling","title":"Layer-wise Parameter Scaling","text":""},{"location":"note/Natural_Language_Processing/Architecture/#nonlinearity-compensation","title":"Nonlinearity Compensation","text":""},{"location":"note/Natural_Language_Processing/Architecture/#counterparts","title":"Counterparts","text":""},{"location":"note/Natural_Language_Processing/Architecture/#rnn","title":"RNN","text":""},{"location":"note/Natural_Language_Processing/Architecture/#lstm","title":"LSTM","text":""},{"location":"note/Natural_Language_Processing/Architecture/#mamba","title":"Mamba","text":""},{"location":"note/Natural_Language_Processing/Architecture/#rwkv","title":"RWKV","text":""},{"location":"note/Natural_Language_Processing/Chatbot/","title":"Chatbots","text":""},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#frame","title":"Frame","text":"<ul> <li>Frames are knowledge structures representing the details of the user's task specification</li> <li>Each frame consists of a collection of slots, each of which can take a set of possible values</li> <li>Together a set of frames is sometimes called a domain ontology</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#task-oriented-dialogue-systems","title":"Task-oriented dialogue systems","text":"<ul> <li>Converse with users to accomplish fixed tasks</li> <li>Relying on a data structure called the frame</li> <li>e.g. Siri, Alexa</li> <li>The frame and its slots specify what the system needs to know to perform its task</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#frames-and-slot-filling","title":"Frames and Slot Filling","text":"<ul> <li>The system's goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user</li> <li>Together, the domain classification and intent determination tasks decide which frame we are filling</li> <li>The simplest dialogue systems use handwritten rules for slot-filling, like regular expression</li> <li>But most systems use supervised machine-learning: each sentence in a training set is annotated with slots, domain, and intent and a sequence model maps from input words to slot fillers, domain and intent</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#evaluation","title":"Evaluation","text":"<ul> <li>Task error rate, or task success rate</li> <li>Slot error rate, the percentage of slots filled with the correct values</li> <li>Instead of error rate, slot precision, recall and F-socre can also be used</li> <li>Efficiency costs like the length of the dialogue in seconds or turns</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#properties-of-human-conversation","title":"Properties of Human Conversation","text":""},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#turns","title":"Turns","text":"<ul> <li>A dialogue is a sequence of turns, each a single contribution from one speaker to the dialogue</li> <li>The system must know when to start and stop talking</li> <li>Endpointing / endpoint detection : spoken dialogue systems must detect whether a user is done speaking, so they can process the utterance and respond</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#speech-acts","title":"Speech Acts","text":"<ul> <li>Each utterance in a dialogue is a kind of action being performed by the speaker</li> <li>These actions are called speech acts or dialogue acts</li> <li>4 major classes: constatives, directives, commissives, and acknowledgments</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#grounding","title":"Grounding","text":"<ul> <li>Like all collective acts, it's important for the participants to establish what they both agree on, called the common ground</li> <li>Speakers do this by grounding each other's utterances</li> <li>Grounding means acknowledging that the hearer has understood the speaker</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#subdialogues-and-dialogue-structure","title":"Subdialogues and Dialogue Structure","text":"<ul> <li>Adjacency pairs are composed of a first pair part and a second pair part:<ul> <li>QUESTIONS set up an expectation for an ANSWER</li> <li>PROPOSALS are followed by ACCEPTANCE or REJECTION</li> <li>COMPLIMENTS often give rise to DOWNPLAYERS</li> </ul> </li> <li>Dialogue acts aren't always followed immediately by their second pair part, the two parts can be separated by a side sequence or subdialogue</li> <li>Subdialogues examples:<ul> <li>Correction subdialogue</li> <li>Clarification question, which can form a subdialogue between a REQUEST and a RESPONSE</li> </ul> </li> <li>Questions often have presequences</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#initiative","title":"Initiative","text":"<ul> <li>Sometimes a conversation is completely controlled by one participant</li> <li>Normal human-human dialogue is mixed initiative</li> <li>The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can't do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds</li> <li>Modern LLM-based dialogue systems, which come closer to using full mixed initiative, often don't have completely natural initiative switching</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#inference-and-implicature","title":"Inference and Implicature","text":"<ul> <li>Implicature means a particular class of licensed inferences</li> <li>Grice proposed that what enables hearers to draw these inferences is that conversation is guided by a set of maxims, general heuristics that play a guiding role in the interpretation of conversational utterances</li> <li>Grice's maxims: quantity, quality, relevance, manner</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#dialogue-acts","title":"Dialogue Acts","text":"<ul> <li>A generalization of speech acts that also represent grounding</li> <li>The set of acts can be general, or can be designed for particular dialogue tasks</li> <li>Dialogue act detection is done just like domain or intent classification, by passing the input sentence through an encoder and adding an act classifier</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#dialogue-state-tracker-dst","title":"Dialogue State Tracker (DST)","text":"<ul> <li>The job of the dialogue-state tracker is to determine the current state of the frame (the fillers of each slot), and the user's most recent dialogue</li> <li>The dialogue state is not just the slot-fillers in the current sentence; it includes the entire state of the frame at this point, summarizing all of the user's constraints</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#dialogue-policy","title":"Dialogue Policy","text":"<ul> <li>Which act to generate</li> <li>In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user</li> <li>A more sophisticated dialogue policy can help a system decide when to answer the user's question, when to instead ask the user a clarification question, and so on</li> <li>A dialogue policy thus decides what dialogue act to generate</li> <li>Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#natural-language-generation-nlg","title":"Natural Language Generation (NLG)","text":"<ul> <li>Once a dialogue act has been chosen, we need to generate the text of the response to the user</li> <li>The part of the generation process is called sentence realization</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#delexicalize","title":"Delexicalize","text":"<ul> <li>Generalize the training examples by replacing specific slot value words in the training set with a generic placeholder token representing the slot</li> <li>The decoder outputs the delexicalized English sentence, which can then relexicalize</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#chatbots","title":"Chatbots","text":"<ul> <li>Systems that can carry on extended conversations with the goal of mimicking the unstructure conversations or 'chats' characteristic of informal human-human interaction</li> <li>e.g. ELIZA, ChatGPT</li> <li>For training chatbots, it's most common to use the standard causal (decoder-only) language model</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#evaluating-chatbots","title":"Evaluating Chatbots","text":"<p>Participant Evaluation:</p> <ul> <li>The human who talked to the chatbot</li> <li>The human evaluator chats with the model and rates the chatbot on different dimensions on Likert scales</li> </ul> <p>Observer Evaluation:</p> <ul> <li>A third party who reads a transcript of a human/chatbot conversation</li> <li>Use third party annotators to look at the text of a complete conversation</li> <li>The acute-eval metric where annotators look at two separate human-computer conversations and choose the system which performed better</li> </ul>"},{"location":"note/Natural_Language_Processing/Chatbot/Concepts/#user-centered-design","title":"User-centered design","text":"<p>Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design</p> <ol> <li>Study the user and task<ol> <li>Interviewing users</li> <li>Investigating similar systems</li> </ol> </li> <li>Build simulations and prototypes<ol> <li>The Wizard-of-Oz system: the users interact with what they think is a program but is in fact a human \"wizard\" disguised by a software interface</li> </ol> </li> <li>Iteratively test the design on users<ol> <li>Embedded user testing</li> <li>Value sensitive design: the benefits, harms and possible stakeholders of the resulting system</li> </ol> </li> </ol>"},{"location":"note/Natural_Language_Processing/Chatbot/Models/","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Chatbot/Models/#gus-frame-and-slot-architecture","title":"GUS frame-and-slot architecture","text":""},{"location":"note/Natural_Language_Processing/Deployment/","title":"Deployment","text":""},{"location":"note/Natural_Language_Processing/Deployment/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Deployment/#calculate-gpu-memory-for-serving-llms","title":"Calculate GPU memory for serving LLMs","text":"<p>https://www.substratus.ai/blog/calculating-gpu-memory-for-llm</p>"},{"location":"note/Natural_Language_Processing/Deployment/#precision","title":"Precision","text":"<ul> <li>fp, bp, int, mixed, fixed</li> <li>As a rule of thumb, look for at least 4-bit quantized models, these models have a good balance between compression and accuracy</li> </ul>"},{"location":"note/Natural_Language_Processing/Deployment/#gguf","title":"GGUF","text":"<ul> <li>A GGUF model represents a compressed version of its original counterpart through quantization, which reduced the number of bits needed to represent the parameters of an LLM</li> </ul>"},{"location":"note/Natural_Language_Processing/Deployment/#framework","title":"Framework","text":""},{"location":"note/Natural_Language_Processing/Deployment/#llamacpp","title":"llama.cpp","text":""},{"location":"note/Natural_Language_Processing/Finetune/","title":"Fine-tuning","text":""},{"location":"note/Natural_Language_Processing/Finetune/#different-types-of-fine-tuning","title":"Different types of fine-tuning","text":""},{"location":"note/Natural_Language_Processing/Finetune/#continued-pre-training","title":"Continued Pre-training","text":"<ul> <li>Retrain all the parameters on the new data, using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining</li> <li>In a sense it's as if the new data were at the tail end of the pretraining data</li> <li>Retraining all the parameters of the model is very slow and expensive when the language model is huge</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#parameter-efficient-finetuning-peft","title":"Parameter-efficient Finetuning (PEFT)","text":"<ul> <li>Freeze some of the parameters and train only a subset of parameters on the new data</li> <li>e.g. LoRA</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#mlm-finetunig","title":"MLM Finetunig","text":"<ul> <li>Add an extra head</li> <li>Commonly used with masked language models like BERT</li> <li>Freeze the entire pretrained model and only train the classification head on some new data, usually labeled with some class that we want to predict</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#supervised-finetuning-sft","title":"Supervised Finetuning (SFT)","text":"<ul> <li>Often used for instruction tuning, in which we want a pretrained language model to learn to follow text instructions</li> <li>Create a dataset of prompts and desired responses, and train the language model using the normal cross-entropy loss to predict each token in the instruction prompt iteratively, essentially training it to produce the desired response from the command in the prompt</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Finetune/#model-alignment","title":"Model Alignment","text":"<ul> <li>Together we refer to instruction tuning and preference alignment as model alignment</li> <li>The intuition is that we weant the learning objectives of models to be aligned with the goals of the humans that use them</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#instruction-tuning","title":"Instruction Tuning","text":"<ul> <li>A technique that helps LLMs learn to correctly respond to instructions by taking a base pretrained LLM, and finetuning them on a corpus of instructions with their corresponding response</li> <li>To train the LLM to follow instructions for a range of tasks</li> <li>The resulting model not only learns those tasks, but also engages in a form of meta-learning: it improves its ability to follow instructions generally</li> <li>Sometimes called SFT for supervised finetuning</li> <li>Even though it is trained to predict the next token (which we traditionally think of as self-supervised), we call this method SFT because unlike in pretraining, each instruction or question in the instruction tuning data has a supervised objective: a correct answer to the question or a response to the instruction</li> </ul> <p>How to create instruction tuning datasets?</p> <ol> <li>For people to write the instances directly<ul> <li>time consuming and costly</li> </ul> </li> <li>Make use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks<ul> <li>To generate instruction-tuning data, these fields and the ground-truth labels are extracted from the training data, encoded as key/value pairs, and inserted in templates to produce instantiated instruction</li> </ul> </li> <li>Crowdsourcing based on carefully written annotation guidelines, which can include detailed step-by-step instructions, pitfalls to avoid, formatting instructions, length limits, exemplars, etc.</li> <li>Use language models</li> </ol> <p>Evaluation of Instruction-Tuned Models:</p> <ul> <li>Take a leave-one-out approach: instruction-tune a model on some large set of tasks and then assess it on a withheld task<ul> <li>But the enormous numbers of tasks in instruction-tuning datasets (e.g. 1600 for Super Natural Instructions) often overlap</li> </ul> </li> <li>Large instruction-tuning datasets are partitioned into clusters based on task similarity, the leave-one-out training/test approach is then applied at the cluster level<ul> <li>i.e. to evaluate a model's performance on sentiment analysis, all the sentiment analysis datasets are removed from the training set and reserved for testing</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#preference-alignment","title":"Preference Alignment","text":"<ul> <li>A separate model is trained to decide how much a candidate response aligns with human preferences</li> <li>e.g. RLHF, DPO</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#models","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Finetune/#low-rank-adaptation-lora","title":"Low-Rank Adaptation (LoRA)","text":"<ul> <li>Freeze layers \\(\\textbf{W}^Q\\), \\(\\textbf{W}^K\\), \\(\\textbf{W}^V\\), \\(\\textbf{W}^O\\) during finetuning, and instead update a low-rank approximation that has fewer parameters</li> <li>Replace \\(\\textbf{W} + \\Delta \\textbf{W}\\) with \\(\\textbf{W} + \\textbf{AB}\\)</li> <li>Forward pass: \\(\\textbf{h} = \\textbf{xW} + \\textbf{xAB}\\)</li> <li>Dramatically reduces hardware requirements, since gradients don't have to be calculated for most parameters</li> <li>The weight updates can be simply added into the pretrained weights</li> <li>Since \\(\\textbf{AB}\\) is the same size as \\(\\textbf{W}\\), it doesn't add any time during inference</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#reinforcement-learning-with-human-feedback-rlhf","title":"Reinforcement Learning with Human Feedback (RLHF)","text":"<ul> <li>Give a system a dialogue context and sample two possible turns from the language model</li> <li>Then have humans label which of the two is better, creating a large dataset of sentence pairs with human preferences</li> <li>These pairs are used to train a dialogue policy, and reinforcement learning is used to train the language model to generate turns that have higher rewards</li> </ul>"},{"location":"note/Natural_Language_Processing/Finetune/#direct-preference-optimization-dpo","title":"Direct Preference Optimization (DPO)","text":""},{"location":"note/Natural_Language_Processing/LLM/Concepts/","title":"Concepts","text":"<p>Refer to the LLM Bootcamp</p>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#language-models-lms","title":"Language Models (LMs)","text":"<ul> <li>The models that assign a probability to each possible next word</li> <li>LMs can also assign a probability to an entire sequence</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#representation-vs-generative","title":"Representation vs Generative","text":"<ol> <li>Representation Language Models<ul> <li>Do not generate text but are commonly used for task-specific use cases</li> <li>e.g. Classification</li> </ul> </li> <li>Generative Language Models<ul> <li>LLMs that generate text</li> <li>e.g. GPT models</li> </ul> </li> </ol>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#causal-autoregressive-language-models","title":"Causal / Autoregressive Language Models","text":"<ul> <li>Iteratively predict words left-to-right from earlier words</li> <li>Conditional generation</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#proprietary-private-models","title":"Proprietary, Private Models","text":"<ul> <li>Models that do not have their weights and architecture shared with the public</li> <li>Access through API, a paid service</li> <li>e.g. OpenAI's GPT-4 and Anthropic's Claude</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#open-models","title":"Open Models","text":"<ul> <li>Models that share their weights and architecture with the public to use</li> <li>Varying levels of licensing that may or may not allow commercial usage of the model</li> <li>e.g. Cohere's Command R, the Mistral models, Microsoft's Phi, Meta's Llama</li> <li>Leaderboard: https://huggingface.co/open-llm-leaderboard</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#top-k-sampling","title":"Top-k Sampling","text":"<ul> <li>Simple generalization of greedy decoding</li> <li>When \\(k=1\\), identical to greedy decoding</li> </ul> <p>Steps:</p> <ol> <li>Choose in advance a number of words \\(k\\)</li> <li>For each word in vocabulary \\(V\\), use the language model to compute the likelihood of this word given the context \\(p(w_t | \\textbf{w}_{&lt;t&gt;})\\)</li> <li>Sort the words by their likelihood, and throw away any word that is not one of the top \\(k\\) most probable words</li> <li>Renormalize the scores of the \\(k\\) words to be a legitimate probability distribution</li> <li>Randomly sample a word from within these remaining \\(k\\) most-probable words according to its probability</li> </ol>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#nucleus-or-top-p-sampling","title":"Nucleus or Top-p Sampling","text":"<ul> <li>To keep not the top \\(k\\) words, but the top \\(p\\) percent of the probability mass</li> <li>The hope is that the measure will be more robust in very different contexts, dynamically increasing and decreasing the pool of word candidates</li> </ul> <p>Given a distribution \\(P(w_{t} | \\textbf{w}_{&lt;t})\\), the top-\\(p\\) vocabulary \\(V^{(p)}\\) is the smallest set of words such that:</p> \\[\\sum_{w \\in V^{(p)}}{P(w_{t} | \\textbf{w}_{&lt;t}) \\geq p}\\]"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#temperature-sampling","title":"Temperature Sampling","text":"\\[\\textbf{y} = \\text{softmax}(u / \\tau)\\] <ul> <li>Don't truncate the distribution, but instead reshape it</li> <li>The intuition comes from thermodynamics:<ul> <li>A system at a high temperature is very flexible and can explore many possible states</li> <li>A system at a lower temperature is likely to explore a subset of lower energy (better) states</li> </ul> </li> <li>In low-temperature sampling, we smoothly increase the probability of the most probable words and decrease the probability of the rare words<ul> <li>The lower \\(\\tau\\) is, the larger the scores being passed to the softmax</li> <li>Softmax tends to push high values toward 1 and low values toward 0</li> </ul> </li> <li>Divide the logit by a temperature parameter \\(\\tau\\)<ul> <li>Low-temperature sampling: \\(\\tau \\in (0,1]\\), making the distribution more greedy</li> <li>High-temperature sampling: \\(\\tau &gt; 1\\), flatten the distribution</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#pretraining","title":"Pretraining","text":""},{"location":"note/Natural_Language_Processing/LLM/Concepts/#self-supervised-training-algorithm","title":"Self-supervised Training Algorithm","text":"<ul> <li>Cross-entropy loss: the negative log probability the model assigns to the next word in the training sequence<ul> <li>\\(L_{CE}(\\hat{\\textbf{y}}_t, \\textbf{y}_t) = - \\log{\\textbf{y}_t [w_{t+1}]}\\)</li> </ul> </li> <li>Teacher forcing: always give the model the corrext history sequence to predict the next word</li> </ul> <ul> <li>At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary</li> <li>During trianing, the probabiliy assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence</li> <li>The loss for a training sequence is the average cross-entropy loss over the entire sequence</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#training-corpora","title":"Training Corpora","text":"<ul> <li>Mainly trained on text scraped from the web, augmented by more carefully curated data<ul> <li>e.g. Common Crawl, The Pile</li> </ul> </li> <li>Likely to contain many natural examples<ul> <li>e.g. question-answer pairs (e.g. from FAQ lists), translations of sentences between various languages, documents together with their summaries</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#filtering-for-quality-and-safety","title":"Filtering for quality and safety","text":"<ul> <li>Pretraining data drawn from the web is filtered for both quality and safety</li> <li>Quality filtering:<ul> <li>Subjective, different quality filters are trained in different ways, but often to value high-quality reference corpora like Wikipedia, books, and particular websites</li> <li>To avoid websites with lots of PII (Personal Identifiable Information) or adult content</li> <li>Removes boilerplate text</li> <li>Deduplication, various levels: remove duplicate documents, duplicate web pages, or duplicate text</li> </ul> </li> <li>Safety filtering:<ul> <li>Subjective</li> <li>Often includes toxicity detection</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#evaluation","title":"Evaluation","text":""},{"location":"note/Natural_Language_Processing/LLM/Concepts/#perplexity","title":"Perplexity","text":"\\[\\text{Perplexity}_{\\theta}(w_{1:n}) = \\sqrt[n]{\\prod_{i=1}^{n}{\\frac{1}{P_{\\theta}(w_i | w_{&lt;i})}}}\\] <ul> <li>Because perplexity depends on the length of a text, it is very sensitive to differences in the tokenization algorithm</li> <li>Hard to exactly compare perplexities produced by two language models if they have very different tokenizers</li> <li>Perplexity is best used when comparing language models that use the same tokenizer</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#other-factors","title":"Other factors","text":"<ul> <li>Task-specific metrics that allow us to evaluate how accuracy or correct language models are at the downstream tasks</li> <li>How big a model is, and how long it takes to train or do inference</li> <li>Constraints on memory, since the GPUs have fixed memory sizes</li> <li>Measuring performance normalized to a giving compute or memory budget, or directly measure the energy usage of the model in kWh or in kilograms of \\(\\text{CO}_2\\) emitted</li> <li>Fairness: because language models are biased</li> <li>Leaderboard: e.g. Dynabench, HELM</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#scale","title":"Scale","text":"<p>The performance of LLMs are mainly determined by 3 factors:</p> <ol> <li>Model Size:<ul> <li>The number of parameters not counting embeddings</li> <li>Improve a model by adding parameters (adding more layers or having wider contexts or both)</li> </ul> </li> <li>Dataset Size:<ul> <li>The amount of training data</li> <li>Improve a model by adding more training data</li> </ul> </li> <li>Amount of Compute Used for Training:<ul> <li>Improve a model bt training for more iterations</li> </ul> </li> </ol>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#scaling-laws","title":"Scaling Laws","text":"<ul> <li>Loss \\(L\\) as a function ofthe number of non-embedding parameters \\(N\\), the dataset size \\(D\\), and the compute budget \\(C\\):</li> <li>The constants depend on the exact transformer architecture, tokenization, and vocabulary size, so rather than all the precise values, scaling laws focus on the relationship with loss</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#kv-cache","title":"KV Cache","text":"<ul> <li>The attention vector can be very efficiently computed in parallel for training</li> <li>Not the case during inference, because we iteratively generate the next tokens one at a time. For a new token just generated, call it \\(\\textbf{x}_i\\), we need to compute its query, key, values by multiplying by \\(\\textbf{W}^Q\\), \\(\\textbf{W}^K\\), \\(\\textbf{W}^V\\), respectively</li> <li>But a waste of computation time to recompute the key and value vectors for all the prior tokens \\(\\textbf{x}_{&lt;i}\\)</li> <li>Instead of recomputing, whenever we compute the key and value vectors we store them in memory in the KV cache, and then we can just grab them from the cache when we need them</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/Concepts/#mixture-of-experts-moe","title":"Mixture of Experts (MoE)","text":""},{"location":"note/Natural_Language_Processing/LLM/Models/","title":"Models","text":"<ul> <li>GPT Family</li> <li>BERT Family</li> <li>PaLM Family</li> <li>LLaMA Family</li> <li>Claude Family</li> <li>Mistral?</li> <li>Tsinghua?</li> </ul>"},{"location":"note/Natural_Language_Processing/LLM/simple_phi3/","title":"Simple phi3","text":"In\u00a0[1]: Copied! <pre>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n</pre> import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  from transformers import pipeline from transformers import AutoTokenizer, AutoModelForCausalLM <pre>/home/junwai/miniconda3/envs/wise/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[2]: Copied! <pre># Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"microsoft/Phi-3-mini-4k-instruct\",\n  device_map=\"cuda\",\n  torch_dtype=\"auto\",\n  trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n</pre> # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(   \"microsoft/Phi-3-mini-4k-instruct\",   device_map=\"cuda\",   torch_dtype=\"auto\",   trust_remote_code=True )  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") <pre>`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:01&lt;00:00,  1.52it/s]\n</pre> In\u00a0[3]: Copied! <pre>model\n</pre> model Out[3]: <pre>Phi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)</pre> In\u00a0[4]: Copied! <pre>tokenizer\n</pre> tokenizer Out[4]: <pre>LlamaTokenizerFast(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;|endoftext|&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"&lt;/s&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n\t32000: AddedToken(\"&lt;|endoftext|&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t32001: AddedToken(\"&lt;|assistant|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32002: AddedToken(\"&lt;|placeholder1|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32003: AddedToken(\"&lt;|placeholder2|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32004: AddedToken(\"&lt;|placeholder3|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32005: AddedToken(\"&lt;|placeholder4|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32006: AddedToken(\"&lt;|system|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32007: AddedToken(\"&lt;|end|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32008: AddedToken(\"&lt;|placeholder5|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32009: AddedToken(\"&lt;|placeholder6|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n\t32010: AddedToken(\"&lt;|user|&gt;\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n}</pre> In\u00a0[5]: Copied! <pre># Create a pipeline\ngenerator = pipeline(\n  \"text-generation\",\n  model=model,\n  tokenizer=tokenizer,\n  return_full_text=False,\n  max_new_tokens=500,\n  do_sample=False\n)\n</pre> # Create a pipeline generator = pipeline(   \"text-generation\",   model=model,   tokenizer=tokenizer,   return_full_text=False,   max_new_tokens=500,   do_sample=False ) In\u00a0[7]: Copied! <pre># The prompt\nmessages = [\n  {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n]\n\n# Generate output\noutput = generator(messages)\nprint(output[0][\"generated_text\"])\n</pre> # The prompt messages = [   {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"} ]  # Generate output output = generator(messages) print(output[0][\"generated_text\"]) <pre> Why did the chicken join the band? Because it had the drumsticks!\n</pre>"},{"location":"note/Natural_Language_Processing/LLM/simple_phi3/#model","title":"Model\u00b6","text":""},{"location":"note/Natural_Language_Processing/LLM/simple_phi3/#tokenizer","title":"Tokenizer\u00b6","text":""},{"location":"note/Natural_Language_Processing/LLM/simple_phi3/#pipeline","title":"Pipeline\u00b6","text":""},{"location":"note/Natural_Language_Processing/LLM/simple_phi3/#prompt","title":"Prompt\u00b6","text":""},{"location":"note/Natural_Language_Processing/Machine_Translation/","title":"Machine Translation (MT)","text":""},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/","title":"Concept","text":""},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#information-access","title":"Information access","text":"<ul> <li>The freedom or ability to identify, obtain and make use of database or information effectively</li> <li>The most common current use of machine translation</li> <li>Improvements in machine translation can help reduce the digital divide in information access: that fact that much more information is available in English and other languages spoken in wealthy countries</li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#word-order-typology","title":"Word Order Typology","text":"<ol> <li>SVO: e.g. German, French, English, Mandarin</li> <li>SOV: e.g. Hindi, Japanese</li> <li>VSO: e.g. Irish, Arabic</li> </ol> <ul> <li>SOV = Subject-Verb-Object</li> <li>Two languages that share their basic word order type often have other similarities</li> <li>e.g. VO languages generally have prepositions, whereas OV languages generally have postpositions</li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#lexical-divergences","title":"Lexical Divergences","text":"<ul> <li>Languages differ in lexically dividing up the conceptual space, either one-to-many or even many-to-many translation</li> <li>Sometimes one language places more grammatical constraints on word choise than another</li> <li>One language may have a lexical gap, where no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language</li> <li>The field of MT and Word Sense Disambiguation are closely linked</li> <li>Languages differ in how the conceptual properties of an event are mapped onto specific words:<ul> <li>Verb-framed languages: e.g. Spanish</li> <li>Satellite-framed languages: e.g. English</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#morphological-typology","title":"Morphological Typology","text":"<p>Morphologically, languages are often characterized along two dimensions of variation:</p> <ol> <li>The number of morphemes per word<ol> <li>Isolating languages: e.g. Vietnamese and Cantonese, in which each word generally has one morpheme</li> <li>Polysynthetic languages e.g. Siberian Yupik (\"Eskimo\"), in which a single word may have very many morphemes, corresponding to a whole sentence in English</li> </ol> </li> <li>The degree to which morphemes are segmentatble<ol> <li>Agglutinative languages: e.g. Turkish, in which morphemes have relatively clean boundaries</li> <li>Fusion languages: e.g. Russian, in which a single affix may conflate multiple morphemes</li> </ol> </li> </ol> <ul> <li>Translating between languages with rich morphology requires dealing with structure below the word level</li> <li>Thus generally use subword models like WordPiece or BPE</li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#referential-density","title":"Referential Density","text":"<ul> <li>The differences in frequencies of omission across pro-drop languages, e.g. Japanese and Chinese tend to omit far more than Spanish</li> <li>Pro-drop languages: languages that can omit pronouns</li> <li>Languages that tend to use more pronouns are more referentially dense than those that use more zeros</li> <li>Cold languages: referentially sparse languages, e.g. Chinese or Japanese, require the hearer to do more inferential work to recover antecedents</li> <li>Hot languages: languages that are more explicit and make it easier for the hearer</li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Concepts/#mt-evaluation","title":"MT Evaluation","text":"<ul> <li>chrF, BLEU</li> </ul>"},{"location":"note/Natural_Language_Processing/Machine_Translation/Models/","title":"Model","text":""},{"location":"note/Natural_Language_Processing/Machine_Translation/Models/#bertscore","title":"BERTScore","text":""},{"location":"note/Natural_Language_Processing/Masked_LM/","title":"Masked Language Models","text":""},{"location":"note/Natural_Language_Processing/Multimodal/","title":"Multimodel LLM","text":"<p>e.g. LLaVa</p>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/","title":"Prompt Engineering","text":"<ul> <li>https://www.developer.tech.gov.sg/products/collections/data-science-and-artificial-intelligence/playbooks/prompt-engineering-playbook-beta-v3.pdf</li> <li>https://www.promptingguide.ai/</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#prompt","title":"Prompt","text":"<ul> <li>A text string that a user issues to a language model to get the model to do something useful</li> <li>In prompting, the user's prompt string is passed to the language model, which iteratively generates tokens conditioned on the prompt</li> </ul> <p>We prompt an LM by transforming each task into a form that is amenable to contextual generation by an LLM:</p> <ol> <li>For a given task, develop a task-specific template that has a free parameter for the input text</li> <li>Given that input and the task-specific template, the input is used to instantiate a filled prompt that is then passed to a pretrained language model</li> <li>Autoregressive decoding is then used to generate a sequence of token outputs</li> <li>The output of the model can either be used directly as the desired output (as in the case of naturally generative tasks such as translation or summarization), or a task-appropriate answer can be extracted from the generated output (as in the case of classification)</li> </ol>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#few-shot-prompting","title":"Few-shot Prompting","text":"<ul> <li>Include some demonstrations in the prompt template</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#how-many-demonstrations","title":"How Many Demonstrations?","text":"<ul> <li>A small number of randomly selected labeled examples used as demonstrations can be sufficient to improve performance over the zero-shot setting</li> <li>The largest performace gains in few-shot prompting tends to come from the first demonstration, with diminishing returns for subsequent demonstrations</li> <li>Why isn\u2019t it useful to have more demonstrations? The reason is that the primary benefit in examples is to demonstrate the task to be performed to the LLM and the format of the sequence, not to provide relevant information as to the right answer</li> <li>Demonstrations that have incorrect answers can still improve a system</li> <li>Adding too many examples seems to cause the model to overfit to details of the exact examples chosen and generalize poorly</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#how-to-select-demonstrations","title":"How to Select Demonstrations?","text":"<ul> <li>Demonstrations are generally created by formatting examples drawn from a labeled training set</li> <li>Some heuristics about what makes a good demonstration: using demonstrations that are similar to the current input seems to improve performance</li> <li>More generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set</li> <li>Systems like DSPy, a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#in-context-learning","title":"In-context Learning","text":"<ul> <li>Prompting as in-context learning: learning that improves model performance or reduces some loss but does not involve gradient-based updates to the model's underlying parameters</li> <li>We don't know how in-context learning works, but there are some hypotheses</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#induction-heads","title":"Induction Heads","text":"<ul> <li>The function of the induction head is to predict repeated sequences</li> <li>If it sees the pattern \\(AB ... A\\) in an input sequence, it predicts that \\(B\\) will follow, instatiating the pattern completion rule \\(AB ... A \\rightarrow B\\)</li> <li>A generalized fuzzy version: \\(A^* B^* ... A \\rightarrow B^*\\), where \\(A^* \\approx A\\) and \\(B^* \\approx B\\), might be responsible for in-context learning</li> <li>Suggestive evidence show that ablating induction heads causes in-context learning performance to decrease</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#chain-of-thought-cot","title":"Chain-of-Thought (CoT)","text":"<ul> <li>Each of the demonstrations in the few-shot prompt is augmented with some text explaining some reasoning steps</li> <li>The goal is t cause the language model to output similar kinds of reasoning steps for the problem being solved, and for the output of those reasoning steps to cause the system to generate the correct answer</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#automatic-prompt-optimization","title":"Automatic Prompt Optimization","text":"<ul> <li>Given a prompt for a task (human or computer generated), prompt optimization methods search for prompts with improved performance</li> <li>Can be viewed as a form of iterative improvement search through a space of possible prompts for those that optimize performance on a task</li> <li>Components:<ul> <li>A start state: an intial human or machine generated prompt or prompts suitable for some task</li> <li>A scoring metric: a method for assessingn how well a given prompt performs on the task</li> <li>An expansion method: a method for generating variations of a prompt</li> </ul> </li> <li>Beam search</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#candidate-scoring","title":"Candidate Scoring","text":"<ul> <li>Assess the likely performance of potential prompts, both to identify promising avenues of seach and to prune those that are unlikely to be effective</li> <li>Given access to labeled training data, candidate prompts can be scored based on execution accuracy<ul> <li>Candidate prompts are combined with inputs sampled from the training data and passed to an LLM for decoding</li> <li>The LLM output is evaluated againts training label using a metric appropirate for the task</li> </ul> </li> <li>Evaluating each candidate prompt against a complete training set would be infeasible</li> <li>Instead, prompt performance is estimated from a small sample of training data</li> </ul>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#prompt-expansion","title":"Prompt Expansion","text":"<ul> <li>Generates variants of a given prompt to create an expanded set of neighboring prompts that improve performance over the original</li> </ul> <p>Methods:</p> <ol> <li>Use language models to create paraphrases for the prompt</li> <li>Truncate the current prompt as a set of random locations, generating a set of prompt prefixes, the paraphrasing LLM is then asked to continue each of the prefixes to generate a complete prompt<ul> <li>Uninformed search: the candidate expansion step is not directed towards generating better candidates; candidates are generated without regard to their quality. It it is the job of the priority queue to elevate improved candidates when they are found</li> </ul> </li> <li>Attempts to generate superior prompts:<ol> <li>Run the prompt on a sample of training examples</li> <li>Identify examples where the prompt fails</li> <li>Ask an LLM to produce a critique of the prompt in light of the failed examples</li> <li>Provide the resulting critique to an LLM, and ask it to generate improved prompts</li> </ol> </li> </ol>"},{"location":"note/Natural_Language_Processing/Prompt_Engineering/#evaulate-prompted-language-models","title":"Evaulate Prompted Language Models","text":""},{"location":"note/Natural_Language_Processing/RAG/","title":"Retrieval Augmented Generation","text":""},{"location":"note/Natural_Language_Processing/RAG/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/RAG/#factoid-questions","title":"Factoid Questions","text":"<ul> <li>Questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#information-retrieval-ir","title":"Information Retrieval (IR)","text":"<ul> <li>The task of finding the document \\(d\\) from the \\(D\\) documents in some collection that best matches a query \\(q\\)</li> <li>The resulting IR system is often called a search engine</li> <li>Represent the document and query with length \\(|V|\\)</li> <li>Compare two vectors to find how similar they are</li> <li>In modern IR systems, the use of stop lists is much less common, partly due to improved efficiency and partly because much of their function is already handled by IDF weighting, which downweights function words that occur in every document</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#ad-hoc-retrieval","title":"Ad-hoc Retrieval","text":"<ul> <li>An example of the bag-of-words model, since words are considered independently of their positions</li> <li>A user poses a query to a retrieval system, which then returns an ordered set of documents from some collection</li> <li>Uses the vector space model, in which we map queries and document to vectors based on unigram word counts</li> <li>Uses the cosine similarity between the vectors to rank potential documents</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#document","title":"Document","text":"<ul> <li>Whatever unit of text the system indexes and retrieves (web pages, scientific papers, news articles, or even shorter passages like paragraphs)</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#collection","title":"Collection","text":"<ul> <li>A set of documents being used to satisfy user requests</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#term","title":"Term","text":"<ul> <li>A word in a collection, but it may also include phrases</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#query","title":"Query","text":"<ul> <li>A user's information need expressed as a set of terms</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#term-weighting","title":"Term Weighting","text":"<ul> <li>We don't use raw word counts in IR, instead computing a term weight for each word</li> <li>e.g. TF-IDF, BM25 (slightly powerful variant)</li> <li>Conceptual flaw: they work only if there is exact overlap of words between the query and the document</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#document-scoring","title":"Document Scoring","text":"<ul> <li>Used for ranking documents based on their similarity to a query</li> </ul> <p>We score document \\(d\\) by the cosine of its vector \\(\\textbf{d}\\) with the query vector \\(\\textbf{q}\\):</p> \\[\\text{score}(q, d) = \\cos{(\\textbf{q}, \\textbf{d})} = \\frac{\\textbf{q} \\cdot \\textbf{d}}{|\\textbf{q}| \\cdot |\\textbf{d}|} = \\sum_{t \\in \\textbf{q}}{\\frac{\\text{tf-idf}(t, q)}{\\sqrt{\\sum_{q_i \\in q}{\\text{tf-idf}^2(q_i, q)}}} \\cdot \\frac{\\text{tf-idf}(t, d)}{\\sqrt{\\sum_{d_i \\in d}{\\text{tf-idf}^2(d_i, d)}}}}\\]"},{"location":"note/Natural_Language_Processing/RAG/#inverted-index","title":"Inverted Index","text":"<ul> <li>The basic search problem in IR is to find all documents \\(d \\in C\\) that contain a term \\(q \\in Q\\)</li> <li>An inverted index, given a query term, gives a list of documents that contain the term</li> <li>It consists of two parts:<ul> <li>Dictionary: a list of terms, each poiting to a postings list for term; can also store the document frequency for each term</li> <li>Postings: a postings list is the list of document IDs associated with each term, which can also contain information like the term frequency or even the exact positions of terms in the document</li> </ul> </li> <li>Given a list of terms in query, we can very efficiently get lists of all candidate documents, together with the information necessary to compute the tf-idf scores we need</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#ir-with-dense-vectors","title":"IR with Dense Vectors","text":"<ul> <li>To handle the vocabulary mismatch problem: the user posing a query needs to guess exactly what words the writer of the answer might have used</li> <li>An approach that can handle synonymy: instead of (sparse) word-count vectors, using (dense) embeddings</li> <li>Supervised algorithms (e.g. ColBERT) need training data in the form of queries together with relevant and irrelevant passages or documents</li> <li>If datasets don\u2019t have labeled positive examples, Relevance-Guided Supervision can be used</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#bert","title":"BERT","text":""},{"location":"note/Natural_Language_Processing/RAG/#colbert","title":"ColBERT","text":"<ul> <li>For each token in \\(q\\), ColBERT finds the most contextually similar token in \\(d\\), and then sums up these similarities</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#nearest-neighbor-search","title":"Nearest Neighbor Search","text":"<ul> <li>Every possible document must be ranked for its similarity to the query:<ul> <li>Sparse word-count vector: inverted index</li> <li>Dense vector: nearest neighbor search, e.g. Faiss</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#evaluation-of-ir-systems","title":"Evaluation of IR Systems","text":""},{"location":"note/Natural_Language_Processing/RAG/#precision-and-recall","title":"Precision and Recall","text":"<ul> <li>Assume that each document returned by the IR system is either relevant or not</li> <li>Precision: the fraction of returned documents that are relevant</li> <li>Recall: the fraction of relevant documents that are returned</li> <li>Don't adequately measure the performance of a system that ranks the documents it returns</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#interpolated-precision","title":"Interpolated Precision","text":"<ul> <li>Let us average performance over a set of queries</li> <li>Also helps smooth over the irregular precision values</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#mean-average-precision-map","title":"Mean Average Precision (MAP)","text":"<p>Average Precision (AP) for a single query:</p> \\[\\text{AP} = \\frac{1}{|R_r|} \\sum_{d \\in R_r}{\\text{Precision}_r (d)}\\] <ul> <li>\\(R_r\\): the set of relevant documents at or above \\(r\\)</li> <li>\\(\\text{Precision}_r (d)\\): the precision measured at the rank at which document \\(d\\) is found</li> </ul> <p>For an ensemble of queries \\(Q\\):</p> \\[\\text{MAP} = \\frac{1}{|Q|} \\sum_{q \\in Q}{\\text{AP}(q)}\\]"},{"location":"note/Natural_Language_Processing/RAG/#rag","title":"RAG","text":"<ul> <li>Address hallucination (unable to show textual evidence to support the answer), and the problem of unable to answer questions from proprietary data</li> <li>Condition on the retrieved passages as part of the prefix</li> <li>As with the span-based extraction reader, successfully applying the retrievalaugmented generation algorithm for QA requires a successful retriever, and often a two-stage retrieval algorithm is used in which the retrieval is reranked</li> <li>Some multihop complex questions may require multi-hop architectures, in which a query is used to retrieve documents, which are then appended to the original query for a second stage of retrieval</li> <li>Details of prompt engineering also have to be worked out, like decising whether to demarcate passages</li> </ul> \\[p(x_1, ..., x_n) = \\prod_{i=1}^{n}{p(x_i | R(q); ~ \\text{prompt}; ~ \\text{[Q:]}; q; ~ \\text{[A:]}; ~ x_{&lt;i})}\\]"},{"location":"note/Natural_Language_Processing/RAG/#rag-evaluation","title":"RAG Evaluation","text":""},{"location":"note/Natural_Language_Processing/RAG/#question-answering-datasets","title":"Question Answering Datasets","text":"<ul> <li>Scores of QA datasets, used both for instruction tuning and for evaluation of the QA abilities of language models</li> <li>Dimensions:<ul> <li>Questions: natural, probing</li> <li>Answers: multiple-choice, free form</li> <li>Prompting: zero-shot (just the question), few-shot (given demos of answers to similar questions)</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#natural-information-seeking-questions","title":"Natural information-seeking questions","text":"<ul> <li>e.g. Natural Questions, MS MARCO, DuReader, TyDi QA</li> <li>Open book QA task: Question answering given one or more documents, e.g. RAG</li> <li>Closed book QA task: Answering directly from the LM with no retrieval component</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#questions-designed-for-probing","title":"Questions designed for probing","text":"<ul> <li>Probing: evaluating or testing systems or humans</li> <li>e.g. MMLU</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#evaluation-of-qa","title":"Evaluation of QA","text":""},{"location":"note/Natural_Language_Processing/RAG/#exact-match","title":"Exact match","text":"<ul> <li>For multiple-choice questions like in MMLU</li> <li>The percentage of predicted answers that match the true answer exactly</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#f-1-score","title":"F-1 score","text":"<ul> <li>For questions with free text answers, like Natural Questions</li> <li>The average token overlap between predicted and true answers</li> <li>Treat the prediction and true answer as a bag of tokens, and compute the \\(F_1\\) score for each question</li> <li>Then return the average \\(F_1\\) score across all questions</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#mean-reciprocal-rank-mrr","title":"Mean Reciprocal Rank (MRR)","text":"<ul> <li>For QA systems who return multiple ranked answers</li> <li>Each test question is scored with the reciprocal of the rank of the first correct answer</li> </ul> \\[\\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|}{\\frac{1}{rank_i}}\\]"},{"location":"note/Natural_Language_Processing/RAG/#models","title":"Models","text":""},{"location":"note/Natural_Language_Processing/RAG/#best-match-25-bm25","title":"Best Match 25 (BM25)","text":"<ul> <li>Sometimes called Okapi BM25, after the Okapi IR system</li> <li>Adds two parameters:<ul> <li>\\(k\\): adjust the balance between tf and idf</li> <li>\\(b\\): controls the important of document length normalization</li> </ul> </li> <li>\\(|d_{\\text{avg}}|\\): the average document length in the collection</li> </ul> <p>The BM25 score for a query \\(q\\) and document \\(d\\) is:</p> \\[\\text{score}(q, d) = \\sum_{t \\in q}{\\log{\\left(\\frac{N}{df_t}\\right)} \\cdot \\frac{tf_{t, d}}{k(1 - b + b \\cdot \\frac{|d|}{|d_{\\text{avg}}|}) + tf_{t,d}}}\\] <ul> <li>When \\(k = 0\\), BM25 reverts to no use of term frequency, just a binary selection of terms in the query (plus idf)</li> <li>A large \\(k\\) results in raw term frequency (plus idf)</li> <li>\\(b\\) ranges from 1 (scaling by document length) to 0 (no length scaling)</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#hnsw","title":"HNSW","text":"<ul> <li>Hierarchical navigable small world</li> </ul>"},{"location":"note/Natural_Language_Processing/RAG/#graphrag","title":"GraphRAG","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-indexing","title":"RAG Indexing","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-retrieval","title":"RAG Retrieval","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-generation","title":"RAG Generation","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-routing","title":"RAG Routing","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-query-translation","title":"RAG Query Translation","text":""},{"location":"note/Natural_Language_Processing/RAG/#rag-query-construction","title":"RAG Query Construction","text":""},{"location":"note/Natural_Language_Processing/SLM/","title":"Small Language Models","text":""},{"location":"note/Natural_Language_Processing/SLM/#references","title":"References","text":"<ol> <li>Small Language Models: Survey, Measurements, and Insight</li> </ol>"},{"location":"note/Natural_Language_Processing/SLM/Concepts/","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/SLM/Concepts/#overview-of-slms","title":"Overview of SLMs","text":""},{"location":"note/Natural_Language_Processing/SLM/Concepts/#training-datasets","title":"Training Datasets","text":""},{"location":"note/Natural_Language_Processing/SLM/Models/","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Speech/","title":"Speech","text":""},{"location":"note/Natural_Language_Processing/Speech/Concepts/","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Speech/Concepts/#automatic-speech-recognition-asr","title":"Automatic Speech Recognition (ASR)","text":"<ul> <li>To process human speech into readable text (sequenes of graphemes)</li> <li>Also known as speech-to-text</li> <li>Generally based on the encoder-decoder architecture</li> <li>Speaker-independent: trained on large corpora with thousands of hours of speech from many speakers</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#task-variation","title":"Task Variation","text":"<ol> <li>Vocabulary size<ol> <li>Open-ended tasks like transcribing videos or conversations are much harder</li> </ol> </li> <li>Who the speaker is talking to<ol> <li>Humans speaking to machines (either read speech or talking to a dialogue system) are easier to recognize than humans speaking to humans (conversational speech)</li> </ol> </li> <li>Channel and noise</li> <li>Accent or speaker-class characteristics</li> </ol>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#evaluation-word-error-rate","title":"Evaluation: Word Error Rate","text":"<ul> <li>How much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription</li> <li>Compute the minimum edit distance to get the minimum number of word substituitions, word insertions, and word deletions necessary to map between the correct and hypothesized strings</li> <li>The standard method for computing WER is a free script called sclite</li> </ul> \\[\\text{Word Error Rate} = 100 \\times \\frac{\\text{Insertions} + \\text{Substitutions} + \\text{Deletions}}{\\text{Total Words in Correct Transcript}}\\]"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#statistical-significance","title":"Statistical Significance","text":"<ul> <li>Matched-Pair Sentence Segment Word Error (MAPSSWE)</li> <li>McNemar's Test</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#log-mel-spectrum","title":"Log Mel Spectrum","text":"<ul> <li>The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal</li> <li>We represent sound waves by plotting the change in air pressure over time</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#sampling-and-quantization","title":"Sampling and Quantization","text":"<ul> <li>Analog-to-digital conversion has two steps: sampling and frequency</li> <li>Nyquist theorem: e.g. most information in human speech is in frequencies below 10,000 Hz, thus a 20,000 Hz sampling rate would be necessary for complete accuracy</li> <li>Amplitude measurements are stored as integers, either 8-bit (\\(-128\u2013127\\)) or 16-bit (\\(-32768\u201332767\\))</li> <li>Once data is quantized, it is store in various formats: e.g. Microsoft's .wav and Apple's AIFF<ul> <li>Sample rate: e.g. 8 kHz</li> <li>Sample size: e.g. 8-bit</li> <li>Number of channels: e.g. for stereo data or for two-party conversations, we can store both channels in the same file or separate files</li> <li>Individual sample storage: linear or compression, e.g. linear PCM, \\(\\mu\\)-law</li> </ul> </li> </ul> <p>Compressing a linear PCM sample value \\(x\\) to 8-bit \\(\\mu\\)-law (\\(\\mu = 255\\)):</p> \\[F(x) = \\frac{\\text{sgn}(x) \\log{(1 + \\mu |x|)}}{\\log{(1 + \\mu)}}\\]"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#windowing","title":"Windowing","text":"<ul> <li>Extract spectral features from a small window of speech that characterizes part of a particular phoneme</li> <li>Inside this small window, the signal as stationary. By contrast, in general, speech is a non-stationary signal</li> <li>The speech extracted from each window is called a frame</li> <li>Windowing parameters: window/frame size, frame stride/shift/offset, shape of the window</li> <li>The rectangular window abruptly cuts off the signal at its boundaries, which creates problem during Fourier analysis</li> <li>Hamming window shrinks the values of the signal toward zero at the window boundaries, avoiding discontinuities</li> </ul> <p>To extract the signal we multiply the value of the signal at time \\(n\\), \\(s[n]\\) by the value of the window at time \\(n\\), \\(w[n]\\)</p> \\[y[n] = w[n]s[n]\\]"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#discrete-fourier-transform","title":"Discrete Fourier Transform","text":"<ul> <li>Extract spectral information from windowed signal, to know how much energy the signal contains at different frequency bands</li> <li>FFT to compute DFT, only works for values of \\(N\\) that are powers of 2</li> <li>The results of the FFT tell us the energy at each frequency band</li> </ul> \\[X[k] = \\sum_{n=0}^{N-1}{x[n]\\exp^{-j \\frac{2\\pi}{N}kn}}\\]"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#mel-filter-bank-and-log","title":"Mel Filter Bank and Log","text":"<ul> <li>Human hearing is not equally sensitive at all frequency bands, it is less sensitive at higher frequencies</li> <li>Information in low frequencies (e.g. formants) is crucial for distinguishing vowels or nasals, while information in high frequencies (e.g. stop bursts or fricative noise) is less crucial for successful recognition</li> <li>The mel filter bank collect energy from each frequency band, spread logarithmically so that we have very fine resolution at low frequencies, and less resolution at high frequencies</li> <li>Then take the log of each mel spectrum values: human response to signal level is logarithmic, also makes the feature estimates less sensitive to variations in input such as power variations</li> </ul> <p>The mel frequency \\(m\\):</p> \\[mel(f) = 1127 \\ln{(1 + \\frac{f}{700})}\\]"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#text-to-speech-tts","title":"Text-to-speech (TTS)","text":"<ul> <li>To process text into audible speech</li> <li>Like ASR systems, TTS systems are generally based on the encoder-decoder architecture</li> <li>Speaker-dependent: trained to have a consistent voice, on much less data, but all from one speaker</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#text-normalizaton","title":"Text Normalizaton","text":"<ul> <li>Verbalize non-standard words, depends on its meaning (semiotic class)</li> <li>Normalization can be done by rule or by an encoder-decoder model</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#spectogram-prediction","title":"Spectogram Prediction","text":"<ul> <li>Maps from strings of letters to mel spectographs: sequences of mel spectral values over time</li> <li>e.g. Tacotron2</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#vocoding","title":"Vocoding","text":"<ul> <li>Generate waveforms from intermediate representations like mel spectograms</li> <li>e.g. Wavenet</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#evaluation","title":"Evaluation","text":"<ul> <li>Evaluated by human listeners</li> <li>Mean Opinion Score (MOS), usually on a scale from 1-5</li> <li>AB test to compare exactly two systems</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#wake-word-detection","title":"Wake Word Detection","text":"<ul> <li>Detect a word or short phrase, usually in order to wake-up a voice-enable assistant</li> <li>e.g. Alexa, Siri, Google Assistant</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#voice-activity-detection-vad","title":"Voice Activity Detection (VAD)","text":"<ul> <li>A process that identifies the presence or absence of human speech in an audio signal</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#speaker-diarization","title":"Speaker Diarization","text":"<ul> <li>The task of determining 'who spoke when' in a long multi-speaker audio recording</li> <li>Marking the start and end of each speaker's turns in the interaction</li> <li>Often use VAD to find segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Concepts/#speaker-recognition","title":"Speaker Recognition","text":"<ul> <li>The task of identifying a speaker</li> <li>Speaker verification: make a binary decision (is this speaker \\(X\\) or not?)</li> <li>Speaker identification: make a one of \\(N\\) decision trying to match a speaker's voice against a database of many speakers</li> <li>Language identification: given a wavefile, identify which language is being spoken</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Speech/Models/#listen-attend-and-spell-las","title":"Listen, Attend and Spell (LAS)","text":"<ul> <li>Attention-based Encoder Decoder (AED)</li> <li>The input is a sequence of \\(t\\) acoustic feature vectors \\(F = f_1, f_2, ..., f_t\\), one vector per 10 ms frame</li> <li>The output can be letters or wordpieces</li> <li>The encoder-decoder architecture is appropriate when input and output sequences have stark length differences: very long acoustic feature sequences mapping to much shorter sequences of letters or words</li> <li>Encoder-decoder architecture for speech need to have a compression stage that shortens the acoustic feature sequence before the encoder stage; alternatively use CTC loss function<ul> <li>e.g. Low frame rate algorithm: for time \\(i\\), concatenate the acoustic feature vector \\(f_i\\) with the prior two vectors \\(f_{i-1}, f_{i-2}\\)</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#adding-a-lm","title":"Adding a LM","text":"<ul> <li>Since an encoder-decoder model is essentially a conditional language model, they implicitly learn a language model for the output domain of letters from their training data</li> <li>Speech-text datasets are much smaller than pure text datasets</li> <li>We can usually improve a model at least slightly by incorporating a very large language model</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#beam-search","title":"Beam Search","text":"\\[score(Y|X) = \\frac{1}{|Y|_c} \\log{P(Y|X)} + \\lambda \\log{P_{LM}(Y)}\\] <ul> <li>To get a final beam of hypothesized sentences, a.k.a n-best list</li> <li>The scoring is done by interpolating the LM score and encoder-decoder score, with a weight \\(\\lambda\\) tune on the held-out set</li> <li>Since most models prefer shorter sentences, normalize the probability by the number of characters in the hypothesis \\(|Y|_c\\)</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#connectionist-temporal-classification-ctc","title":"Connectionist Temporal Classification (CTC)","text":"<ul> <li>The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input</li> <li>And then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence</li> <li>The CTC collapsing function is many-to-one; lots of different alignments map to the same output string</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#inference","title":"Inference","text":"<ul> <li>The most probable output sequence \\(Y\\) is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments:</li> </ul> \\[P_{CTC}(Y|X) = \\sum_{A \\in B^{-1}(Y)}{P(A|X)} = \\sum_{A \\in B^{-1}(Y)}{\\prod_{t=1}^{T}{p(a_t|h_t)}}\\] \\[\\hat{Y} = \\argmax_{Y}{P_{CTC}{(Y|X)}}\\] <ul> <li>Because of the strong conditional independence assumption (given the input, the output at time \\(t\\) is independent of the output at time \\(t-1\\)), CTC does not implicitly learn a language model</li> <li>Essential using CTC to interpolate a language model</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#training","title":"Training","text":"<p>The loss for an entire dataset \\(D\\) is the sum of the negative log-likelihoods of the correct output \\(Y\\) for each input \\(X\\):</p> \\[ L_{CTC} = \\sum_{(X,Y) \\in D}{-\\log{P_{CTC}(Y|X)}} \\]"},{"location":"note/Natural_Language_Processing/Speech/Models/#rnn-transducer-rnn-t","title":"RNN-Transducer (RNN-T)","text":"<ul> <li>Because of the strong independence assumption in CTC, recognizers based on CTC don't achieve as high an accuracy as the attention-based encoder-decoder recognizers</li> <li>CTC recognizers have the advantage of streaming, recognizing words on-line rather than waiting until the end of the sentence to recognize them</li> <li>Removes the conditional independence assumtpion</li> <li>Two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history</li> <li>At each time step \\(t\\), the CTC encoder outputs a hidden state \\(h_{t}^{\\text{enc}}\\) given the input \\(x_1 \\dots x_t\\)</li> <li>The language model predictor takes as input the previous output token, outputting a hidden state \\(h_{t}^{\\text{pred}}\\)</li> <li>The two hidden states are passed through the joint network, whose output is then passed through a softmax to predict the next character</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#tacotron2","title":"Tacotron2","text":"<ul> <li>Extends the earlier Tacotron architecture and the Wavenet vocoder</li> <li>An encoder-decoder maps from graphemes to mel spectograms, followed by a vocoder that maps to wavefiles</li> <li>Location-based attention, in which the computation of the \\(\\(\\alpha\\)\\) values makes use of the \\(\\(\\alpha\\)\\) values from the prior time-state</li> <li>Autoregressively predict one 80-dimensional log-mel filterbank vector frame (50 ms, with a 12.5 ms stride) at each step</li> <li>Stop token prediction, decision about whether to stop producing output</li> <li>Trained on gold log-mel filterbank features, using teacher forcing</li> </ul>"},{"location":"note/Natural_Language_Processing/Speech/Models/#wavenet","title":"WaveNet","text":"<ul> <li>An autoregressive network</li> <li>Takes spectograms as input and produces output represented as sequences of 8-bit \\(\\mu\\)-law audio samples</li> <li>This means that we can predict the value of each sample with a simple 256-way categorical classifier</li> </ul> <p>The probability of a waveform, a sequence of 8-bit mu-law values \\(Y = y_1, \\dots, y_t\\), given an intermediate input mel spectogram \\(h\\) is computed as:</p> \\[ p(Y) = \\prod_{t=1}^{t}{P(y_t | y_1,\\dots, y_{t-1}, h_1, \\dots, h_t)} \\] <ul> <li>The probability distribution is modeled by dilated convolution, a subtype of causal convolutional layer</li> <li>Dilated convolutions allow the vocoder to grow the receptive field exponentially with depth</li> </ul> <p></p>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/","title":"Tokens and Embeddings","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#distributional-hypothesis","title":"Distributional Hypothesis","text":"<ul> <li>Definition: Words that occur in similar contexts tend to have similar meanings</li> <li>Vector semantics: instantiates this linguistic hypothesis by learning representations of the meaning of words, called embeddings, directly from their distribution in texts</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#word-sense","title":"Word Sense","text":"<ul> <li>A sense refers to a specific meaning of a word</li> <li>Polysemy: words can have multiple senses</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#synonymy","title":"Synonymy","text":"<ul> <li>Synonyms: One word has a sense whose meaning is identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms</li> <li>A more formal definition of synonymy (between words rather than senses): two words are synonymous if they are substituable for one another in any sentence without changing the truth conditions of the sentence, the situations in which the sentence would be true</li> <li>In practice, the word synonym is used to describe a relationship of approximate or rough synonymy</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#principle-of-contrast","title":"Principle of Contrast","text":"<ul> <li>A difference in linguistic form is always associated with some difference in meaning</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#word-similarity","title":"Word Similarity","text":"<ul> <li>While words don't have many synonyms, most words do have lots of similar words</li> <li>Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are</li> <li>One way of getting values for word similarity is to ask humans to judge how similar one word is to another</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#word-relatedness","title":"Word Relatedness","text":"<ul> <li>Traditionally called word association in psychology</li> <li>One common kind of relatedness between words is if they belong to the same semantic field</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#semantic-field","title":"Semantic Field","text":"<ul> <li>A set of words which cover a particular semantic domain and bear structured relations with each other</li> <li>e.g. the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital)</li> <li>Also related to topic models, like Latent Dirichlet Allocation (LDA), which apply unsupervised learning on large sets of texts to induce sets of associated words from text</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#semantic-frame","title":"Semantic Frame","text":"<ul> <li>A set of words that denote perspectives or participants in a particular type of event</li> <li>Frames have semantic roles, and words in a sentence can take on these roles</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#connotation","title":"Connotation","text":"<ul> <li>The aspects of a word's meaning that are related to a writer or reader's emotions, sentiment, opinions, or evaluations</li> <li>Words varied along three important dimensions of affective meaning<ul> <li>Valence: the pleasantness of the stimulus</li> <li>Arousal: the intensity of emotion provoked by the stimulus</li> <li>Dominance: the degree of control exerted by the stimulus</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#sparse-embeddings","title":"Sparse Embeddings","text":"<ul> <li>Represent a word as a sparse, long vector correponsing to words in the vocabulary or documents in a collection</li> <li>e.g. TF-IDF, PPMI</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#dense-embeddings","title":"Dense Embeddings","text":"<ul> <li>Short embeddings, with number of dimensions \\(d\\) ranging from 50-1000, rather than the much larger vocabulary size |V| or number of documents \\(D\\)</li> <li>It turns out that dense vectors work better in every NLP task than sparse vectors</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#static-embeddings","title":"Static Embeddings","text":"<ul> <li>Each word has exactly one vector representation</li> <li>Cannot handle polysemy</li> <li>e.g. Word2Vec, GloVe, FastText</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#contextualized-embeddings","title":"Contextualized Embeddings","text":"<ul> <li>Words get different vectors based on their context</li> <li>e.g. BERT, GPT</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#co-occurrence-matrix","title":"Co-occurrence Matrix","text":"<ul> <li>A way of representing how often words co-occur</li> <li>Two popular matrices: term-document matrix, term-term matrix</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#term-document-matrix","title":"Term-Document Matrix","text":"<ul> <li>Originally defined as a means of finding similar documents for the task of document information retrieval</li> <li>\\(|V|\\) rows: each row represents a word in the vocabulary</li> <li>\\(D\\) columns: each column represents a document from some collection of documents</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#term-term-word-word-term-context-matrix","title":"Term-Term / Word-Word / Term-Context Matrix","text":"<ul> <li>The columns are labeled by words rather than documents</li> <li>Dimensionality of \\(|V| \\times |V|\\) and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus</li> <li>The context could be the document, however it is most common to use a window around the word</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#cosine-similarity","title":"Cosine Similarity","text":"\\[\\text{cosine}(\\bold{v}, \\bold{w}) = \\frac{\\bold{v} \\cdot \\bold{w}}{|\\bold{v}||\\bold{w}|}\\] <ul> <li>Cosine of the angle between the vectors</li> <li>The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions</li> <li>But since raw frequency values are non-negative, the cosine for these vectors ranges from 0-1</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#semantic-properties-of-embeddings","title":"Semantic Properties of Embeddings","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#size-of-the-context-window","title":"Size of the context window","text":"<ul> <li>Shorter context windows tend to lead to representations that are a bit more syntactic, since the information is coming from immediately nearby words</li> <li>When the vectors are computed from short context windows, the most similar words to a target word \\(w\\) tend to be semantically similar words with the same parts of speech</li> <li>When vectors are computed from long context windows, the highest cosine words to a target word \\(w\\) tend to be words that are topically related but not similar.</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#first-order-co-occurrence","title":"First-order co-occurrence","text":"<ul> <li>Two words have first-order co-occurrence (sometimes called syntagmatic association) if they are typically nearby each other</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#second-order-co-occurrence","title":"Second-order co-occurrence","text":"<ul> <li>Two words have second-order co-occurrence (sometimes called paradigmatic association) if they have similar neighbors</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#analogyrelational-similarity","title":"Analogy/Relational Similarity","text":"<p>Parallelogram model: for solving simple analogy problems of the form a is to b as a* is to what?</p> \\[\\bold{\\hat{b}}^{*} = \\argmin_{x}{\\text{distance}(\\bold{x}, \\bold{b} - \\bold{a} + \\bold{a}^{*})}\\] <ul> <li>e.g. Embeddings can roughly model relational similarity: \u2018queen\u2019 as the closest word to \u2018king\u2019 - \u2018man\u2019 + \u2018woman\u2019 implies the analogy man:king:queen</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#models","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#top-down-rule-based-tokenization","title":"Top-down (rule-based) tokenization","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#penn-treebank-tokenization","title":"Penn Treebank Tokenization","text":"<ul> <li>Separates out clitics (doesn\u2019t becomes does plus n\u2019t)</li> <li>Keeps hyphenated words together</li> <li>Separates out all punctuation</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#bottom-up-tokenization","title":"Bottom-up Tokenization","text":"<ul> <li>To deal with unknown word problem</li> <li>Induce sets of tokens that include tokens smaller than words, called subwords</li> <li>Most tokenization schemes have two parts:</li> <li>Token learner: takes a raw training corpus (sometimes pre-separated into words, for example by whitespace) and induces a vocabulary, a set of tokens</li> <li>Token segmenter: takes a raw test sentence and segments it into the tokens in the vocabulary</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#wordpiece","title":"WordPiece","text":"<ul> <li>TO-DO</li> <li>SentencePiece?</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#byte-pair-encoding-bpe","title":"Byte-Pair Encoding (BPE)","text":"<p>Token Learner:</p> <ul> <li>The BPE token learner begins with a vocabulary that is just the set of all individual characters</li> <li>It then examines the training corpus, chooses the two symbols that are most frequently adjacent (say 'A', 'B')</li> <li>Adds a new merged symbol 'AB' to the vocabulary, and replaces every adjacent 'A', 'B' in the new corpus with the new 'AB'</li> <li>It continues to count and merge, creating new longer and longer character strings, until \\(k\\) merges have been done, creating \\(k\\) novel tokens</li> <li>The algorithm is usually run inside words (not merging across word boundaries),so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol _, and its counts</li> </ul> <p>Token Segmenter:</p> <ul> <li>The token segmenter just runs greedily on the merges we have learned from the training data on the test data</li> <li>Thus the frequencies in the test data don't play a role, just the frequencies in the training data</li> <li>First we segment each test sentence word into characters</li> <li>Then look of the most frequent pair of adjacent characters or subwords in the current segmentation that matches a learned merge</li> <li>Replace this pair with the corresponding merged token</li> <li>Continues the merging process until no more merges can be applied</li> <li>In real settings BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#unigram-language-modeling","title":"Unigram Language Modeling","text":"<ul> <li>Often use the name SentencePiece to simply mean unigram language modeling tokenization</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#term-frequency-inverse-document-frequency-tf-idf","title":"Term Frequency-Inverse Document Frequency (TF-IDF)","text":"<ul> <li>To computer document similarity, word similarity</li> <li>Motivation: to balance these two conflicting constraints<ul> <li>Words that occur nearby frequently are more important than words that only appear once or twice</li> <li>Words that are too frequent/ubiquitous are unimportant</li> </ul> </li> <li>Usually when the dimensions are documents; term-document matrices</li> <li>TF-IDF weighting: the product of term frequency and document frequency</li> </ul> \\[w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t\\]"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#term-frequency","title":"Term Frequency","text":"<p>The frequency of the word \\(t\\) in the document \\(d\\):</p> \\[\\text{tf}_{t,d} = \\begin{cases}   1 + \\log_{10}{\\text{count}(t, d)} &amp; \\text{if} ~ \\text{count}(t, d) &gt; 0 \\\\   0 &amp; \\text{otherwise} \\end{cases}\\] <ul> <li>Tells us how frequent the word is</li> <li>Words that occur more often in a document are likely to be informative about the document's contents</li> <li>Why \\(\\log_{10}\\) of the word frequency instead of the raw count? The intuition is that a word appearing 100 times in a document doesn't make that word 100 times more likely to be relevant to the meaning of the document</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#document-frequency","title":"Document Frequency","text":"<ul> <li>\\(\\text{df}_t\\): document frequency of a term \\(t\\) is the number of documents it occurs in</li> <li>Give a higher weight to words that occur only in a few documents</li> <li>Terms that are limited to a few documents are useful for discriminating those documetns from the rest of the collection</li> <li>Terms that occur frequently across the entire collection aren't as helpful</li> </ul> <p>Inverse Document Frequency:</p> \\[\\text{idf}_t = \\log_{10}{\\left( \\frac{N}{\\text{df}_t} \\right)}\\] <ul> <li>\\(N\\) is the total number of documents in the collection</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#positive-pointwise-mutual-information-ppmi","title":"Positive Pointwise Mutual Information (PPMI)","text":"\\[\\text{PPMI}(w,c) = \\max{\\left( \\log_{2}{\\frac{P(w,c)}{P(w)P(c)}}, 0 \\right)}\\] <ul> <li>To compute word similarity</li> <li>Usually when the dimensions are words; term-term matrices</li> <li>Intuition: the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by by chance</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#pointwise-mutual-information-pmi","title":"Pointwise Mutual Information (PMI)","text":"<ul> <li>A measure of how often two events \\(x\\) and \\(y\\) occur, compared with what we would expect if they were independent</li> <li>The numerator tells us how often we observed the two words together (assuming computed by MLE)</li> <li>The denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently</li> <li>The ratio gives us an estimate of how much more the two words co-occur than we expect by chance</li> <li>PMI values range from negative to positive infinity</li> <li>But negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous</li> </ul> <p>PMI between a target word \\(w\\) and a context word \\(c\\):</p> \\[\\text{PMI}(w,c) = \\log_{2}{\\frac{P(w,c)}{P(w)P(c)}}\\]"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#word2vec","title":"Word2Vec","text":"<ul> <li>Intuition: self-supervision<ul> <li>Instead of counting how often each word \\(w\\) occurs near, say, apricot, we'll instread train a classifier on a binary prediction task: \"Is word \\(w\\) likely to show up near apricot?\"</li> <li>We don't actually care about this prediction task</li> <li>Instead we'll take the learned classifier weights as embeddings</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/#skip-gram","title":"Skip-gram","text":"<p>Intuition:</p> <ol> <li>Treat the target word and a neighboring context word as positive examples</li> <li>Randomly sample other words in the lexicon to get negative samples</li> <li>Use logistic regression to train a classifier to distinguish those two cases</li> <li>Use the learned weights as the embeddings</li> </ol> \\[P(+|w,c_{1:L}) = \\prod_{i=1}^{L}{\\sigma(\\bold{c_i} \\cdot \\bold{w})}\\] <ul> <li>Skip-gram model learns two separate embeddings for each word \\(i\\): the target embedding \\(\\bold{w}_i\\) and the context embedding \\(\\bold{c}_i\\)</li> <li>Thus the parameters we need to learn are two matrices \\(\\bold{W}\\) (target matrix) and \\(\\bold{C}\\) (context matrix), each containing an embedding for every one of the \\(|V|\\) words in the vocabulary \\(V\\)</li> <li>\\(L\\) is the context window size</li> <li>We can represent word \\(i\\) with the vector \\(\\bold{w}_i + \\bold{c}_i\\), or just by the vector \\(\\bold{w}_i\\)</li> </ul> <p>Loss Function:</p> \\[L = - \\log{\\left[ P(+|w,c_{pos}) \\prod_{i=1}^{k}P(-|w,c_{neg_i}) \\right]}\\] <ul> <li>Skip-gram with negative sampling (SGNS) uses more negative examples than positive examples</li> <li>For each \\((w, c_{pos})\\) we will create \\(k\\) negative samples</li> </ul>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/tokenizer/","title":"Tokenizer","text":"In\u00a0[1]: Copied! <pre>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda:1\"\n</pre> import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  from transformers import pipeline from transformers import AutoTokenizer, AutoModelForCausalLM  device = \"cuda:1\" <pre>/home/junwai/miniconda3/envs/wise/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre># Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"microsoft/Phi-3-mini-4k-instruct\",\n  device_map=device,\n  torch_dtype=\"auto\",\n  trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n</pre> # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(   \"microsoft/Phi-3-mini-4k-instruct\",   device_map=device,   torch_dtype=\"auto\",   trust_remote_code=True )  tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") In\u00a0[\u00a0]: Copied! <pre>prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. &lt;|assistant|&gt;\"\n\n# Tokenize the input prompt\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# Generate the text\ngeneration_output = model.generate(\n  input_ids=input_ids,\n  max_new_tokens=20\n)\n\n# Print the output\nprint(tokenizer.decode(generation_output[0]))\n</pre> prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. &lt;|assistant|&gt;\"  # Tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)  # Generate the text generation_output = model.generate(   input_ids=input_ids,   max_new_tokens=20 )  # Print the output print(tokenizer.decode(generation_output[0])) <pre>Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. &lt;|assistant|&gt; Subject: Heartfelt Apologies for the Gardening Mishap\n\n\nDear\n</pre> In\u00a0[\u00a0]: Copied! <pre>for id in input_ids[0]:\n  print(tokenizer.decode(id), id.item())\n</pre> for id in input_ids[0]:   print(tokenizer.decode(id), id.item()) <pre>Write 14350\nan 385\nemail 4876\napolog 27746\nizing 5281\nto 304\nSarah 19235\nfor 363\nthe 278\ntrag 25305\nic 293\ngarden 16423\ning 292\nm 286\nish 728\nap 481\n. 29889\nExp 12027\nlain 7420\nhow 920\nit 372\nhappened 9559\n. 29889\n 29871\n&lt;|assistant|&gt; 32001\n</pre>"},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/tokenizer/#output","title":"Output\u00b6","text":""},{"location":"note/Natural_Language_Processing/Tokens_%26_Embeddings/tokenizer/#token-id","title":"Token ID\u00b6","text":""},{"location":"note/Natural_Language_Processing/Traditional_NLP/","title":"Traditional NLP","text":""},{"location":"note/Natural_Language_Processing/Traditional_NLP/#applications","title":"Applications","text":"<ol> <li>Text Classification</li> <li>Sequence Labelling: NER, POS</li> <li>Dependency Parsing</li> <li>Text-to-Speech</li> </ol>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#concepts","title":"Concepts","text":""},{"location":"note/Natural_Language_Processing/Traditional_NLP/#regular-expressions","title":"Regular Expressions","text":"<ul> <li>TO-DO</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#word-type","title":"Word Type","text":"<ul> <li>The number of distinct words in a corpus</li> <li>If the set of words in the vocabulary is \\(V\\), the number of types is the vocabulary size \\(|V|\\)</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#word-instance","title":"Word Instance","text":"<ul> <li>The total number of \\(N\\) running words</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#corpus","title":"Corpus","text":"<ul> <li>Plural corpora</li> <li>A computer-readable collection of text or speech</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#code-switching","title":"Code Switching","text":"<ul> <li>The phenomenon of speakers or writers to use multiple languages in a single communicative act</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#utterance","title":"Utterance","text":"<ul> <li>The spoken correlate of a sentence</li> <li>e.g. I do uh main- mainly business data processing<ul> <li>This utterance has two kinds of disfluencies</li> <li>Fragment: the broken-off word main-</li> <li>Filled pause: words like uh and um</li> </ul> </li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#heaps-law-herdans-law","title":"Heaps' Law / Herdan's Law","text":"<p>The relationship between the number of types \\(|V|\\) and number of instances \\(N\\):</p> \\[|V| = kN^{\\beta}\\]"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#lemma-citation-form","title":"Lemma / Citation Form","text":"<ul> <li>A set of lexical forms having the same stem, the same major part-of-speech, and the same word sense</li> <li>e.g. Consider inflected forms like cats and cat, they are different wordforms but have the same lemma</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#word-normalization","title":"Word Normalization","text":"<ul> <li>The process of transforming text into a standard or canonical form</li> <li>At least three tasks are commonly applied<ol> <li>Tokenizing (segmenting) words</li> <li>Normalizing word formats</li> <li>Segmenting sentences</li> </ol> </li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#stop-words","title":"Stop Words","text":"<ul> <li>Commonly used words that are often filtered out before processing text</li> <li>These words are considered to have little value in terms of meaning</li> <li>e.g. and, the, a, is, are, to etc.</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#morphology","title":"Morphology","text":"<ul> <li>The study of the way words are built up from morphemes</li> <li>Stems: the central morpheme of the word, supplying the main meaning</li> <li>Affixes: adding \"additional\" meanings of various kinds</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#phoneme","title":"Phoneme","text":"<ul> <li>Expert term for 'sound'</li> <li>e.g. /k/</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#grapheme","title":"Grapheme","text":"<ul> <li>Expert term for 'letter(s) that spell a sound'</li> <li>e.g. the following table shows 4 different graphemes to represent 1 phoneme, the /k/ sound</li> </ul> Word Grapheme cat c kite k duck -ck school ch"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#morpheme","title":"Morpheme","text":"<ul> <li>The smalles meaning-bearing unit of a language</li> <li>e.g. unwashable has the morphemes un-, wash, -able</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#case-folding","title":"Case Folding","text":"<ul> <li>The process of converting all characters in a text to a uniform case</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#lemmatizaion","title":"Lemmatizaion","text":"<ul> <li>The task of determining that two words have the same root, despite their surface differences</li> <li>e.g. The words am, are, is have the shared lemma be</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#stemming","title":"Stemming","text":"<ul> <li>Chopping off word-final affixes</li> <li>e.g. Porter stemmer</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#sentence-segmentaion","title":"Sentence Segmentaion","text":"<ul> <li>The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points</li> <li>In general, sentence segmentation methods work by first deciding whether a period is part of the word or is a sentence-boundary marker</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#minimum-edit-distance","title":"Minimum Edit Distance","text":"<ul> <li>The minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another</li> <li>The Levenshtein distance between two sequences is the simplest weighting factor in which each of the three operations has a cost of 1 </li> <li>Can be computed by dynamic programming, which also results in an alignment of the two strings</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#perplexity","title":"Perplexity","text":"<p>The perplexity of \\(W\\) computed with a bigram language model:</p> \\[\\text{perplexity}(W) = \\sqrt[N]{\\prod^{N}_{i=1}{\\frac{1}{P(w_i|w_{i-1})}}}\\] <ul> <li>The higher the probabiity of the word sequence, the lower the perplexity</li> <li>The lower the perpexity of a model on the data, the better the model</li> <li>Minimizng the perplexity is equivalent to maximizing the test set probability according to the language model</li> <li>Can also be thought of as the weighted average branching factor</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#smoothing","title":"Smoothing","text":"<ul> <li>TO-DO</li> <li>smoothing, interpolation and backoff</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#statistical-significance-test","title":"Statistical Significance Test","text":"<ul> <li>TO-DO</li> <li>Used to determine whether we can be confident that one versrion of a model is better than another</li> <li>Bootstrap test</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#model-card","title":"Model Card","text":"<p>Documents a ML model with information like:</p> <ul> <li>Training algorithms and parameters</li> <li>Training data sources, motivation, and preprocessing</li> <li>Evaluation data sources, motivation, and preprocessing</li> <li>Intended use and users</li> <li>Model performance across demographic or other groups and environmental situations</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#part-of-speech-pos","title":"Part of Speech (POS)","text":"<ul> <li>A category of words that have similar grammatical properties:</li> <li>e.g. noun, verb, pronoun, adjective, adverb, preposition, etc.</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#natural-language-inference-nli","title":"Natural Language Inference (NLI)","text":"<p>The task of determining whether a \"hypothesis\" is:</p> <ul> <li>true (entailment)</li> <li>false (contradiction)</li> <li>undetermined (neutral)</li> </ul> <p>given a premise</p>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#models","title":"Models","text":""},{"location":"note/Natural_Language_Processing/Traditional_NLP/#n-gram-language-models","title":"N-gram Language Models","text":"<p>Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence:</p> \\[P(w_{1:n}) \\approx \\prod_{k=1}^{n}{P(w_k|w_{k-1})}\\] <ul> <li>An n-gram is a sequence of \\(n\\) words</li> <li>Markov assumption: the probability of a words depends only on the previous word</li> <li>The n-gram model looks \\(n-1\\) words into the past</li> <li>Estimate n-gram probability through Maximum Likelihood Estimation (MLE)</li> </ul> <p>For example, the bigram probability of a word \\(w_n\\) given a previous word \\(w_{n-1}\\): \\(\\(P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n)}{\\sum_w{C(w_{n-1}w)}} = \\frac{C(w_{n-1}w_n)}{C(w_{n-1})}\\)\\)</p> <ul> <li>Compute the count of the bigram \\(C(w_{n-1}w_n)\\), and normalize by the sum of all the bigrams that share the same first word \\(w_{n-1}\\)</li> <li>The sum of all bigram counts that start with a given word \\(w_{n\u22121}\\) must be equal to the unigram count for that word \\(w_{n\u22121}\\)</li> </ul> <p>Dealing with scale in large n-gram models:</p> <ul> <li>Use log probabilities, since multiplying enough n-grams together would result in numerical underflow</li> <li>Although for pedagogical purposes we have only described bitrigram gram models, when there is sufficient training data we use trigram models, which condition on the previous two words, or 4-gram or 5-gram models.</li> <li>For these larger n-grams, we\u2019ll need to assume extra contexts to the left and right of the sentence end</li> <li>The infini-gram project allows n-gram of any length</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#bag-of-words","title":"Bag-Of-Words","text":"<ul> <li>Tokenization: Split the text into individual tokens</li> <li>Vocabulary Creation: An unordered set of words with their position ignored, keeping only their frequency in the text document</li> <li>Vectorization: Represent each document as a vector, where each element corresponds to the count of a word from the vocabulary</li> </ul>"},{"location":"note/Natural_Language_Processing/Traditional_NLP/#naive-bayes","title":"Naive Bayes","text":"<p>A probabilistic classifier, for a document \\(d\\), out of all classes \\(c \\in C\\), the classifier returns the class \\(\\hat{c}\\) which has the maximum posterior probability given the document:</p> \\[\\hat{c} = \\argmax_{c \\in C}{P(c|d)} = \\argmax_{c \\in C}{\\frac{P(d|c)P(c)}{P(d)}} = \\argmax_{c \\in C}{P(d|c)P(c)}\\] <ul> <li>We call Naive Bayes a generative model, an implicit assumption about how a document is generated:<ul> <li>A class is sampled from \\(P(c)\\) (prior probability of the class)</li> <li>Then the words are generated by sampling from \\(P(d|c)\\) (likelihood of the document)</li> </ul> </li> </ul> <p>We can represent a document \\(d\\) as a set of features \\(f_1, f_2, \\dots, f_n\\):</p> \\[\\hat{c} = \\argmax_{c \\in C} P(f_1, f_2, \\dots, f_n | c)P(c)\\] <ul> <li>Estimating the probability of every possible combination of features would require huge number of parameters and impossibly large training sets</li> <li>Naive Bayes classifiers make two simplifying assumptions:<ol> <li>Bag-of-words assumption: assume position doesn't matter</li> <li>Naive Bayes assumption: conditional independence assumption</li> </ol> </li> </ul> <p>Final equation for the class chosen by a Naive Bayes classifier:</p> \\[c_{NB} = \\argmax_{c \\in C}{P(c) \\prod_{i \\in positions}{P(w_i|c)}}\\] <p>Naive Bayes as a Language Model:</p> \\[P(s|c) = \\prod_{i \\in positions}{P(w_i | c)}\\]"},{"location":"note/Operating_System/","title":"Operating Systems","text":"<ul> <li>understanding operating systems</li> <li>how linux works</li> </ul>"},{"location":"note/Recommendation_System/","title":"Recommendation Systems","text":""},{"location":"note/Software_Engineering/","title":"Software Engineering","text":"<ul> <li>Software Engineering for Data Scientist</li> <li>Fundamentals of Software Architecture</li> </ul>"},{"location":"note/mlops/","title":"ML Ops","text":"<ul> <li>Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications</li> </ul>"},{"location":"project/","title":"Projects","text":""},{"location":"project/RAG/","title":"RAG","text":"<p>Ideas: text-to-SQL</p> <p>https://docs.google.com/presentation/d/1XDM8DKsV7tp2o-tzNDU3I2lToAlJkR0-BFFPoMzAFD8/edit#slide=id.g3167ccc43ed_0_1065</p>"},{"location":"project/Recommend/","title":"Recommendation","text":"<p>Recommendation system with knowledge graph and LLM</p>"}]}